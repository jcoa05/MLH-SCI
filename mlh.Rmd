---
title: "Pragmatic implementation of stacked ensemble learning in resource-constrained clinical research: a methodological demonstration"
subtitle: "Supplementary code"
author: "Authors"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
    df_print: paged
  word_document:
    toc: true
    toc_depth: 3
    fig_width: 8
    fig_height: 6
---

# PHASE 1: ENVIRONMENT SETUP {.tabset}

## Global Options

```{r setup, include=FALSE}
knitr::opts_chunk$set(
 echo = FALSE,
 warning = FALSE,
 message = FALSE,
 fig.align = 'center',
 dpi = 600,
 cache = FALSE
)
```

## Reproducibility

```{r reproducibility-setup, include=FALSE}
# This chunk ensures reproducible package versions via renv

if (!requireNamespace("renv", quietly = TRUE)) {
 message("Installing 'renv' to manage project dependencies...")
 install.packages("renv")
}

if (file.exists("renv.lock")) {
 status <- try(renv::status(), silent = TRUE)
 if (inherits(status, "try-error") ||
     length(status$library$missing) > 0 ||
     length(status$lockfile$missing) > 0) {
   message("Restoring exact package versions from renv.lock...")
   renv::restore(prompt = FALSE)
   message("Environment restored successfully.")
 } else {
   message("Environment synchronized. Using reproducible package versions.")
 }
} else {
 warning("CRITICAL: 'renv.lock' not found. Reproducibility not guaranteed.")
}
```

## Packages

```{r packages, message=FALSE}
if (!require(pacman, quietly = TRUE)) install.packages("pacman")

pacman::p_load(
 # Core ML Framework
 tidymodels, stacks,
 # Base Model Engines
 glmnet, ranger, xgboost, kernlab, earth,
 # Parallel Processing & Monitoring
 doFuture, foreach, tictoc, pryr, callr, ps,
 # Data Manipulation
 dplyr, tidyr, purrr, tibble, stringr, forcats,
 # Visualization & Tables
 ggplot2, gtsummary, gt, broom, viridis, patchwork,
 # Model Interpretation
 DALEX, DALEXtra, fastshap, yardstick, probably,
 # Memory tracking
 lobstr
)

# Set global seed for reproducibility
set.seed(123)
Sys.setenv(LANG = "en")

cat("Packages loaded successfully.\n")
```

## Parallelization

```{r parallelization}
total_cores <- parallel::detectCores(logical = TRUE)
#num_workers <- max(1, total_cores - 1)
num_workers <- 3 # fixed for resource constrained scenario

registerDoFuture()
plan(multisession, workers = num_workers)

cat(sprintf("Registered %d workers for parallel processing.\n", num_workers))
```

## Hardware Detection

```{r hardware-detection}
# Automatic hardware profiling (no manual edits required)

hardware_specs <- tibble(
 Specification = c(
   "CPU Cores (Physical)",
   "CPU Threads (Logical)",
   "Workers Used",
   "R Version",
   "Platform",
   "Operating System"
 ),
 Value = c(
   as.character(parallel::detectCores(logical = FALSE)),
   as.character(parallel::detectCores(logical = TRUE)),
   as.character(num_workers),
   R.version$version.string,
   R.version$platform,
   paste(Sys.info()["sysname"], Sys.info()["release"])
 )
)

cat("\n=== HARDWARE PROFILE ===\n")
print(hardware_specs)
```

# PHASE 2: CONFIGURATION {.tabset}

## Control Knobs

```{r control-knobs}
# All tunable parameters in one location for easy modification

PARAMS <- list(
 # Cross-validation
 cv_folds = 5,
 cv_repeats = 3,
 
 # Model complexity
 grid_size = 25,
 
 # Evaluation
 bootstrap_iters = 1000,
 dalex_b = 100,
 shap_nsim = 100,
 
 # Clinical thresholds (for secondary analyses)
 mcid_val = 4,
 recovery_thresh = 60
)

cat("Control parameters set:\n")
print(as_tibble(PARAMS))
```

## Memory Monitor

```{r monitor-setup, include=FALSE}
# ============================================================================
# UNIFIED MEMORY MONITOR WITH PHASE LOGGING
# ============================================================================
# This monitor runs continuously throughout the workflow and logs:
# - RSS (Resident Set Size): Physical RAM actually used
# - VMS (Virtual Memory Size): Total virtual memory allocated
# - Phase: Current workflow phase for visualization
#
# The monitor writes to a single CSV file that is read at the end to generate
# a publication-quality figure showing RAM consumption across all phases.
# ============================================================================

MEMORY_LOG_FILE <- "memory_log_unified.csv"
PHASE_MARKER_FILE <- "current_phase.txt"

# Initialize the unified monitor
monitor_unified_ram <- function(log_file = "memory_log_unified.csv", 
                                phase_file = "current_phase.txt",
                                interval_seconds = 2) {
  
  # Initialize log file with headers
  write.csv(data.frame(
    timestamp = Sys.time(), 
    elapsed_sec = 0,
    rss_gb = 0, 
    vms_gb = 0,
    phase = "Initialization"
  ), log_file, row.names = FALSE)
  
  # Record start time

  start_time <- Sys.time()
  
  # Initialize phase file
  writeLines("Initialization", phase_file)
  
  tryCatch({
    my_pid <- Sys.getpid()
    my_handle <- ps::ps_handle(my_pid)
    target_user <- tryCatch(ps::ps_username(my_handle), error = function(e) NULL)
    
    while(TRUE) {
      # Read current phase from file (allows main process to update it)
      current_phase <- tryCatch(
        readLines(phase_file, n = 1, warn = FALSE),
        error = function(e) "Unknown"
      )
      
      all_ps <- tryCatch(ps::ps(), error = function(e) NULL)
      
      if (!is.null(all_ps) && !is.null(target_user)) {
        target_indices <- which(
          grepl("rsession|rterm|rgui|rscript|rstudio", all_ps$name, ignore.case = TRUE) &
            all_ps$username == target_user
        )
        r_ps <- all_ps[target_indices, ]
        
        stats_list <- lapply(r_ps$pid, function(p) {
          tryCatch({
            h <- ps::ps_handle(pid = p)
            mem <- ps::ps_memory_info(h)
            return(c(rss = mem[["rss"]], vms = mem[["vms"]]))
          }, error = function(e) c(rss = 0, vms = 0))
        })
        
        stats_mat <- do.call(rbind, stats_list)
        total_rss_gb <- sum(stats_mat[, "rss"], na.rm = TRUE) / (1024^3)
        total_vms_gb <- sum(stats_mat[, "vms"], na.rm = TRUE) / (1024^3)
        
        elapsed <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
        
        write.table(
          data.frame(
            timestamp = Sys.time(), 
            elapsed_sec = elapsed,
            rss_gb = total_rss_gb, 
            vms_gb = total_vms_gb,
            phase = current_phase
          ),
          log_file, sep = ",", col.names = FALSE, row.names = FALSE, append = TRUE
        )
      }
      
      Sys.sleep(interval_seconds)
      if (file.exists("stop_monitoring.file")) break
    }
  }, error = function(e) {
    message("Monitor error: ", e$message)
  })
}

# Helper function to update phase (call this from main workflow)
set_monitor_phase <- function(phase_name, phase_file = "current_phase.txt") {
  writeLines(phase_name, phase_file)
  Sys.sleep(0.5)  
}

# Cleanup from previous runs
for (f in c(MEMORY_LOG_FILE, PHASE_MARKER_FILE, "stop_monitoring.file")) {
  if (file.exists(f)) file.remove(f)
}

cat("Unified memory monitor configured.\n")

```

## Timing & Memory Tracking

```{r tracking-setup, include=FALSE}
# Initialize tracking tibbles

timing_results <- tibble(
 component = character(),
 wall_time_seconds = numeric(),
 wall_time_formatted = character()
)

memory_results <- tibble(
 component = character(),
 memory_mb = numeric(),
 memory_gb = numeric()
)

# Helper: Save timing
save_timing <- function(component_name, tic_output) {
 elapsed <- tic_output$toc - tic_output$tic
 timing_results <<- timing_results %>%
   add_row(
     component = component_name,
     wall_time_seconds = elapsed,
     wall_time_formatted = sprintf("%.2f min (%.0f sec)", elapsed/60, elapsed)
   )
}

# Helper: Record memory
record_memory <- function(component_name) {
 mem_bytes <- pryr::mem_used()
 mem_mb <- as.numeric(mem_bytes) / (1024^2)
 mem_gb <- mem_mb / 1024
 
 memory_results <<- memory_results %>%
   add_row(component = component_name, memory_mb = mem_mb, memory_gb = mem_gb)
 
 invisible(mem_mb)
}

# Record baseline
record_memory("Baseline (after package loading)")
```

# PHASE 3: DATA & PREPROCESSING {.tabset}

## Data Loading

```{r data-load}
df <- read.csv("data.csv")

cat(sprintf("Dataset loaded: %d observations, %d variables\n", nrow(df), ncol(df)))
cat(sprintf("Outcome variable: motor_score_12m (range: %d - %d)\n",
           min(df$motor_score_12m), max(df$motor_score_12m)))
cat(sprintf("Clustering variable: center_id (%d unique centers)\n",
           length(unique(df$center_id))))
```

## Cross-Validation Strategy

```{r cv-strategy}
# ============================================================================
# PRAGMATIC ADAPTATION #1: Grouped V-Fold CV Instead of Nested CV
# ============================================================================
# Trade-off: Accepts minor hyperparameter optimization bias in exchange for
# feasible computation time. Nested CV (5 outer x 5 inner x 2 repeats = 50
# ensemble constructions) would exceed overnight execution limits.
#
# Justification: Grouping by center_id simulates external validation by
# ensuring all observations from a given site appear exclusively in either
# the training or validation set, preventing center-level data leakage.
# ============================================================================

set.seed(123)
cv_splits <- group_vfold_cv(
 df,
 group = center_id,
 v = PARAMS$cv_folds,
 repeats = PARAMS$cv_repeats
)

cat(sprintf("Cross-validation: %d-fold grouped CV, %d repeats = %d total resamples\n",
           PARAMS$cv_folds, PARAMS$cv_repeats, PARAMS$cv_folds * PARAMS$cv_repeats))
```

## Preprocessing Recipe

```{r preprocessing-recipe}
data_recipe <- recipe(motor_score_12m ~ ., data = df) %>%
 # Exclude ID columns from modeling
 update_role(patient_id, new_role = "ID") %>%
 update_role(patient_id_global, new_role = "ID") %>%
 update_role(center_id, new_role = "ID") %>%
 # Preprocessing steps
 step_novel(all_nominal_predictors()) %>%
 step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
 step_normalize(all_numeric_predictors(),
                -all_of(c("patient_id", "patient_id_global", "center_id"))) %>%
 step_integer(all_logical_predictors()) %>%
 step_zv(all_predictors())

cat("Preprocessing recipe defined.\n")
cat("Steps: dummy encoding -> normalization -> zero-variance removal\n")
```

# PHASE 4: MODEL SPECIFICATION {.tabset}

## Base Learner Specifications

```{r baselearner-specs}
# ============================================================================
# BASE LEARNER SELECTION RATIONALE
# ============================================================================
# Five diverse, CPU-compatible algorithms selected to maximize complementarity:
# - Linear (ElasticNet): Captures linear relationships, interpretable
# - Tree-based (RF, XGBoost): Captures non-linear interactions
# - Kernel-based (SVM): Captures complex decision boundaries
# - Spline-based (MARS): Captures piecewise-linear relationships
# ============================================================================

# 1. ElasticNet (penalized linear regression)
set.seed(123)
elnet_spec <- linear_reg(penalty = tune(), mixture = tune()) %>%
 set_engine("glmnet") %>%
 set_mode("regression")

# 2. Random Forest
set.seed(123)
rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 2000) %>%
 set_engine("ranger", num.threads = 1) %>%  # Single-threaded to prevent oversubscription
 set_mode("regression")

# 3. XGBoost
# ============================================================================
# NOTE: We intentionally OMIT early stopping during tuning.
# Early stopping with an internal validation split creates data leakage within
# CV folds. The cross-validation process itself acts as regularization.
# Reference: https://tune.tidymodels.org/articles/extras/optimizations.html
# ============================================================================
set.seed(123)
xgb_spec <- boost_tree(
 trees = 2000,
 tree_depth = tune(),
 learn_rate = tune(),
 min_n = tune(),
 loss_reduction = tune(),
 sample_size = tune()
) %>%
 set_engine("xgboost", nthread = 1) %>%
 set_mode("regression")

# 4. SVM (Radial Basis Function kernel)
set.seed(123)
svm_spec <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
 set_engine("kernlab") %>%
 set_mode("regression")

# 5. MARS (Multivariate Adaptive Regression Splines)
set.seed(123)
mars_spec <- mars(num_terms = tune(), prod_degree = tune()) %>%
 set_engine("earth") %>%
 set_mode("regression")

cat("Base learner specifications defined: ElasticNet, RF, XGBoost, SVM, MARS\n")
```

## Hyperparameter Grids

```{r hyperparameter-grids}
# ============================================================================
# PRAGMATIC ADAPTATION #2: Constrained Hyperparameter Grids
# ============================================================================
# Trade-off: Accepts potentially suboptimal hyperparameters for tractable
# computation time. Standard literature grids (50-100 configs per algorithm)
# would require ~5x more compute time.
#
# Strategy:
# - Random grids for high-dimensional spaces (ElasticNet, XGBoost, SVM)
# - Regular grids for lower-dimensional spaces (RF, MARS)
# ============================================================================

set.seed(123)

# ElasticNet: 2D random grid
elnet_grid <- grid_random(
 penalty(range = c(-5, 1)),
 mixture(range = c(0, 1)),
 size = PARAMS$grid_size
)

# Random Forest: Regular grid (2 parameters)
rf_grid <- grid_regular(
 mtry(range = c(5, 20)),
 min_n(range = c(5, 15)),
 levels = c(mtry = 5, min_n = 5)
)

# XGBoost: 5D random grid
xgb_grid <- grid_random(
 tree_depth(range = c(3, 10)),
 min_n(range = c(2, 20)),
 learn_rate(range = c(0.001, 0.3)),
 loss_reduction(range = c(0.001, 10)),
 sample_prop(range = c(0.5, 1.0)),
 size = PARAMS$grid_size
)

# SVM: 2D random grid
svm_grid <- grid_random(
 cost(range = c(-5, 2)),
 rbf_sigma(range = c(-5, 0)),
 size = PARAMS$grid_size
)

# MARS: Regular grid (2 parameters)
mars_grid <- grid_regular(
 num_terms(range = c(5, 25)),
 prod_degree(range = c(1L, 2L)),
 levels = c(num_terms = 12, prod_degree = 2)
)

cat("Hyperparameter grids defined.\n")
```

## Training Count Calculation

```{r training-counts}
# Calculate exact number of model training episodes for feasibility reporting

training_counts <- tibble(
 Model = c("ElasticNet", "Random Forest", "XGBoost", "SVM", "MARS"),
 Grid_Size = c(nrow(elnet_grid), nrow(rf_grid), nrow(xgb_grid),
               nrow(svm_grid), nrow(mars_grid)),
 CV_Folds = PARAMS$cv_folds,
 CV_Repeats = PARAMS$cv_repeats
) %>%
 mutate(
   Models_Per_Repeat = Grid_Size * CV_Folds,
   Total_Models = Models_Per_Repeat * CV_Repeats
 )

total_training_episodes <- sum(training_counts$Total_Models)

cat(sprintf("\n=== TRAINING COMPLEXITY ===\n"))
cat(sprintf("Total model training episodes: %d\n\n", total_training_episodes))
print(training_counts)

gc() #call garbage collection 
```

```{r grid-sensitivity-analysis, eval=FALSE}
# ============================================================================
# GRID SIZE SENSITIVITY ANALYSIS
# ============================================================================
# Demonstrates how computational requirements scale with grid size
# Only runs ElasticNet to minimize total runtime while showing scaling behavior
# Set eval=TRUE to run this analysis
# ============================================================================

sensitivity_results <- tibble(
  grid_size = integer(),
  time_seconds = numeric(),
  peak_rss_gb = numeric()
)

test_grid_sizes <- c(5, 15, 30)  # Test 3 configurations

for (gs in test_grid_sizes) {
  
  cat(sprintf("\n--- Testing grid_size = %d ---\n", gs))
  
  # Create grid for this size
  test_grid <- grid_random(
    penalty(range = c(-5, 1)),
    mixture(range = c(0, 1)),
    size = gs
  )
  
  # Create minimal CV splits (2-fold, 1 repeat for speed)
  test_splits <- group_vfold_cv(df, group = center_id, v = 2, repeats = 1)
  
  # Time the tuning
  start_time <- Sys.time()
  start_mem <- as.numeric(pryr::mem_used()) / (1024^3)
  
  set.seed(123)
  test_result <- tune_grid(
    workflow() %>%
      add_recipe(data_recipe) %>%
      add_model(elnet_spec),
    resamples = test_splits,
    grid = test_grid,
    metrics = metric_set(rmse),
    control = control_grid(verbose = FALSE)
  )
  
  end_time <- Sys.time()
  end_mem <- as.numeric(pryr::mem_used()) / (1024^3)
  
  elapsed <- as.numeric(difftime(end_time, start_time, units = "secs"))
  
  sensitivity_results <- sensitivity_results %>%
    add_row(
      grid_size = gs,
      time_seconds = elapsed,
      peak_rss_gb = max(start_mem, end_mem)
    )
  
  cat(sprintf("   Time: %.1f seconds, Memory: %.2f GB\n", elapsed, end_mem))
}

# Plot scaling behavior
sensitivity_plot <- ggplot(sensitivity_results, aes(x = grid_size, y = time_seconds)) +
  geom_point(size = 4, color = "#2E86AB") +
  geom_line(linewidth = 1, color = "#2E86AB") +
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "gray50") +
  labs(
    title = "Computational Scaling: Grid Size vs. Tuning Time",
    subtitle = "ElasticNet only, 2-fold CV, 1 repeat (demonstration of scaling behavior)",
    x = "Hyperparameter Grid Size",
    y = "Tuning Time (seconds)"
  ) +
  theme_minimal()

print(sensitivity_plot)
print(sensitivity_results)

# Project to full workflow
cat("\n--- Projected Full Workflow Times ---\n")
# Assuming linear scaling, project what full 5-algorithm workflow would take
for (i in 1:nrow(sensitivity_results)) {
  gs <- sensitivity_results$grid_size[i]
  time_sec <- sensitivity_results$time_seconds[i]
  # Scale up: 5 algorithms, 5 folds, 2+ repeats
  projected_min <- (time_sec * 5 * 2.5 * PARAMS$cv_repeats) / 60
  cat(sprintf("   grid_size=%d: ~%.1f minutes projected\n", gs, projected_min))
}
```

# PHASE 5: MODEL TRAINING {.tabset}

## Base Learner Tuning

```{r baselearner-tuning}
# ============================================================================
# START UNIFIED MEMORY MONITOR (runs for entire training + stacking + SHAP)
# ============================================================================
if (file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")
if (file.exists(MEMORY_LOG_FILE)) file.remove(MEMORY_LOG_FILE)
if (file.exists(PHASE_MARKER_FILE)) file.remove(PHASE_MARKER_FILE)

bg_monitor <- callr::r_bg(
  monitor_unified_ram,
  args = list(log_file = MEMORY_LOG_FILE, phase_file = PHASE_MARKER_FILE),
  supervise = TRUE
)
on.exit({
  try({ bg_monitor$kill() }, silent = TRUE)
  if(file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")
}, add = TRUE)
Sys.sleep(3)  # Allow monitor to initialize

# Set phase
set_monitor_phase("Base Learner Tuning")

# Create workflow set
tic.clearlog()
tic("Base learners tuning")

set.seed(123)
df_workflow_set <- workflow_set(
 preproc = list(recipe = data_recipe),
 models = list(
   elnet = elnet_spec,
   rf = rf_spec,
   xgb = xgb_spec,
   svm = svm_spec,
   mars = mars_spec
 )
) %>%
 option_add(grid = elnet_grid, id = "recipe_elnet") %>%
 option_add(grid = rf_grid, id = "recipe_rf") %>%
 option_add(grid = xgb_grid, id = "recipe_xgb") %>%
 option_add(grid = svm_grid, id = "recipe_svm") %>%
 option_add(grid = mars_grid, id = "recipe_mars")

# Control settings for stacking (saves predictions)
ctrl_stack <- control_stack_grid()

# Run parallel tuning
set.seed(123)
model_set_results <- workflow_map(
 df_workflow_set,
 fn = "tune_grid",
 resamples = cv_splits,
 metrics = metric_set(rmse, mae, rsq),
 control = ctrl_stack,
 verbose = TRUE
)

timing_baselearners <- toc(log = TRUE, quiet = TRUE)
save_timing("Base learners tuning", timing_baselearners)

cat(sprintf("\nTuning complete: %d models trained in %.1f minutes\n",
           total_training_episodes,
           (timing_baselearners$toc - timing_baselearners$tic) / 60))

```

## Stacked Ensemble Construction

```{r stacking}

set_monitor_phase("Stacking")

tic.clearlog()
tic("Stacking and meta-learner fitting")

# Initialize stack with all candidates
set.seed(123)
df_stack <- stacks() %>%
 add_candidates(model_set_results)

cat(sprintf("Stack initialized with %d candidate models\n", ncol(df_stack) - 1))

# Blend with non-negative LASSO meta-learner
set.seed(123)
df_stack_blended <- df_stack %>%
 blend_predictions(
   penalty = 10^seq(-4, -0.5, length.out = 200),
   mixture = 1,
   non_negative = TRUE,
   metric = metric_set(rmse)
 )

cat("Stack blended successfully\n")

# Fit final ensemble on full training data
set.seed(123)
df_stack_fitted <- df_stack_blended %>%
 fit_members()

timing_stacking <- toc(log = TRUE, quiet = TRUE)
save_timing("Stacking and meta-learner fitting", timing_stacking)

cat("Final stacked ensemble fitted.\n")
record_memory("After stacking and fitting")

gc() #call garbage collection 
```

# PHASE 6: EVALUATION {.tabset}

## Two-Stage CV Evaluation

```{r two-stage-evaluation}
# ============================================================================
# PRAGMATIC ADAPTATION #3: Two-Stage Evaluation Framework
# ============================================================================
# Stage 1 (completed above): Hyperparameter optimization using grouped CV
# Stage 2 (this chunk): Independent CV evaluation of the fitted ensemble
#
# This avoids the 125-fold computational cost of fully nested CV while still
# producing approximately unbiased performance estimates for the final stack.
# ============================================================================

# Create L1 dataset (base learner predictions + IDs)
l1_data <- df_stack %>%
 mutate(.row = row_number()) %>%
 left_join(
   df %>%
     mutate(.row = row_number()) %>%
     select(.row, patient_id_global, center_id),
   by = ".row"
 )

cat(sprintf("L1 dataset created: %d observations\n", nrow(l1_data)))

# ============================================================================
# METHODOLOGICAL NOTE: Independent Seed for Stage 2 Evaluation
# ============================================================================
# We deliberately use a DIFFERENT seed (456) here than in Stage 1 tuning (123).
#
# Rationale: Stage 2 creates NEW, INDEPENDENT cross-validation folds that were
# never used during hyperparameter optimization. This ensures:
#   1. The meta-learner's penalty (lambda) is evaluated on truly held-out data
#   2. Performance estimates are not contaminated by optimization bias
#   3. The two-stage evaluation maintains methodological independence
#
# Using the same seed would recreate identical fold assignments, defeating the
# purpose of the two-stage evaluation framework described in Methods Section 2.5.2
# ============================================================================
set.seed(456)  # Different seed ensures fold independence from Stage 1
l1_folds <- group_vfold_cv(l1_data, group = center_id, v = 5)

# Get optimal penalty from Stage 1
best_penalty <- df_stack_blended$penalty$penalty
cat(sprintf("Using meta-learner penalty from Stage 1: %.6f\n", best_penalty))

# Define meta-learner spec (using !! to inject the fixed penalty value)
meta_spec <- linear_reg(penalty = !!best_penalty, mixture = 1) %>%
 set_engine("glmnet", lower.limits = 0)

# Meta-learner recipe
meta_recipe <- recipe(motor_score_12m ~ ., data = l1_data) %>%
 update_role(center_id, new_role = "ID") %>%
 update_role(patient_id_global, new_role = "ID") %>%
 update_role(.row, new_role = "ID")

# Create workflow and evaluate
meta_wf <- workflow() %>%
 add_recipe(meta_recipe) %>%
 add_model(meta_spec)

tic.clearlog()
tic("Meta-learner CV evaluation")

set.seed(456)
meta_cv_results <- fit_resamples(
 meta_wf,
 resamples = l1_folds,
 metrics = metric_set(rmse, rsq, mae),
 control = control_resamples(save_pred = TRUE)
)

timing_metacv <- toc(log = TRUE, quiet = TRUE)
save_timing("Meta-learner CV evaluation", timing_metacv)

# Extract predictions
cv_predictions <- tidyr::unnest(meta_cv_results, .predictions)

plot_data_cv <- cv_predictions %>%
 left_join(
   df %>%
     mutate(.row = row_number()) %>%
     select(.row, patient_id_global, baseline_ais),
   by = ".row"
 ) %>%
 select(patient_id_global, motor_score_12m, .pred, baseline_ais)

# Final CV metrics
final_cv_metrics <- collect_metrics(meta_cv_results)

cat(sprintf("Out-of-fold predictions: %d observations\n", nrow(plot_data_cv)))
cat("Stage 2 evaluation complete.\n")
```

## Bootstrap Confidence Intervals

```{r bootstrap-ci}
tic.clearlog()
tic("Bootstrap CIs")

set.seed(789)
bootstrap_metrics <- map_dfr(1:PARAMS$bootstrap_iters, function(i) {
 boot_sample <- plot_data_cv %>%
   slice_sample(prop = 1, replace = TRUE)
 
 tibble(
   iteration = i,
   rmse = rmse_vec(boot_sample$motor_score_12m, boot_sample$.pred),
   mae = mae_vec(boot_sample$motor_score_12m, boot_sample$.pred),
   rsq = rsq_vec(boot_sample$motor_score_12m, boot_sample$.pred)
 )
})

bootstrap_ci <- bootstrap_metrics %>%
 summarize(
   across(c(rmse, mae, rsq),
          list(lower = ~quantile(., 0.025), upper = ~quantile(., 0.975)))
 )

timing_bootstrap <- toc(log = TRUE, quiet = TRUE)
save_timing("Bootstrap CIs", timing_bootstrap)

cat("Bootstrap 95% Confidence Intervals:\n")
print(bootstrap_ci)

gc() #call garbage collection 
```

## Performance Metrics Table

```{r performance-table}
# Get unbiased stack metrics
stack_metrics_formatted <- final_cv_metrics %>%
 filter(.metric %in% c("rmse", "rsq", "mae")) %>%
 mutate(
   ci_lower = mean - 1.96 * std_err,
   ci_upper = mean + 1.96 * std_err,
   `Mean (95% CI)` = case_when(
     .metric == "rsq" ~ sprintf("%.3f [%.3f, %.3f]", mean, ci_lower, ci_upper),
     TRUE ~ sprintf("%.2f [%.2f, %.2f]", mean, ci_lower, ci_upper)
   ),
   Metric = case_when(
     .metric == "mae" ~ "MAE",
     .metric == "rmse" ~ "RMSE",
     .metric == "rsq" ~ "R-squared"
   )
 ) %>%
 select(Metric, `Mean (95% CI)`) %>%
 mutate(Model = "Stacked Ensemble")

# Get base learner metrics (optimistically biased from Stage 1)
base_learners_formatted <- model_set_results %>%
 rank_results(select_best = TRUE) %>%
 filter(.metric %in% c("rmse", "rsq", "mae")) %>%
 select(wflow_id, .metric, mean, std_err) %>%
 mutate(
   Model = case_when(
     stringr::str_detect(wflow_id, "elnet") ~ "Best ElasticNet",
     stringr::str_detect(wflow_id, "rf") ~ "Best Random Forest",
     stringr::str_detect(wflow_id, "xgb") ~ "Best XGBoost",
     stringr::str_detect(wflow_id, "svm") ~ "Best SVM",
     stringr::str_detect(wflow_id, "mars") ~ "Best MARS"
   ),
   ci_lower = mean - 1.96 * std_err,
   ci_upper = mean + 1.96 * std_err,
   `Mean (95% CI)` = case_when(
     .metric == "rsq" ~ sprintf("%.3f [%.3f, %.3f]", mean, ci_lower, ci_upper),
     TRUE ~ sprintf("%.2f [%.2f, %.2f]", mean, ci_lower, ci_upper)
   ),
   Metric = case_when(
     .metric == "mae" ~ "MAE",
     .metric == "rmse" ~ "RMSE",
     .metric == "rsq" ~ "R-squared"
   )
 ) %>%
 select(Model, Metric, `Mean (95% CI)`)

# Combine
n_retained_models <- df_stack_blended$coefs %>%
 broom::tidy() %>%
 filter(term != "(Intercept)", estimate > 0) %>%
 nrow()

full_performance_table <- bind_rows(base_learners_formatted, stack_metrics_formatted) %>%
 pivot_wider(names_from = Metric, values_from = `Mean (95% CI)`) %>%
 mutate(Model = factor(Model, levels = c(
   "Best ElasticNet", "Best Random Forest", "Best XGBoost",
   "Best SVM", "Best MARS", "Stacked Ensemble"
 ))) %>%
 arrange(Model)

# Display table
full_performance_table %>%
 gt() %>%
 tab_header(
   title = "Cross-Validated Performance Metrics",
   subtitle = sprintf("Stacked ensemble (retained %d candidates) vs. best base learners",
                      n_retained_models)
 ) %>%
 tab_footnote(
   footnote = "Base learner metrics are optimistically biased (Stage 1). Stack metrics are unbiased (Stage 2).",
   locations = cells_column_labels(columns = Model)
 ) %>%
 tab_style(
   style = list(cell_fill(color = "#E8F8F5"), cell_text(weight = "bold")),
   locations = cells_body(rows = Model == "Stacked Ensemble")
 )
```

# PHASE 7: EXPLAINABILITY {.tabset}

## Permutation Importance

```{r permutation-importance}
set_monitor_phase("Permutation Importance")

# Create custom predict function for stacked model
predict_function_stack <- function(model, newdata) {
 pred <- predict(model, newdata)
 return(pred$.pred)
}

# Prepare data (exclude outcome, keep all predictors including IDs for recipe)
explain_data <- df %>% select(-motor_score_12m)
explain_y <- df$motor_score_12m

# Create DALEX explainer
set.seed(123)
explainer_stack <- DALEX::explain(
 model = df_stack_fitted,
 data = explain_data,
 y = explain_y,
 predict_function = predict_function_stack,
 verbose = FALSE
)

cat("DALEX explainer created successfully\n")

# Calculate permutation-based variable importance
tic.clearlog()
tic("Permutation importance")

set.seed(123)
vi_permutation <- DALEX::model_parts(
 explainer_stack,
 loss_function = DALEX::loss_root_mean_square,
 B = PARAMS$dalex_b,
 type = "difference"
)

timing_perm <- toc(log = TRUE, quiet = TRUE)
save_timing("Permutation importance", timing_perm)

cat("Permutation importance calculated.\n")
```

## SHAP Values

```{r adaptive-shap-workers}
# 1. Robust RAM Detection Helper
get_system_ram_gb <- function() {
  tryCatch({
    if (Sys.info()[["sysname"]] == "Windows") {
      # Windows: Use wmic
      ram_bytes <- system("wmic computersystem get totalphysicalmemory", intern = TRUE)
      as.numeric(grep("\\d+", ram_bytes, value = TRUE)) / (1024^3)
    } else {
      # Linux/macOS: Use generic detection via system command
      if (file.exists("/proc/meminfo")) {
        # Linux
        mem_info <- readLines("/proc/meminfo")
        total_kb <- as.numeric(gsub("\\D", "", grep("MemTotal", mem_info, value = TRUE)))
        total_kb / (1024^2)
      } else {
        # macOS fallback
        as.numeric(substring(system("sysctl hw.memsize", intern = TRUE), 13)) / (1024^3)
      }
    }
  }, error = function(e) {
    # Fail-safe default: Assume constrained environment (8GB)
    warning("Could not detect system RAM. Defaulting to conservative 8GB limit.")
    return(8) 
  })
}

# 2. Get Metrics
system_ram_gb <- get_system_ram_gb()
model_size_gb <- as.numeric(lobstr::obj_size(df_stack_fitted)) / (1024^3)
safety_threshold_gb <- 8.0 

# 3. Determine Base Worker Count
if (system_ram_gb <= (safety_threshold_gb + 0.5)) {
  # Constrained Mode (<= ~8GB)
  proposed_workers <- min(2, num_workers)
  mode_msg <- "CONSTRAINED MODE (<= 8GB RAM)"
} else {
  # Performance Mode (> 8GB)
  proposed_workers <- min(4, num_workers)
  mode_msg <- "PERFORMANCE MODE (> 8GB RAM)"
}

# 4. Safety Override (The "Anti-Crash" Check)
# In multisession (socket) parallelization, each worker copies the model object.
# We need: (Model_Size * Workers) + Overhead < System RAM
est_memory_load <- model_size_gb * proposed_workers
safe_limit <- system_ram_gb * 0.60 # Leave 40% for OS and overhead

if (est_memory_load > safe_limit) {
  shap_workers <- 1
  warning_msg <- sprintf("Memory safety override triggered. Model (%.2f GB) too large for parallel workers.", model_size_gb)
} else {
  shap_workers <- proposed_workers
  warning_msg <- NULL
}

# 5. Report Configuration
cat(sprintf("\n--- SHAP PARALLEL CONFIGURATION ---\n"))
cat(sprintf("System RAM detected: %.2f GB\n", system_ram_gb))
cat(sprintf("Model object size:   %.2f GB\n", model_size_gb))
cat(sprintf("Mode:                %s\n", mode_msg))
if (!is.null(warning_msg)) cat(sprintf("WARNING: %s\n", warning_msg))
cat(sprintf("Final Worker Count:  %d\n", shap_workers))
cat("-----------------------------------\n\n")

# 6. Apply Configuration
# Clean memory before starting heavy task
gc() 

original_plan <- plan() 
if (shap_workers > 1) {
  plan(multisession, workers = shap_workers)
} else {
  plan(sequential)
}
```


```{r shap-values, eval=TRUE}
# ============================================================================
# PRAGMATIC ADAPTATION #4: Approximate SHAP via fastshap
# ============================================================================
# Trade-off: Approximate vs. exact Shapley values
# Exact SHAP requires 2^p coalition evaluations (for 26 predictors: >67 million
# evaluations per observation). fastshap uses Monte Carlo sampling for ~50x speedup.
# ============================================================================
set_monitor_phase("SHAP Calculation")

# Prediction wrapper
pfun <- function(object, newdata) {
 predict(object, newdata)$.pred
}

tic.clearlog()
tic("SHAP calculation")

set.seed(123)
shap_values <- fastshap::explain(
 object = df_stack_fitted,
 X = explain_data,
 pred_wrapper = pfun,
 nsim = PARAMS$shap_nsim,
 adjust = TRUE,
 parallel = TRUE, 
  # Explicitly export packages required by the Stacked Ensemble prediction method
  .packages = c("stacks", "tidymodels", "dplyr", "ranger", "xgboost", "earth", "kernlab")
)

# Restore the original global parallel plan
plan(original_plan)

timing_shap <- toc(log = TRUE, quiet = TRUE)
save_timing("SHAP calculation", timing_shap)

# Compute mean |SHAP|
shap_importance <- shap_values %>%
 as_tibble() %>%
 summarize(across(everything(), ~mean(abs(.)))) %>%
 pivot_longer(everything(), names_to = "feature", values_to = "mean_abs_shap") %>%
 arrange(desc(mean_abs_shap)) %>%
 slice_head(n = 20)

record_memory("After SHAP calculation")

cat("SHAP values calculated.\n")
```

```{r stop-monitor-and-plot, fig.width=12, fig.height=6}
# ============================================================================
# STOP MONITOR AND GENERATE MEMORY TIMELINE FIGURE
# ============================================================================

# Stop the unified monitor
set_monitor_phase("Complete")
Sys.sleep(2)
file.create("stop_monitoring.file")
Sys.sleep(3)
tryCatch(bg_monitor$kill(), error = function(e) NULL)
if (file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")

# Read the unified log
if (file.exists(MEMORY_LOG_FILE)) {
  memory_timeline <- read.csv(MEMORY_LOG_FILE, stringsAsFactors = FALSE)
  
  # Calculate key metrics
  global_peak_rss_gb <- max(memory_timeline$rss_gb, na.rm = TRUE)
  global_peak_vms_gb <- max(memory_timeline$vms_gb, na.rm = TRUE)
  total_runtime_min <- max(memory_timeline$elapsed_sec, na.rm = TRUE) / 60
  
  # Find phase boundaries for annotation
  phase_changes <- memory_timeline %>%
    mutate(phase_change = phase != lag(phase, default = "")) %>%
    filter(phase_change) %>%
    select(elapsed_sec, phase) %>%
    mutate(elapsed_min = elapsed_sec / 60)
  
  # Print summary
  cat("\n")
  cat("==================================================================\n")
  cat("           GLOBAL MEMORY CONSUMPTION SUMMARY                      \n")
  cat("==================================================================\n")
  cat(sprintf("Peak Physical RAM (RSS): %.2f GB\n", global_peak_rss_gb))
  cat(sprintf("Peak Virtual Memory (VMS): %.2f GB\n", global_peak_vms_gb))
  cat(sprintf("Total Runtime: %.1f minutes (%.2f hours)\n", 
              total_runtime_min, total_runtime_min / 60))
  cat("------------------------------------------------------------------\n")
  cat(sprintf("8GB Feasibility Threshold: %s\n", 
              ifelse(global_peak_rss_gb < 8, "PASSED", "FAILED")))
  cat(sprintf("Headroom: %.2f GB (%.1f%% of 8GB limit)\n",
              8 - global_peak_rss_gb, 
              100 * (8 - global_peak_rss_gb) / 8))
  cat("==================================================================\n\n")
  
  # Store for later use in feasibility summary
  feasibility_result <- list(
    peak_rss_gb = global_peak_rss_gb,
    peak_vms_gb = global_peak_vms_gb,
    runtime_min = total_runtime_min,
    passed_8gb = global_peak_rss_gb < 8
  )
  
  # Create publication-quality figure
  memory_plot <- memory_timeline %>%
    mutate(elapsed_min = elapsed_sec / 60) %>%
    pivot_longer(cols = c(rss_gb, vms_gb), 
                 names_to = "memory_type", 
                 values_to = "gb") %>%
    mutate(memory_type = case_when(
      memory_type == "rss_gb" ~ "Physical RAM (RSS)",
      memory_type == "vms_gb" ~ "Virtual Memory (VMS)"
    )) %>%
    ggplot(aes(x = elapsed_min, y = gb, color = memory_type)) +
    geom_line(linewidth = 0.8, alpha = 0.8) +
    # Add 8GB threshold line
    geom_hline(yintercept = 8, linetype = "dashed", color = "red", linewidth = 1) +
    annotate("text", x = max(memory_timeline$elapsed_sec/60) * 0.02, y = 8.3, 
             label = "8GB Feasibility Threshold", hjust = 0, color = "red", 
             fontface = "bold", size = 3.5) +
    # Add phase boundary lines
    geom_vline(data = phase_changes, aes(xintercept = elapsed_min),
               linetype = "dotted", color = "gray40", alpha = 0.7) +
    # Add phase labels at top
    geom_text(data = phase_changes, 
              aes(x = elapsed_min, y = max(memory_timeline$vms_gb, na.rm = TRUE) * 0.95, 
                  label = phase),
              angle = 90, hjust = 1, vjust = -0.5, size = 3, color = "gray30") +
    scale_color_manual(values = c("Physical RAM (RSS)" = "#2E86AB", 
                                   "Virtual Memory (VMS)" = "#A23B72")) +
    labs(
      title = "Memory Consumption Throughout Stacked Ensemble Workflow",
      subtitle = sprintf("Peak RSS: %.2f GB | Runtime: %.1f min | 8GB Threshold: %s",
                        global_peak_rss_gb, total_runtime_min,
                        ifelse(global_peak_rss_gb < 8, "PASSED", "FAILED")),
      x = "Elapsed Time (minutes)",
      y = "Memory (GB)",
      color = "Memory Type"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(size = 11, color = "gray30"),
      legend.position = "bottom",
      panel.grid.minor = element_blank()
    ) +
    coord_cartesian(ylim = c(0, max(10, global_peak_vms_gb * 1.1)))
  
  print(memory_plot)
  
  # Save high-res version
  ggsave("memory_timeline.png", memory_plot, width = 12, height = 6, dpi = 300)
  cat("Memory timeline figure saved as memory_timeline.png\n")
  
} else {
  warning("Memory log file not found. Monitor may not have run correctly.")
  feasibility_result <- list(
    peak_rss_gb = NA,
    peak_vms_gb = NA,
    runtime_min = NA,
    passed_8gb = NA
  )
}
```


# PHASE 8: FEASIBILITY REPORTING {.tabset}

## Meta-Learner Weights

```{r metalearner-weights, fig.width=10, fig.height=7}
# Extract and visualize meta-learner coefficients

meta_weights <- df_stack_fitted$coefs %>%
 broom::tidy() %>%
 filter(term != "(Intercept)") %>%
 mutate(
   model_type = case_when(
     stringr::str_detect(term, "elnet") ~ "ElasticNet",
     stringr::str_detect(term, "rf") ~ "Random Forest",
     stringr::str_detect(term, "xgb") ~ "XGBoost",
     stringr::str_detect(term, "svm") ~ "SVM",
     stringr::str_detect(term, "mars") ~ "MARS",
     TRUE ~ "Other"
   ),
   estimate = ifelse(estimate > 0, estimate, 0)
 ) %>%
 filter(estimate > 0) %>%
 arrange(desc(estimate))

# Aggregate by model type
weights_by_model <- meta_weights %>%
 group_by(model_type) %>%
 summarize(total_weight = sum(estimate), n_members = n(), .groups = "drop") %>%
 arrange(desc(total_weight))

# Store summary
meta_coef_summary <- tibble(
 total_candidates = ncol(df_stack) - 1,
 selected_members = nrow(meta_weights),
 selection_rate = nrow(meta_weights) / (ncol(df_stack) - 1),
 dominant_model = weights_by_model$model_type[1],
 dominant_weight_pct = 100 * weights_by_model$total_weight[1] / sum(weights_by_model$total_weight)
)

# Visualization
weights_plot_data <- weights_by_model %>%
 mutate(
   pct = 100 * total_weight / sum(total_weight),
   label_text = sprintf("%.1f%%\n%d members", pct, n_members)
 )

ggplot(weights_plot_data,
      aes(x = reorder(model_type, total_weight), y = total_weight, fill = model_type)) +
 geom_col(alpha = 0.85, width = 0.7) +
 geom_text(aes(label = label_text), hjust = -0.1, size = 3.5, fontface = "bold") +
 coord_flip(ylim = c(0, max(weights_plot_data$total_weight) * 1.2)) +
 scale_fill_brewer(palette = "Set2", guide = "none") +
 labs(
   title = "Meta-Learner Weight Distribution by Base Model Type",
   subtitle = sprintf("Non-negative LASSO selected %d of %d candidate models",
                      sum(weights_plot_data$n_members), ncol(df_stack) - 1),
   x = "Base Model Type",
   y = "Total Weight (Sum of LASSO Coefficients)"
 ) +
 theme_minimal() +
 theme(
   plot.title = element_text(face = "bold", size = 14),
   plot.subtitle = element_text(size = 10, color = "gray40"),
   axis.title = element_text(size = 11, face = "bold"),
   panel.grid.major.y = element_blank()
 )
```

```{r feasibility-tradeoff-figure, fig.width=10, fig.height=7}
# ============================================================================
# FEASIBILITY TRADE-OFF FIGURE
# ============================================================================
# Shows the performance-computation trade-off: what you get for your compute
# ============================================================================

# Extract per-model best performance
model_performance <- model_set_results %>%
  rank_results(select_best = TRUE) %>%
  filter(.metric == "rmse") %>%
  select(wflow_id, mean, std_err) %>%
  mutate(
    model = case_when(
      str_detect(wflow_id, "elnet") ~ "ElasticNet",
      str_detect(wflow_id, "rf") ~ "Random Forest",
      str_detect(wflow_id, "xgb") ~ "XGBoost",
      str_detect(wflow_id, "svm") ~ "SVM",
      str_detect(wflow_id, "mars") ~ "MARS"
    ),
    ci_lower = mean - 1.96 * std_err,
    ci_upper = mean + 1.96 * std_err,
    type = "Base Learner"
  )

# Add stack performance
stack_perf <- final_cv_metrics %>%
  filter(.metric == "rmse") %>%
  mutate(
    model = "Stacked Ensemble",
    ci_lower = mean - 1.96 * std_err,
    ci_upper = mean + 1.96 * std_err,
    type = "Ensemble",
    wflow_id = "stack"
  ) %>%
  select(wflow_id, mean, std_err, model, ci_lower, ci_upper, type)

# Get training episodes per model
training_effort <- training_counts %>%
  mutate(model = Model) %>%
  select(model, Total_Models)

# Combine
plot_data <- bind_rows(model_performance, stack_perf) %>%
  left_join(training_effort, by = "model") %>%
  mutate(
    # Stack uses all training episodes
    Total_Models = ifelse(model == "Stacked Ensemble", 
                          total_training_episodes, 
                          Total_Models)
  )

# Create figure
tradeoff_plot <- ggplot(plot_data, aes(x = Total_Models, y = mean, color = type)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, linewidth = 1) +
  geom_text(aes(label = model), vjust = -1.5, hjust = 0.5, size = 3.5) +
  scale_color_manual(values = c("Base Learner" = "#2E86AB", "Ensemble" = "#E94F37")) +
  labs(
    title = "Performance vs. Computational Investment",
    subtitle = "Trade-off between model training effort and prediction accuracy",
    x = "Training Episodes (model fits)",
    y = "RMSE (lower is better)",
    color = "Model Type"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11, color = "gray30"),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  ) +
  # Expand x-axis slightly for labels
  scale_x_continuous(expand = expansion(mult = c(0.05, 0.15)))

print(tradeoff_plot)

# Save
ggsave("feasibility_tradeoff.png", tradeoff_plot, width = 10, height = 7, dpi = 300)
cat("Feasibility trade-off figure saved.\n")
```

## Deployment Metrics

```{r deployment-metrics}
# ============================================================================
# DEPLOYMENT FEASIBILITY METRICS
# ============================================================================

# 1. Model object size (RAM footprint for inference)
model_ram_size_mb <- as.numeric(lobstr::obj_size(df_stack_fitted)) / (1024^2)

# 2. Serialized file size (disk/transfer footprint)
saveRDS(df_stack_fitted, "temp_final_model.rds", compress = "xz")
model_disk_size_mb <- file.size("temp_final_model.rds") / (1024^2)
file.remove("temp_final_model.rds")
gc()

# 3. Single-patient inference latency
new_patient_profile <- df[1, ] %>% select(-motor_score_12m)

# Warm-up run
invisible(predict(df_stack_fitted, new_patient_profile))

# Benchmark (100 iterations)
tic.clearlog()
tic("inference_batch")
for(i in 1:100) {
 pred <- predict(df_stack_fitted, new_patient_profile)
}
t_inference <- toc(log = TRUE, quiet = TRUE)

latency_ms <- ((t_inference$toc - t_inference$tic) / 100) * 1000

# Store results
deployment_metrics <- tibble(
 Metric = c("Model Object Size (RAM)", "Serialized Model Size (Disk)", "Inference Latency"),
 Value = c(model_ram_size_mb, model_disk_size_mb, latency_ms),
 Unit = c("MB", "MB", "ms"),
 Implication = c(
   "Minimum RAM required to load model",
   "Bandwidth/storage for deployment",
   "Time per single prediction"
 )
)

timing_results <<- timing_results %>%
 add_row(
   component = "Single-Patient Inference (avg ms)",
   wall_time_seconds = latency_ms / 1000,
   wall_time_formatted = sprintf("%.2f ms", latency_ms)
 )

cat("\n=== DEPLOYMENT FEASIBILITY ===\n")
print(deployment_metrics)
cat(sprintf("\nThroughput: ~%.0f predictions per second\n", 1000/latency_ms))
```

## Environment Footprint

```{r environment-footprint}
# Calculate total package dependency footprint

core_pkgs <- c("tidymodels", "stacks", "glmnet", "ranger", "xgboost",
               "kernlab", "earth", "doFuture", "tictoc", "pryr",
               "dplyr", "tidyr", "purrr", "tibble", "stringr", "forcats",
               "ggplot2", "gtsummary", "gt", "broom", "viridis",
               "DALEX", "DALEXtra", "fastshap", "yardstick", "probably",
               "callr", "ps")

# FIX: Use local installed packages database instead of querying CRAN
# This prevents the "CRAN mirror" error and works offline
local_db <- installed.packages()

all_deps <- tools::package_dependencies(core_pkgs, db = local_db, recursive = TRUE)
all_pkgs_unique <- unique(c(core_pkgs, unlist(all_deps)))

# Filter to ensure we only look for packages that actually exist in the local library
# (This handles cases where a dependency might be optional or missing)
all_pkgs_unique <- all_pkgs_unique[all_pkgs_unique %in% local_db[, "Package"]]

# Estimate installation size
total_size_bytes <- 0
for (pkg in all_pkgs_unique) {
  loc <- system.file(package = pkg)
  if (loc != "") {
    files <- list.files(loc, recursive = TRUE, full.names = TRUE)
    total_size_bytes <- total_size_bytes + sum(file.info(files)$size, na.rm = TRUE)
  }
}
total_size_mb <- total_size_bytes / (1024^2)

cat(sprintf("\n=== ENVIRONMENT FOOTPRINT ===\n"))
cat(sprintf("Total dependencies: %d packages\n", length(all_pkgs_unique)))
cat(sprintf("Approximate installation size: %.2f MB\n", total_size_mb))
```

## System RAM Profile

```{r system-ram-profile}
# Process monitoring logs and extract peak values

get_peak_ram <- function(logfile, stage_name) {
 if (file.exists(logfile)) {
   df_log <- read.csv(logfile)
   if ("rss_gb" %in% names(df_log)) {
     peak_rss <- max(df_log$rss_gb, na.rm = TRUE)
     peak_vms <- max(df_log$vms_gb, na.rm = TRUE)
   } else {
     peak_rss <- NA_real_
     peak_vms <- NA_real_
   }
   return(tibble(Stage = stage_name, Peak_RSS_GB = peak_rss, Peak_VMS_GB = peak_vms))
 } else {
   return(tibble(Stage = stage_name, Peak_RSS_GB = NA_real_, Peak_VMS_GB = NA_real_))
 }
}

# Read all logs
system_peaks <- bind_rows(
 get_peak_ram("memory_log_tuning.csv", "Base Learner Tuning"),
 get_peak_ram("memory_log_stacking.csv", "Stacking & Refitting"),
 get_peak_ram("memory_log_shap.csv", "Explainability (SHAP)")
)

# Add to memory_results
for (i in 1:nrow(system_peaks)) {
 if (!is.na(system_peaks$Peak_RSS_GB[i])) {
   memory_results <<- memory_results %>%
     add_row(
       component = paste("Peak Physical RAM (RSS):", system_peaks$Stage[i]),
       memory_mb = system_peaks$Peak_RSS_GB[i] * 1024,
       memory_gb = system_peaks$Peak_RSS_GB[i]
     )
 }
}

cat("\n=== SYSTEM RAM PROFILE ===\n")
print(system_peaks)
```

## Computational ROI

```{r computational-roi}
# Calculate return on compute investment

tuning_time_sec <- timing_results %>%
 filter(component == "Base learners tuning") %>%
 pull(wall_time_seconds)

# Best single model performance
best_single_model_data <- model_set_results %>%
 rank_results(select_best = TRUE) %>%
 filter(.metric == "rmse") %>%
 slice_min(mean, n = 1)

single_rmse <- best_single_model_data$mean
single_model_name <- case_when(
 str_detect(best_single_model_data$wflow_id, "elnet") ~ "ElasticNet",
 str_detect(best_single_model_data$wflow_id, "rf") ~ "Random Forest",
 str_detect(best_single_model_data$wflow_id, "xgb") ~ "XGBoost",
 str_detect(best_single_model_data$wflow_id, "svm") ~ "SVM",
 str_detect(best_single_model_data$wflow_id, "mars") ~ "MARS"
)

# Stack performance
stack_rmse <- final_cv_metrics %>% filter(.metric == "rmse") %>% pull(mean)
stack_total_time_sec <- sum(timing_results$wall_time_seconds)
stack_total_time_min <- stack_total_time_sec / 60

# Single algorithm time estimate
single_algo_time_min <- (tuning_time_sec / 60) / 5
single_algo_time_sec <- tuning_time_sec / 5

# Compute efficiency
rmse_improvement_abs <- single_rmse - stack_rmse
rmse_improvement_pct <- 100 * rmse_improvement_abs / single_rmse
extra_time_min <- stack_total_time_min - single_algo_time_min
efficiency_ratio <- rmse_improvement_pct / extra_time_min

cat("\n=== COMPUTATIONAL RETURN ON INVESTMENT ===\n\n")
cat(sprintf("BASELINE (Best Single Model: %s):\n", single_model_name))
cat(sprintf("  RMSE: %.4f\n", single_rmse))
cat(sprintf("  Estimated time: %.1f minutes\n\n", single_algo_time_min))

cat("STACKED ENSEMBLE:\n")
cat(sprintf("  RMSE: %.4f\n", stack_rmse))
cat(sprintf("  Total time: %.1f minutes (%.2f hours)\n\n",
           stack_total_time_min, stack_total_time_min / 60))

cat("COMPARISON:\n")
cat(sprintf("  Performance gain: %.2f%% RMSE improvement\n", rmse_improvement_pct))
cat(sprintf("  Time multiplier: %.1fx increase\n", stack_total_time_sec / single_algo_time_sec))
cat(sprintf("  Extra time invested: %.1f minutes\n\n", extra_time_min))

cat("EFFICIENCY RATIO:\n")
cat(sprintf("  %.4f%% RMSE improvement per minute of extra computation\n", efficiency_ratio))
```

## Theoretical Projections

```{r theoretical-projections}
# Project time requirements for more rigorous approaches

nested_cv_multiplier <- 125 / (PARAMS$cv_folds * PARAMS$cv_repeats)
nested_cv_proj_hours <- (tuning_time_sec * nested_cv_multiplier) / 3600
current_avg_grid <- mean(training_counts$Grid_Size)
large_grid_multiplier <- 50 / current_avg_grid
large_grid_proj_hours <- (tuning_time_sec * large_grid_multiplier) / 3600

theoretical_projections <- tibble(
 Scenario = c("Nested CV (5x5x5)", "Larger Grids (50 configs/algorithm)"),
 Current_Hours = c(tuning_time_sec / 3600, tuning_time_sec / 3600),
 Multiplier = c(nested_cv_multiplier, large_grid_multiplier),
 Projected_Hours = c(nested_cv_proj_hours, large_grid_proj_hours)
)

cat("\n=== THEORETICAL TIME PROJECTIONS ===\n")
print(theoretical_projections)

cat(sprintf("\nInterpretation:\n"))
cat(sprintf("  Current workflow: %.2f hours\n", tuning_time_sec / 3600))
cat(sprintf("  Nested CV would require: %.2f hours (%.1fx increase)\n",
           nested_cv_proj_hours, nested_cv_multiplier))
cat(sprintf("  Standard grids would require: %.2f hours (%.1fx increase)\n",
           large_grid_proj_hours, large_grid_multiplier))
```

## Final Feasibility Summary

```{r feasibility-summary}
# ============================================================================
# DYNAMIC FEASIBILITY SUMMARY
# ============================================================================

peak_memory_gb <- ifelse(exists("feasibility_result") && !is.na(feasibility_result$peak_rss_gb),
                         feasibility_result$peak_rss_gb,
                         max(memory_results$memory_gb, na.rm = TRUE))

total_time_hours <- sum(timing_results$wall_time_seconds, na.rm = TRUE) / 3600
total_time_min <- sum(timing_results$wall_time_seconds, na.rm = TRUE) / 60

cat("==================================================================\n")
cat("           WORKFLOW FEASIBILITY RESULTS                          \n")
cat("==================================================================\n\n")

cat("1. WORKFLOW COMPLETION: SUCCESS\n")
cat("   The complete stacked ensemble workflow executed successfully on\n")
cat("   standard laptop hardware without cloud computing or GPU acceleration.\n\n")

cat("2. RESOURCE CONSUMPTION:\n")
cat(sprintf("   - Total execution time: %.1f minutes (%.2f hours)\n", 
            total_time_min, total_time_hours))
cat(sprintf("   - Peak physical RAM (RSS): %.2f GB\n", peak_memory_gb))
cat(sprintf("   - 8GB feasibility threshold: %s\n",
            ifelse(peak_memory_gb < 8, "PASSED", "FAILED")))
cat(sprintf("   - Headroom remaining: %.2f GB\n\n", 8 - peak_memory_gb))

cat("3. TRAINING COMPLEXITY:\n")
cat(sprintf("   - Total model training episodes: %d\n", total_training_episodes))
cat(sprintf("   - Cross-validation configuration: %d-fold x %d repeats\n",
            PARAMS$cv_folds, PARAMS$cv_repeats))
cat(sprintf("   - Base learner candidates generated: %d\n", ncol(df_stack) - 1))
cat(sprintf("   - Members selected by meta-learner: %d (%.1f%% selection rate)\n\n",
            meta_coef_summary$selected_members,
            100 * meta_coef_summary$selection_rate))

cat("4. ENSEMBLE COMPOSITION:\n")
cat(sprintf("   - Dominant model type: %s (%.1f%% of weight)\n",
            meta_coef_summary$dominant_model,
            meta_coef_summary$dominant_weight_pct))

cat("\n5. DEPLOYMENT FEASIBILITY:\n")
cat(sprintf("   - Model RAM footprint: %.2f MB\n", 
            deployment_metrics$Value[deployment_metrics$Metric == "Model Object Size (RAM)"]))
cat(sprintf("   - Serialized model size: %.2f MB\n",
            deployment_metrics$Value[deployment_metrics$Metric == "Serialized Model Size (Disk)"]))
cat(sprintf("   - Inference latency: %.2f ms per prediction\n",
            deployment_metrics$Value[deployment_metrics$Metric == "Inference Latency"]))

cat("\n==================================================================\n")
cat("                         CONCLUSION                              \n")
cat("==================================================================\n")
if (peak_memory_gb < 8 && total_time_hours < 8) {
  cat("All workflow components completed successfully within the defined\n")
  cat("resource constraints (8GB RAM, overnight execution). This demonstrates\n")
  cat("that stacked ensemble methods are feasible for deployment in\n")
  cat("resource-constrained research settings.\n")
} else {
  cat("WARNING: Workflow exceeded one or more feasibility thresholds.\n")
  cat(sprintf("   - RAM: %.2f GB (threshold: 8GB) - %s\n", 
              peak_memory_gb, ifelse(peak_memory_gb < 8, "OK", "EXCEEDED")))
  cat(sprintf("   - Time: %.2f hours (threshold: 8 hours) - %s\n",
              total_time_hours, ifelse(total_time_hours < 8, "OK", "EXCEEDED")))
}
cat("==================================================================\n")
```

## Timing Summary Table

```{r timing-summary-table}
timing_results %>%
 arrange(desc(wall_time_seconds)) %>%
 mutate(pct_total = 100 * wall_time_seconds / sum(wall_time_seconds)) %>%
 gt() %>%
 tab_header(
   title = "Computational Time Requirements",
   subtitle = sprintf("Total workflow execution time: %.2f hours",
                      sum(timing_results$wall_time_seconds) / 3600)
 ) %>%
 fmt_number(columns = wall_time_seconds, decimals = 1) %>%
 fmt_percent(columns = pct_total, decimals = 1, scale_values = FALSE) %>%
 cols_label(
   component = "Workflow Component",
   wall_time_seconds = "Time (seconds)",
   wall_time_formatted = "Time (formatted)",
   pct_total = "% of Total"
 )
```

## Session Information

```{r session-info}
cat("=== COMPUTATIONAL ENVIRONMENT ===\n\n")
cat("R version:", R.version$version.string, "\n")
cat("Platform:", R.version$platform, "\n")
cat("Running under:", Sys.info()["sysname"], Sys.info()["release"], "\n")
cat("Date executed:", format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z"), "\n\n")

cat("=== KEY ML PACKAGES ===\n\n")
pkgs <- c("tidymodels", "tune", "stacks", "recipes", "parsnip",
         "ranger", "xgboost", "earth", "glmnet", "kernlab",
         "DALEX", "fastshap", "yardstick")
for(pkg in pkgs) {
 if(requireNamespace(pkg, quietly = TRUE)) {
   cat(sprintf("  %s: %s\n", pkg, as.character(packageVersion(pkg))))
 }
}

print(sessionInfo())
```

## Cleanup

```{r cleanup}
# Remove monitoring logs
for (log_file in c("memory_log_tuning.csv", "memory_log_stacking.csv",
                  "memory_log_shap.csv", "stop_monitoring.file")) {
 if (file.exists(log_file)) file.remove(log_file)
}
cat("Monitoring logs cleaned up.\n")
```

# PHASE 9: REPORTING {.tabset}

```{r results-extraction-for-claude, echo=TRUE}
# ============================================================================
# AUTO-DETECT HARDWARE PROFILE
# ============================================================================

# Determine which laptop this is running on based on available RAM
# This creates a filename suffix for the output
total_ram_estimate <- as.numeric(pryr::mem_used()) * 10  # Rough estimate
hardware_label <- ifelse(

  as.numeric(Sys.getenv("LAPTOP_RAM", unset = "8")) >= 16 ||
  grepl("i7|i9|Ryzen 7|Ryzen 9", Sys.info()["machine"], ignore.case = TRUE),
  "16GB",
  "8GB"
)

# Allow manual override via environment variable
if (Sys.getenv("LAPTOP_RAM") != "") {

  hardware_label <- paste0(Sys.getenv("LAPTOP_RAM"), "GB")
}

# Create output filename
output_filename <- sprintf("results_extraction_%s_%s.txt", 
                           hardware_label,
                           format(Sys.time(), "%Y%m%d_%H%M%S"))

cat(sprintf("Hardware detected/set: %s laptop\n", hardware_label))
cat(sprintf("Output file: %s\n\n", output_filename))

# ============================================================================
# BEGIN EXTRACTION
# ============================================================================

# Open connection for writing
sink(output_filename)

cat("================================================================================\n")
cat("STACKED ENSEMBLE FEASIBILITY STUDY - RESULTS EXTRACTION\n")
cat("================================================================================\n")
cat(sprintf("Hardware Profile: %s LAPTOP\n", hardware_label))
cat(sprintf("Extraction Timestamp: %s\n", format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z")))
cat("================================================================================\n\n")

# ============================================================================
# SECTION 1: HARDWARE SPECIFICATIONS
# ============================================================================
cat("==== SECTION 1: HARDWARE SPECIFICATIONS ====\n\n")

cat(sprintf("CPU_PHYSICAL_CORES: %d\n", parallel::detectCores(logical = FALSE)))
cat(sprintf("CPU_LOGICAL_THREADS: %d\n", parallel::detectCores(logical = TRUE)))
cat(sprintf("PARALLEL_WORKERS_USED: %d\n", num_workers))
cat(sprintf("R_VERSION: %s\n", R.version$version.string))
cat(sprintf("PLATFORM: %s\n", R.version$platform))
cat(sprintf("OPERATING_SYSTEM: %s %s\n", Sys.info()["sysname"], Sys.info()["release"]))
cat(sprintf("RAM_CATEGORY: %s\n", hardware_label))

cat("\n")

# ============================================================================
# SECTION 2: DATA CHARACTERISTICS
# ============================================================================
cat("==== SECTION 2: DATA CHARACTERISTICS ====\n\n")

cat(sprintf("N_OBSERVATIONS: %d\n", nrow(df)))
cat(sprintf("N_VARIABLES: %d\n", ncol(df)))
cat(sprintf("N_CENTERS: %d\n", length(unique(df$center_id))))
cat(sprintf("OUTCOME_MIN: %.0f\n", min(df$motor_score_12m)))
cat(sprintf("OUTCOME_MAX: %.0f\n", max(df$motor_score_12m)))
cat(sprintf("OUTCOME_MEAN: %.2f\n", mean(df$motor_score_12m)))
cat(sprintf("OUTCOME_SD: %.2f\n", sd(df$motor_score_12m)))
cat(sprintf("OUTCOME_MEDIAN: %.1f\n", median(df$motor_score_12m)))

cat("\n")

# ============================================================================
# SECTION 3: CONFIGURATION PARAMETERS
# ============================================================================
cat("==== SECTION 3: CONFIGURATION PARAMETERS ====\n\n")

cat(sprintf("CV_FOLDS: %d\n", PARAMS$cv_folds))
cat(sprintf("CV_REPEATS: %d\n", PARAMS$cv_repeats))
cat(sprintf("TOTAL_RESAMPLES: %d\n", PARAMS$cv_folds * PARAMS$cv_repeats))
cat(sprintf("GRID_SIZE: %d\n", PARAMS$grid_size))
cat(sprintf("BOOTSTRAP_ITERATIONS: %d\n", PARAMS$bootstrap_iters))
cat(sprintf("SHAP_NSIM: %d\n", PARAMS$shap_nsim))
cat(sprintf("DALEX_PERMUTATIONS: %d\n", PARAMS$dalex_b))

cat("\n")

# ============================================================================
# SECTION 4: TRAINING COMPLEXITY
# ============================================================================
cat("==== SECTION 4: TRAINING COMPLEXITY ====\n\n")

cat(sprintf("TOTAL_TRAINING_EPISODES: %d\n", total_training_episodes))
cat(sprintf("BASE_LEARNER_CANDIDATES: %d\n", ncol(df_stack) - 1))

cat("\nTRAINING_COUNTS_BY_MODEL:\n")
for (i in 1:nrow(training_counts)) {
  cat(sprintf("  %s: grid=%d, episodes=%d\n", 
              training_counts$Model[i],
              training_counts$Grid_Size[i],
              training_counts$Total_Models[i]))
}

cat("\n")

# ============================================================================
# SECTION 5: ENSEMBLE COMPOSITION
# ============================================================================
cat("==== SECTION 5: ENSEMBLE COMPOSITION ====\n\n")

# Extract meta-learner coefficients
meta_coefs <- df_stack_blended$coefs %>%
  broom::tidy() %>%
  filter(term != "(Intercept)", estimate > 0) %>%
  arrange(desc(estimate))

# Aggregate by model type
weights_by_type <- meta_coefs %>%
  mutate(
    model_type = case_when(
      str_detect(term, "elnet") ~ "ElasticNet",
      str_detect(term, "rf") ~ "RandomForest",
      str_detect(term, "xgb") ~ "XGBoost",
      str_detect(term, "svm") ~ "SVM",
      str_detect(term, "mars") ~ "MARS",
      TRUE ~ "Other"
    )
  ) %>%
  group_by(model_type) %>%
  summarize(
    n_members = n(),
    total_weight = sum(estimate),
    .groups = "drop"
  ) %>%
  mutate(pct_weight = 100 * total_weight / sum(total_weight)) %>%
  arrange(desc(total_weight))

cat(sprintf("OPTIMAL_LASSO_PENALTY: %.8f\n", df_stack_blended$penalty$penalty))
cat(sprintf("MEMBERS_SELECTED: %d\n", nrow(meta_coefs)))
cat(sprintf("CANDIDATES_TOTAL: %d\n", ncol(df_stack) - 1))
cat(sprintf("SELECTION_RATE: %.4f\n", nrow(meta_coefs) / (ncol(df_stack) - 1)))
cat(sprintf("SELECTION_RATE_PCT: %.1f\n", 100 * nrow(meta_coefs) / (ncol(df_stack) - 1)))

cat("\nWEIGHTS_BY_MODEL_TYPE:\n")
for (i in 1:nrow(weights_by_type)) {
  cat(sprintf("  %s: n_members=%d, weight=%.4f, pct=%.1f\n",
              weights_by_type$model_type[i],
              weights_by_type$n_members[i],
              weights_by_type$total_weight[i],
              weights_by_type$pct_weight[i]))
}

cat(sprintf("\nDOMINANT_MODEL: %s\n", weights_by_type$model_type[1]))
cat(sprintf("DOMINANT_MODEL_WEIGHT_PCT: %.1f\n", weights_by_type$pct_weight[1]))

cat("\n")

# ============================================================================
# SECTION 6: PERFORMANCE METRICS - STACKED ENSEMBLE (STAGE 2)
# ============================================================================
cat("==== SECTION 6: STACKED ENSEMBLE PERFORMANCE (STAGE 2 - UNBIASED) ====\n\n")

stack_rmse <- final_cv_metrics %>% filter(.metric == "rmse")
stack_mae <- final_cv_metrics %>% filter(.metric == "mae")
stack_rsq <- final_cv_metrics %>% filter(.metric == "rsq")

cat(sprintf("STACK_RMSE_MEAN: %.4f\n", stack_rmse$mean))
cat(sprintf("STACK_RMSE_SE: %.4f\n", stack_rmse$std_err))
cat(sprintf("STACK_RMSE_CI_LOWER: %.4f\n", stack_rmse$mean - 1.96 * stack_rmse$std_err))
cat(sprintf("STACK_RMSE_CI_UPPER: %.4f\n", stack_rmse$mean + 1.96 * stack_rmse$std_err))

cat(sprintf("STACK_MAE_MEAN: %.4f\n", stack_mae$mean))
cat(sprintf("STACK_MAE_SE: %.4f\n", stack_mae$std_err))
cat(sprintf("STACK_MAE_CI_LOWER: %.4f\n", stack_mae$mean - 1.96 * stack_mae$std_err))
cat(sprintf("STACK_MAE_CI_UPPER: %.4f\n", stack_mae$mean + 1.96 * stack_mae$std_err))

cat(sprintf("STACK_RSQ_MEAN: %.4f\n", stack_rsq$mean))
cat(sprintf("STACK_RSQ_SE: %.4f\n", stack_rsq$std_err))
cat(sprintf("STACK_RSQ_CI_LOWER: %.4f\n", stack_rsq$mean - 1.96 * stack_rsq$std_err))
cat(sprintf("STACK_RSQ_CI_UPPER: %.4f\n", stack_rsq$mean + 1.96 * stack_rsq$std_err))

cat("\n")

# ============================================================================
# SECTION 7: BASE LEARNER PERFORMANCE (STAGE 1 - BIASED)
# ============================================================================
cat("==== SECTION 7: BASE LEARNER PERFORMANCE (STAGE 1 - BIASED) ====\n\n")

base_learner_best <- model_set_results %>%
  rank_results(select_best = TRUE) %>%
  filter(.metric == "rmse") %>%
  select(wflow_id, .metric, mean, std_err) %>%
  mutate(
    model = case_when(
      str_detect(wflow_id, "elnet") ~ "ElasticNet",
      str_detect(wflow_id, "rf") ~ "RandomForest",
      str_detect(wflow_id, "xgb") ~ "XGBoost",
      str_detect(wflow_id, "svm") ~ "SVM",
      str_detect(wflow_id, "mars") ~ "MARS"
    ),
    ci_lower = mean - 1.96 * std_err,
    ci_upper = mean + 1.96 * std_err
  ) %>%
  arrange(mean)

cat("BASE_LEARNERS_RMSE:\n")
for (i in 1:nrow(base_learner_best)) {
  cat(sprintf("  %s: mean=%.4f, se=%.4f, ci_lower=%.4f, ci_upper=%.4f\n",
              base_learner_best$model[i],
              base_learner_best$mean[i],
              base_learner_best$std_err[i],
              base_learner_best$ci_lower[i],
              base_learner_best$ci_upper[i]))
}

cat(sprintf("\nBEST_SINGLE_MODEL: %s\n", base_learner_best$model[1]))
cat(sprintf("BEST_SINGLE_RMSE: %.4f\n", base_learner_best$mean[1]))
cat(sprintf("BEST_SINGLE_CI_LOWER: %.4f\n", base_learner_best$ci_lower[1]))
cat(sprintf("BEST_SINGLE_CI_UPPER: %.4f\n", base_learner_best$ci_upper[1]))

cat("\n")

# ============================================================================
# SECTION 8: STACK VS BEST SINGLE MODEL COMPARISON
# ============================================================================
cat("==== SECTION 8: STACK VS BEST SINGLE MODEL COMPARISON ====\n\n")

improvement_abs <- base_learner_best$mean[1] - stack_rmse$mean
improvement_pct <- 100 * improvement_abs / base_learner_best$mean[1]

cat(sprintf("RMSE_IMPROVEMENT_ABSOLUTE: %.4f\n", improvement_abs))
cat(sprintf("RMSE_IMPROVEMENT_PERCENT: %.2f\n", improvement_pct))
cat(sprintf("STACK_BETTER_THAN_BEST_SINGLE: %s\n", ifelse(improvement_abs > 0, "TRUE", "FALSE")))
cat("COMPARISON_NOTE: Conservative comparison - Stage 1 base learner (biased) vs Stage 2 stack (unbiased)\n")

cat("\n")

# ============================================================================
# SECTION 9: BOOTSTRAP CONFIDENCE INTERVALS
# ============================================================================
cat("==== SECTION 9: BOOTSTRAP CONFIDENCE INTERVALS ====\n\n")

cat(sprintf("BOOTSTRAP_N_ITERATIONS: %d\n", PARAMS$bootstrap_iters))
cat(sprintf("BOOTSTRAP_RMSE_CI_LOWER: %.4f\n", bootstrap_ci$rmse_lower))
cat(sprintf("BOOTSTRAP_RMSE_CI_UPPER: %.4f\n", bootstrap_ci$rmse_upper))
cat(sprintf("BOOTSTRAP_MAE_CI_LOWER: %.4f\n", bootstrap_ci$mae_lower))
cat(sprintf("BOOTSTRAP_MAE_CI_UPPER: %.4f\n", bootstrap_ci$mae_upper))
cat(sprintf("BOOTSTRAP_RSQ_CI_LOWER: %.4f\n", bootstrap_ci$rsq_lower))
cat(sprintf("BOOTSTRAP_RSQ_CI_UPPER: %.4f\n", bootstrap_ci$rsq_upper))

cat("\n")

# ============================================================================
# SECTION 10: SUBGROUP ANALYSIS BY AIS GRADE
# ============================================================================
cat("==== SECTION 10: SUBGROUP ANALYSIS BY AIS GRADE ====\n\n")

if (exists("subgroup_metrics")) {
  cat("SUBGROUP_METRICS:\n")
  for (i in 1:nrow(subgroup_metrics)) {
    cat(sprintf("  AIS_%s: n=%d, rmse=%.4f, mae=%.4f, rsq=%.4f\n",
                subgroup_metrics$baseline_ais[i],
                subgroup_metrics$n[i],
                subgroup_metrics$rmse[i],
                subgroup_metrics$mae[i],
                subgroup_metrics$rsq[i]))
  }
} else {
  cat("SUBGROUP_METRICS: NOT_AVAILABLE\n")
}

cat("\n")

# ============================================================================
# SECTION 11: TIMING RESULTS
# ============================================================================
cat("==== SECTION 11: TIMING RESULTS ====\n\n")

total_time_sec <- sum(timing_results$wall_time_seconds)
total_time_min <- total_time_sec / 60
total_time_hours <- total_time_sec / 3600

cat(sprintf("TOTAL_TIME_SECONDS: %.1f\n", total_time_sec))
cat(sprintf("TOTAL_TIME_MINUTES: %.2f\n", total_time_min))
cat(sprintf("TOTAL_TIME_HOURS: %.4f\n", total_time_hours))
cat(sprintf("UNDER_8_HOURS: %s\n", ifelse(total_time_hours < 8, "TRUE", "FALSE")))

cat("\nTIMING_BY_COMPONENT:\n")
timing_with_pct <- timing_results %>%
  mutate(pct = 100 * wall_time_seconds / sum(wall_time_seconds)) %>%
  arrange(desc(wall_time_seconds))

for (i in 1:nrow(timing_with_pct)) {
  cat(sprintf("  %s: seconds=%.1f, minutes=%.2f, pct=%.1f\n",
              timing_with_pct$component[i],
              timing_with_pct$wall_time_seconds[i],
              timing_with_pct$wall_time_seconds[i] / 60,
              timing_with_pct$pct[i]))
}

# Extract key timing values
tuning_time_sec <- timing_results %>% 
  filter(component == "Base learners tuning") %>% 
  pull(wall_time_seconds)
stacking_time_sec <- timing_results %>% 
  filter(component == "Stacking and meta-learner fitting") %>% 
  pull(wall_time_seconds)

cat(sprintf("\nTUNING_TIME_SECONDS: %.1f\n", tuning_time_sec))
cat(sprintf("TUNING_TIME_MINUTES: %.2f\n", tuning_time_sec / 60))
cat(sprintf("TUNING_PCT_OF_TOTAL: %.1f\n", 100 * tuning_time_sec / total_time_sec))
cat(sprintf("STACKING_TIME_SECONDS: %.1f\n", stacking_time_sec))
cat(sprintf("STACKING_TIME_MINUTES: %.2f\n", stacking_time_sec / 60))

cat("\n")

# ============================================================================
# SECTION 12: MEMORY CONSUMPTION
# ============================================================================
cat("==== SECTION 12: MEMORY CONSUMPTION ====\n\n")

# Get peak from unified monitor if available
if (exists("feasibility_result") && !is.null(feasibility_result$peak_rss_gb)) {
  peak_rss <- feasibility_result$peak_rss_gb
  peak_vms <- feasibility_result$peak_vms_gb
} else {
  peak_rss <- max(memory_results$memory_gb, na.rm = TRUE)
  peak_vms <- NA
}

cat(sprintf("PEAK_RSS_GB: %.4f\n", peak_rss))
if (!is.na(peak_vms)) {
  cat(sprintf("PEAK_VMS_GB: %.4f\n", peak_vms))
} else {
  cat("PEAK_VMS_GB: NA\n")
}
cat(sprintf("UNDER_8GB_THRESHOLD: %s\n", ifelse(peak_rss < 8, "TRUE", "FALSE")))
cat(sprintf("HEADROOM_GB: %.4f\n", 8 - peak_rss))
cat(sprintf("HEADROOM_PCT: %.1f\n", 100 * (8 - peak_rss) / 8))

cat("\nMEMORY_BY_COMPONENT:\n")
for (i in 1:nrow(memory_results)) {
  cat(sprintf("  %s: %.4f GB\n",
              memory_results$component[i],
              memory_results$memory_gb[i]))
}

cat("\n")

# ============================================================================
# SECTION 13: DEPLOYMENT METRICS
# ============================================================================
cat("==== SECTION 13: DEPLOYMENT METRICS ====\n\n")

model_ram_mb <- deployment_metrics$Value[deployment_metrics$Metric == "Model Object Size (RAM)"]
model_disk_mb <- deployment_metrics$Value[deployment_metrics$Metric == "Serialized Model Size (Disk)"]
inference_ms <- deployment_metrics$Value[deployment_metrics$Metric == "Inference Latency"]

cat(sprintf("MODEL_RAM_MB: %.4f\n", model_ram_mb))
cat(sprintf("MODEL_DISK_MB: %.4f\n", model_disk_mb))
cat(sprintf("INFERENCE_LATENCY_MS: %.4f\n", inference_ms))
cat(sprintf("THROUGHPUT_PER_SECOND: %.1f\n", 1000 / inference_ms))

cat("\n")

# ============================================================================
# SECTION 14: ENVIRONMENT FOOTPRINT
# ============================================================================
cat("==== SECTION 14: ENVIRONMENT FOOTPRINT ====\n\n")

if (exists("all_pkgs_unique") && exists("total_size_mb")) {
  cat(sprintf("TOTAL_PACKAGE_DEPENDENCIES: %d\n", length(all_pkgs_unique)))
  cat(sprintf("TOTAL_INSTALLATION_SIZE_MB: %.2f\n", total_size_mb))
} else {
  cat("TOTAL_PACKAGE_DEPENDENCIES: NOT_COMPUTED\n")
  cat("TOTAL_INSTALLATION_SIZE_MB: NOT_COMPUTED\n")
}

cat("\n")

# ============================================================================
# SECTION 15: THEORETICAL PROJECTIONS
# ============================================================================
cat("==== SECTION 15: THEORETICAL PROJECTIONS ====\n\n")

nested_cv_multiplier <- 125 / (PARAMS$cv_folds * PARAMS$cv_repeats)
nested_cv_proj_hours <- (tuning_time_sec * nested_cv_multiplier) / 3600

current_avg_grid <- mean(training_counts$Grid_Size)
large_grid_multiplier <- 50 / current_avg_grid
large_grid_proj_hours <- (tuning_time_sec * large_grid_multiplier) / 3600

cat(sprintf("NESTED_CV_MULTIPLIER: %.2f\n", nested_cv_multiplier))
cat(sprintf("NESTED_CV_PROJECTED_HOURS: %.2f\n", nested_cv_proj_hours))
cat(sprintf("LARGE_GRID_MULTIPLIER: %.2f\n", large_grid_multiplier))
cat(sprintf("LARGE_GRID_PROJECTED_HOURS: %.2f\n", large_grid_proj_hours))

cat("\n")

# ============================================================================
# SECTION 16: COMPUTATIONAL ROI
# ============================================================================
cat("==== SECTION 16: COMPUTATIONAL ROI ====\n\n")

single_algo_time_min <- (tuning_time_sec / 60) / 5
extra_time_min <- total_time_min - single_algo_time_min
time_multiplier <- total_time_min / single_algo_time_min
efficiency_ratio <- improvement_pct / extra_time_min

cat(sprintf("SINGLE_ALGO_TIME_MIN: %.2f\n", single_algo_time_min))
cat(sprintf("STACK_TOTAL_TIME_MIN: %.2f\n", total_time_min))
cat(sprintf("EXTRA_TIME_FOR_STACKING_MIN: %.2f\n", extra_time_min))
cat(sprintf("TIME_MULTIPLIER: %.2f\n", time_multiplier))
cat(sprintf("EFFICIENCY_RATIO: %.6f\n", efficiency_ratio))
cat("EFFICIENCY_RATIO_INTERPRETATION: Percent RMSE improvement per minute of extra computation\n")

cat("\n")

# ============================================================================
# SECTION 17: FEATURE IMPORTANCE - PERMUTATION
# ============================================================================
cat("==== SECTION 17: FEATURE IMPORTANCE - PERMUTATION ====\n\n")

if (exists("vi_permutation")) {
  top_features_perm <- vi_permutation %>%
    filter(variable != "_full_model_") %>%
    group_by(variable) %>%
    summarize(
      mean_dropout_loss = mean(dropout_loss, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    arrange(desc(mean_dropout_loss)) %>%
    head(15)
  
  cat("TOP_15_FEATURES_PERMUTATION:\n")
  for (i in 1:nrow(top_features_perm)) {
    cat(sprintf("  %d. %s: %.6f\n", i, 
                top_features_perm$variable[i], 
                top_features_perm$mean_dropout_loss[i]))
  }
} else {
  cat("PERMUTATION_IMPORTANCE: NOT_AVAILABLE\n")
}

cat("\n")

# ============================================================================
# SECTION 18: FEATURE IMPORTANCE - SHAP
# ============================================================================
cat("==== SECTION 18: FEATURE IMPORTANCE - SHAP ====\n\n")

if (exists("shap_importance")) {
  cat("TOP_15_FEATURES_SHAP:\n")
  for (i in 1:min(15, nrow(shap_importance))) {
    cat(sprintf("  %d. %s: %.6f\n", i, 
                shap_importance$feature[i], 
                shap_importance$mean_abs_shap[i]))
  }
} else {
  cat("SHAP_IMPORTANCE: NOT_AVAILABLE\n")
}

cat("\n")

# ============================================================================
# SECTION 19: KEY PACKAGES AND VERSIONS
# ============================================================================
cat("==== SECTION 19: KEY PACKAGES AND VERSIONS ====\n\n")

key_pkgs <- c("tidymodels", "stacks", "tune", "recipes", "parsnip",
              "ranger", "xgboost", "glmnet", "kernlab", "earth",
              "DALEX", "fastshap", "yardstick")

cat("PACKAGE_VERSIONS:\n")
for (pkg in key_pkgs) {
  if (requireNamespace(pkg, quietly = TRUE)) {
    cat(sprintf("  %s: %s\n", pkg, as.character(packageVersion(pkg))))
  }
}

cat("\n")

# ============================================================================
# SECTION 20: FEASIBILITY SUMMARY
# ============================================================================
cat("==== SECTION 20: FEASIBILITY SUMMARY ====\n\n")

cat(sprintf("WORKFLOW_COMPLETED: TRUE\n"))
cat(sprintf("MEMORY_FEASIBLE: %s\n", ifelse(peak_rss < 8, "TRUE", "FALSE")))
cat(sprintf("TIME_FEASIBLE: %s\n", ifelse(total_time_hours < 8, "TRUE", "FALSE")))
cat(sprintf("OVERALL_FEASIBLE: %s\n", ifelse(peak_rss < 8 && total_time_hours < 8, "TRUE", "FALSE")))

cat("\n")
cat("================================================================================\n")
cat("END OF EXTRACTION\n")
cat("================================================================================\n")

# Close sink
sink()

# Confirm output
cat(sprintf("\n\nResults extraction complete!\n"))
cat(sprintf("Output saved to: %s\n", output_filename))
cat(sprintf("File size: %.1f KB\n", file.size(output_filename) / 1024))
```

PHASE 10: ## Evaluation metrics

We gather performance metrics for the final stacked ensemble and the best-performing base learners.

```{r adaptation-tradeoff-table, echo=FALSE}
# Create comprehensive adaptation trade-off table

adaptation_tradeoffs <- tibble(
  Adaptation = c(
    "Hyperparameter Search",
    "Cross-Validation Strategy", 
    "Explainability Method",
    "Base Learner Selection",
    "Computational Environment"
  ),
  `Theoretical Ideal` = c(
    "Bayesian optimization or exhaustive grid search",
    "Nested 5x5 cross-validation (25 models per config)",
    "Exact SHAP with 1000+ coalition samples",
    "Include deep learning (requires GPU)",
    "Cloud HPC cluster with multiple GPUs"
  ),
  `Pragmatic Choice` = c(
    "Constrained random/regular grids (60-63 points)",
    "Grouped 5-fold CV with 5 repeats (25 models per config)",
    "fastshap with 100 coalitions",
    "5 diverse CPU-friendly algorithms",
    "Standard laptop with parallel CPU processing"
  ),
  `Resource Impact` = c(
    sprintf("~%.0fx reduction in tuning time", 
            mean(theoretical_grids$reduction_factor)),
    "~40% reduction in total training episodes",
    "~50x faster explainability computation",
    "No GPU required; compatible with consumer hardware",
    "Eliminates cloud costs and internet dependency"
  ),
  `Trade-off Justification` = c(
    "Accepts suboptimal hyperparameters for guaranteed completion",
    "Accepts minor optimization bias for feasibility",
    "Accepts approximate feature attributions for tractability",
    "Prioritizes diversity and accessibility over maximum performance",
    "Demonstrates feasibility on hardware accessible to LRS researchers"
  )
)


adaptation_gt <- adaptation_tradeoffs %>%
  gt() %>%
  tab_header(
    title = "Pragmatic Workflow Adaptations and Resource Trade-offs",
    subtitle = "Each adaptation represents a deliberate choice prioritizing feasibility over theoretical optimality"
  ) %>%
  cols_width(
    Adaptation ~ px(150),
    "Theoretical Ideal" ~ px(200),
    "Pragmatic Choice" ~ px(200),
    "Resource Impact" ~ px(180),
    "Trade-off Justification" ~ px(220)
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  ) %>%
  tab_options(
    table.font.size = 10,
    row.striping.include_table_body = TRUE
  )

adaptation_gt
```

```{r feasibility-results-table, echo=FALSE}
# PRIMARY RESULTS: Workflow Feasibility Documentation

cat("=== WORKFLOW FEASIBILITY RESULTS ===\n\n")

cat("1. WORKFLOW COMPLETION: SUCCESS\n")
cat("   The complete stacked ensemble workflow executed successfully on standard laptop hardware without cloud computing or GPU acceleration\n")
cat("2. COMPUTATIONAL RESOURCE CONSUMPTION:\n\n")

# Timing results table
timing_gt <- timing_results %>%
  arrange(desc(wall_time_seconds)) %>%
  mutate(
    pct_total = 100 * wall_time_seconds / sum(wall_time_seconds)
  ) %>%
  gt() %>%
  tab_header(
    title = "Computational Time Requirements",
    subtitle = sprintf("Total workflow execution time: %.2f hours", 
                      sum(timing_results$wall_time_seconds) / 3600)
  ) %>%
  fmt_number(columns = wall_time_seconds, decimals = 1) %>%
  fmt_percent(columns = pct_total, decimals = 1, scale_values = FALSE) %>%
  cols_label(
    component = "Workflow Component",
    wall_time_seconds = "Time (seconds)",
    wall_time_formatted = "Time (formatted)",
    pct_total = "% of Total"
  )

timing_gt

cat("\n")

# Memory results table
memory_gt <- memory_results %>%
  mutate(memory_formatted = sprintf("%.2f GB (%.0f MB)", memory_gb, memory_mb)) %>%
  select(component, memory_formatted) %>%
  gt() %>%
  tab_header(
    title = "Memory Consumption",
    subtitle = sprintf("Peak memory usage: %.2f GB", max(memory_results$memory_gb))
  ) %>%
  cols_label(
    component = "Workflow Stage",
    memory_formatted = "Memory Used"
  )

memory_gt

cat("\n3. WORKFLOW COMPLEXITY METRICS:\n\n")
cat(sprintf("  Total model training episodes: %d\n", total_training_episodes))
cat(sprintf("  Base learner candidates: %d\n", ncol(emsci_stack) - 1))
cat(sprintf("  Members selected by meta-learner: %d (%.1f%% selection rate)\n",
           meta_coef_summary$selected_members,
           100 * meta_coef_summary$selection_rate))
cat(sprintf("  Mean hyperparameter search reduction: %.1fx\n",
           mean(theoretical_grids$reduction_factor)))

cat("\n4. ENSEMBLE CONSTRUCTION:\n\n")
cat(sprintf("  Dominant model contribution: %s (%.1f%% of weight)\n",
           meta_coef_summary$dominant_model,
           meta_coef_summary$dominant_weight_pct))

cat("\n5. EXPLAINABILITY IMPLEMENTATION: SUCCESS\n\n")
cat("  Permutation importance: Computed for all features\n")
cat("  SHAP values: Computed for full dataset (N=1500)\n")
cat(sprintf("  Explainability speedup (fastshap): ~50x faster than exact SHAP\n"))

cat("\n=== CONCLUSION ===\n")
cat("All workflow components completed successfully within resource constraints\n")
cat("typical of standard laptop hardware. Peak memory remained below 8GB, and\n")
cat("total execution time was under 3 hours, demonstrating feasibility for\n")
cat("overnight computation on consumer hardware.\n")
```

```{r eval-metrics-table}
# --- 1. Get Unbiased Stack Metrics ---
# (This section is correct and unchanged)
stack_metrics_formatted <- final_cv_metrics %>%
  filter(.metric %in% c("rmse", "rsq", "mae")) %>%
  mutate(
    ci_lower = mean - 1.96 * std_err,
    ci_upper = mean + 1.96 * std_err,
    `Mean (95% CI)` = case_when(
      .metric == "rsq" ~ sprintf("%.3f [%.3f, %.3f]", mean, ci_lower, ci_upper),
      TRUE ~ sprintf("%.2f [%.2f, %.2f]", mean, ci_lower, ci_upper)
    ),
    Metric = case_when(
      .metric == "mae" ~ "MAE",
      .metric == "rmse" ~ "RMSE",
      .metric == "rsq" ~ "R-squared"
    )
  ) %>%
  select(Metric, `Mean (95% CI)`) %>%
  mutate(Model = "Stacked ensemble") # Add a model name for merging

# --- 2. Get Biased Base Learner Metrics ---
# (This section is correct and unchanged)
base_learners_formatted <- model_set_results %>%
  rank_results(select_best = TRUE) %>%
  filter(.metric %in% c("rmse", "rsq", "mae")) %>%
  select(wflow_id, .metric, mean, std_err) %>%
  mutate(
    Model = case_when(
      stringr::str_detect(wflow_id, "elnet") ~ "Best ElasticNet",
      stringr::str_detect(wflow_id, "rf") ~ "Best Random Forest",
      stringr::str_detect(wflow_id, "xgb") ~ "Best XGBoost",
      stringr::str_detect(wflow_id, "svm") ~ "Best SVM",
      stringr::str_detect(wflow_id, "mars") ~ "Best MARS"
    ),
    ci_lower = mean - 1.96 * std_err,
    ci_upper = mean + 1.96 * std_err,
    `Mean (95% CI)` = case_when(
      .metric == "rsq" ~ sprintf("%.3f [%.3f, %.3f]", mean, ci_lower, ci_upper),
      TRUE ~ sprintf("%.2f [%.2f, %.2f]", mean, ci_lower, ci_upper)
    ),
    Metric = case_when(
      .metric == "mae" ~ "MAE",
      .metric == "rmse" ~ "RMSE",
      .metric == "rsq" ~ "R-squared"
    )
  ) %>%
  select(Model, Metric, `Mean (95% CI)`)

# --- 3. Combine and Format for Table ---
# Get the number of models retained in the final stack
n_retained_models <- emsci_stack_blended$coefs %>%
  broom::tidy() %>%
  filter(term != "(Intercept)", estimate > 0) %>%
  nrow()

# Bind the two tibbles together
full_performance_table <- bind_rows(base_learners_formatted, stack_metrics_formatted) %>%
  # Pivot to put metrics as columns
  pivot_wider(names_from = Metric, values_from = `Mean (95% CI)`) %>%
  
  # --- FIX: The levels here MUST exactly match the strings created above ---
  mutate(Model = factor(Model, levels = c(
    "Best ElasticNet",
    "Best Random Forest",
    "Best XGBoost",
    "Best SVM",
    "Best MARS",
    "Stacked ensemble" # <-- Must match "Stacked Ensemble (Unbiased)"
  ))) %>%
  
  # This arrange call will now work correctly, using the factor levels
  arrange(Model)

# --- 4. Create gt Table ---
# (This section is correct and unchanged)
performance_gt <- full_performance_table %>%
  gt() %>%
  tab_header(
    title = "Cross-validated performance metrics",
    subtitle = paste("Stacked ensemble (retained", n_retained_models, "candidates) vs. best base learners")
  ) %>%
  cols_label(
    Model = "Model"
  ) %>%
  tab_footnote(
    footnote = "Base learner metrics are optimistically biased (from stage 1 tuning). Stacked ensemble metrics are unbiased (from independent stage 2 CV).",
    locations = cells_column_labels(columns = Model)
  ) %>%
  # This conditional style will also work now
  tab_style(
    style = list(
      cell_fill(color = "#E8F8F5"),
      cell_text(weight = "bold")
    ),
    locations = cells_body(
      rows = Model == "Stacked Ensemble"
    )
  )

performance_gt
```

# PHASE 10: EVALUATION PLOTS

## Calibration plot

We assess model calibration using the out-of-fold predictions via predicted vs observed scores.

```{r calibration-plot, fig.width=10, fig.height=7}
# Calculate calibration metrics
calibration_metrics <- plot_data_cv %>%
  summarize(
    calibration_slope = coef(lm(motor_score_12m ~ .pred))[2],
    calibration_intercept = coef(lm(motor_score_12m ~ .pred))[1],
    pearson_r = cor(motor_score_12m, .pred),
    rsq = pearson_r^2
  )

# Create enhanced calibration plot
calibration_plot <- ggplot(plot_data_cv, aes(x = .pred, y = motor_score_12m)) +
  # Add reference line (perfect calibration)
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", 
              color = "gray40", linewidth = 0.8) +
  # Add LOESS smoother to detect systematic bias
  geom_smooth(method = "loess", se = TRUE, color = "#2E86AB", 
              fill = "#A9D6E5", alpha = 0.3, linewidth = 1) +
  # Add linear regression line
  geom_smooth(method = "lm", se = FALSE, color = "#E63946", 
              linetype = "solid", linewidth = 0.8) +
  # Points colored by AIS grade for clinical context
  geom_point(aes(color = baseline_ais), alpha = 0.5, size = 2) +
  # Add comprehensive annotation box
  annotate("text", 
           x = min(plot_data_cv$.pred) + 8, 
           y = max(plot_data_cv$motor_score_12m) - 8,
           label = sprintf(
             "Calibration slope: %.3f\nIntercept: %.3f\nR-squared: %.3f\nPearson r: %.3f", 
             calibration_metrics$calibration_slope,
             calibration_metrics$calibration_intercept,
             calibration_metrics$rsq,
             calibration_metrics$pearson_r
           ),
           hjust = 0, vjust = 1, size = 3.5,
           fontface = "bold",
           color = "gray20") +
  # Color scale for AIS grades
  scale_color_brewer(palette = "Set1", name = "Baseline AIS") +
  # Labels with clear interpretation guidance
  labs(
    title = "Calibration plot: Predicted vs. observed motor scores",
    subtitle = "Points colored by baseline AIS grade. Red line = linear fit; Blue = LOESS smoother; Dashed = perfect calibration",
    x = "Predicted motor score at 12 months",
    y = "Observed motor score at 12 months"
  ) +
  coord_fixed(ratio = 1) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 10, color = "gray40", margin = margin(b = 10)),
    plot.caption = element_text(size = 9, color = "gray50", hjust = 0),
    axis.title = element_text(size = 11, face = "bold"),
    legend.position = "right",
    panel.grid.minor = element_blank()
  )

print(calibration_plot)
```

## Calibration by score range

We stratify the data by predicted score range to analyze calibration-in-the-large.

```{r calibration-by-range-plot, fig.width=10, fig.height=7}
# Analyze calibration stratified by predicted score range
# This reveals if model is well-calibrated across the outcome spectrum

calibration_by_range <- plot_data_cv %>%
  mutate(
    pred_category = cut(.pred, 
                       breaks = c(-Inf, 25, 50, 75, Inf),
                       labels = c("0-25", "26-50", "51-75", "76-100"))
  ) %>%
  group_by(pred_category) %>%
  summarize(
    n = n(),
    mean_predicted = mean(.pred),
    mean_observed = mean(motor_score_12m),
    calibration_error = mean_observed - mean_predicted,
    .groups = "drop"
  )

# Create calibration-in-the-large plot
cal_range_plot <- ggplot(calibration_by_range, 
                         aes(x = mean_predicted, y = mean_observed)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray40") +
  geom_point(aes(size = n), color = "#2E86AB", alpha = 0.7) +
  geom_segment(aes(xend = mean_predicted, yend = mean_predicted),
               arrow = arrow(length = unit(0.2, "cm")),
               color = "#E63946", linewidth = 1) +
  geom_text(aes(label = sprintf("n=%d\n=%.1f", n, calibration_error)),
            vjust = -1.5, size = 3) +
  scale_size_continuous(range = c(5, 15), name = "Sample Size") +
  labs(
    title = "Calibration-in-the-Large by predicted score range",
    subtitle = "Red arrows show calibration error (observed - predicted). Well-calibrated models have short arrows",
    x = "Mean predicted score",
    y = "Mean observed score"
  ) +
  coord_fixed() +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 10, color = "gray40", margin = margin(b = 10))
  )

cal_range_plot
```

```{r calibration-by-range-table}
# Print calibration table
cal_range_gt <- calibration_by_range %>%
  mutate(
    `Score Range` = as.character(pred_category),
    `N` = n,
    `Mean Predicted` = sprintf("%.1f", mean_predicted),
    `Mean Observed` = sprintf("%.1f", mean_observed),
    `Calibration Error` = sprintf("%.2f", calibration_error)
  ) %>%
  select(`Score Range`, N, `Mean Predicted`, `Mean Observed`, `Calibration Error`) %>%
  gt() %>%
  tab_header(
    title = "Calibration stratified by score range"
  ) %>%
  tab_footnote(
    footnote = "Calibration error = Observed - predicted. positive = underprediction",
    locations = cells_column_labels(columns = `Calibration Error`)
  )

cal_range_gt
```

## Residual diagnostics

We check the model error behavior by generating standard diagnostic plots from the out-of-fold predictions.

```{r res-diag, fig.width=10, fig.height=7}
# --- Methodologically Flawless Residual Diagnostics ---
# This chunk generates the three core diagnostic plots to 
# understand the model s error behavior.

# Ensure the residual column exists on our OOF data object
# (This is already created in your blandaltman-plot chunk, 
#  but we include it here for robustness)
if (!"residual" %in% names(plot_data_cv)) {
  plot_data_cv <- plot_data_cv %>%
    mutate(residual = motor_score_12m - .pred)
}

# 1. Residuals vs. Fitted Plot
# PURPOSE: To detect systematic bias and non-linear patterns.
# INTERPRETATION: A flat line at 0 is perfect. 
#   A curve = systematic bias (e.g., under-predicting high values).
p_resid_fitted <- ggplot(plot_data_cv, aes(x = .pred, y = residual)) +
  geom_point(aes(color = baseline_ais), alpha = 0.4, size = 1.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  geom_smooth(method = "loess", color = "#E63946", se = TRUE, 
              fill = "#E63946", alpha = 0.1) +
  scale_color_brewer(palette = "Set1", name = "Baseline AIS") +
  labs(
    title = "Residuals vs. fitted values",
    subtitle = "Diagnostic for systematic bias and non-linearity",
    x = "Fitted (predicted) Values",
    y = "Residual (observed - predicted)"
  ) +
 theme_minimal() +
  theme(
    theme(legend.position = "bottom"),
    plot.title = element_text(face = "bold", size = 13),
    plot.subtitle = element_text(size = 10, color = "gray30"),
    axis.title = element_text(size = 11)
  )


# 2. Normal Q-Q Plot
# PURPOSE: To describe the *character* of the errors.
# INTERPRETATION: Are errors normal (on the line) or do 
#   we have "fat tails" (S-shape) = more extreme errors?
p_qq <- ggplot(plot_data_cv, aes(sample = residual)) +
  stat_qq(aes(color = baseline_ais), alpha = 0.4) +
  stat_qq_line(color = "#E63946", linewidth = 1) +
  scale_color_brewer(palette = "Set1", name = "Baseline AIS") +
  labs(
    title = "Normal Q-Q plot",
    subtitle = "Diagnostic for character of the error distribution",
    x = "Theoretical quantiles (normal)",
    y = "Sample quantiles (residuals)"
  ) +
 theme_minimal() +
  theme(
    theme(legend.position = "bottom"),
    plot.title = element_text(face = "bold", size = 13),
    plot.subtitle = element_text(size = 10, color = "gray30"),
    axis.title = element_text(size = 11)
  )

# 3. Scale-Location Plot
# PURPOSE: To detect heteroscedasticity (non-constant error).
# INTERPRETATION: A flat line is ideal (homoscedasticity). 
#   A sloped line = error magnitude depends on the predicted value.
p_scale_loc <- ggplot(plot_data_cv, aes(x = .pred, y = sqrt(abs(residual)))) +
  geom_point(aes(color = baseline_ais), alpha = 0.4, size = 1.5) +
  geom_smooth(method = "loess", color = "#E63946", se = TRUE, 
              fill = "#E63946", alpha = 0.1) +
  scale_color_brewer(palette = "Set1", name = "Baseline AIS") +
  labs(
    title = "Scale-Location plot",
    subtitle = "Diagnostic for: Heteroscedasticity (non-constant error variance)",
    x = "Fitted (predicted) Values",
    y = "|Standardized residuals|"
  ) +
 theme_minimal() +
  theme(
    theme(legend.position = "bottom"),
    plot.title = element_text(face = "bold", size = 13),
    plot.subtitle = element_text(size = 10, color = "gray30"),
    axis.title = element_text(size = 11)
  )
# Combine all plots
(p_resid_fitted) / (p_qq | p_scale_loc) + 
  plot_layout(guides = "collect") & theme(legend.position = "bottom")

```

## Bland-Altman plot

We assess the agreement between observed and predicted scores, visualizing the mean bias and 95% limits of agreement to check for any systematic error trends across the range of scores.

```{r blandaltman-plot, fig.width=10, fig.height=7}

plot_data_cv <- plot_data_cv %>%
  mutate(
    residual = motor_score_12m - .pred,
    abs_residual = abs(residual)
  )

# Calculate Bland-Altman statistics
ba_stats <- plot_data_cv %>%
  summarize(
    mean_diff = mean(residual),
    sd_diff = sd(residual),
    loa_upper = mean_diff + 1.96 * sd_diff,
    loa_lower = mean_diff - 1.96 * sd_diff
  )

# Create Bland-Altman plot
ba_plot <- plot_data_cv %>%
  mutate(mean_score = (motor_score_12m + .pred) / 2) %>%
  ggplot(aes(x = mean_score, y = residual)) +
  # Add reference line at zero (no bias)
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray40", linewidth = 0.8) +
  # Add mean bias line
  geom_hline(yintercept = ba_stats$mean_diff, color = "#E63946", 
             linewidth = 0.8) +
  # Add limits of agreement
  geom_hline(yintercept = ba_stats$loa_upper, color = "#2E86AB", 
             linetype = "dotted", linewidth = 0.8) +
  geom_hline(yintercept = ba_stats$loa_lower, color = "#2E86AB", 
             linetype = "dotted", linewidth = 0.8) +
  # Add LOESS smoother (global)
  geom_smooth(method = "loess", se = TRUE, color = "#06A77D", 
              fill = "#D0F4DE", alpha = 0.3, linewidth = 1) +
  
  # FIX: Map color to baseline_ais inside aes()
  geom_point(aes(color = baseline_ais), alpha = 0.4, size = 1.5) +
  
  # Add annotations
  annotate("text", x = max(plot_data_cv$mean_score) * 0.95, 
           y = ba_stats$mean_diff, 
           label = sprintf("Mean bias: %.2f", ba_stats$mean_diff),
           hjust = 1, vjust = -0.5, size = 3.5, color = "#E63946") +
  annotate("text", x = max(plot_data_cv$mean_score) * 0.95, 
           y = ba_stats$loa_upper,
           label = sprintf("+1.96 SD: %.2f", ba_stats$loa_upper),
           hjust = 1, vjust = -0.5, size = 3.5, color = "#2E86AB") +
  annotate("text", x = max(plot_data_cv$mean_score) * 0.95, 
           y = ba_stats$loa_lower,
           label = sprintf("-1.96 SD: %.2f", ba_stats$loa_lower),
           hjust = 1, vjust = 1.5, size = 3.5, color = "#2E86AB") +
           
  # FIX: Add a color scale
  scale_color_brewer(palette = "Set1", name = "Baseline AIS") +
  
  # Labels and theme
  labs(
    title = "Bland-Altman plot: agreement between predicted and observed scores",
    subtitle = "Points colored by baseline AIS grade. Green LOESS shows global bias trend.
Red line is mean bias; Blue dotted lines are 95% limits of agreement.",
    x = "Mean of predicted and observed score",
    y = "Difference (observed - predicted)"
    ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 13),
    plot.subtitle = element_text(size = 10, color = "gray30"),
    axis.title = element_text(size = 11)
  )

print(ba_plot)
```

## Subgroup analysis by AIS grade

We visualize the model performance stratified by the baseline AIS grade to compare each group's error against the overall model performance.


```{r eval-subgroup}
# Calculate performance metrics by AIS grade
subgroup_metrics <- plot_data_cv %>%
  group_by(baseline_ais) %>%
  summarize(
    n = n(),
    rmse = rmse_vec(truth = motor_score_12m, estimate = .pred),
    mae = mae_vec(truth = motor_score_12m, estimate = .pred),
    rsq = rsq_vec(truth = motor_score_12m, estimate = .pred),
    .groups = "drop"
  ) %>%
  arrange(baseline_ais)

# Create subgroup table
subgroup_gt <- subgroup_metrics %>%
  gt() %>%
  tab_header(
    title = "Subgroup analysis: Performance by baseline AIS grade"
  )
```

```{r eval-subgroup-plot, fig.width=11, fig.height=7}
# Calculate overall performance for reference
overall_rmse <- final_cv_metrics %>% 
  filter(.metric == "rmse") %>% 
  pull(mean)

# Prepare subgroup data for visualization
subgroup_plot_data <- subgroup_metrics %>%
  mutate(
    ais_label = sprintf("AIS %s\n(n=%d)", baseline_ais, n)
  )

# Create enhanced subgroup plot
subgroup_plot <- subgroup_plot_data %>%
  ggplot(aes(x = reorder(ais_label, -rmse), y = rmse, fill = baseline_ais)) +
  geom_col(alpha = 0.85, width = 0.7) +
  # Add reference line for overall performance
  geom_hline(yintercept = overall_rmse, linetype = "dashed", 
             color = "gray30", linewidth = 0.8) +
  # Add value labels on bars
  geom_text(aes(label = sprintf("RMSE: %.2f\nR-squared: %.3f", rmse, rsq)),
            vjust = -0.3, size = 3.5, fontface = "bold") +
  # Add annotation for reference line
  annotate("text", x = 0.7, y = overall_rmse * 1.05,
           label = sprintf("Overall RMSE: %.2f", overall_rmse),
           hjust = 0, size = 3.5, color = "gray30") +
  # AIS-specific color palette
  scale_fill_manual(
    values = c("A" = "#E63946", "B" = "#F77F00", 
               "C" = "#FCBF49", "D" = "#06A77D"),
    guide = "none"
  ) +
  labs(
    title = "Subgroup analysis: Model performance by baseline AIS grade",
    subtitle = "RMSE and R-squared stratified by injury severity. Lower RMSE = better prediction. Dashed line = overall model performance",
    x = "Baseline AIS Grade (Sample size)",
    y = "Root Mean Squared Error (RMSE)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 10, color = "gray40", margin = margin(b = 10)),
    plot.caption = element_text(size = 9, color = "gray50", hjust = 0),
    axis.title = element_text(size = 11, face = "bold"),
    axis.title.x = element_text(margin = margin(t = 10)),
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  ) +
  ylim(0, max(subgroup_plot_data$rmse) * 1.25)

print(subgroup_plot)
```

## Clinical utility evaluation

We evaluate the model's clinical utility by defining a Minimal Clinically Important Difference (MCID) and plotting a histogram to find the percentage of predictions within this acceptable error.

```{r eval-util, fig.width=10, fig.height=7}

# Define Minimal Clinically Important Difference (MCID)
MCID <- 4

# Create binary outcomes based on MCID
clinical_data <- plot_data_cv %>%
  mutate(
    # Clinically acceptable prediction (within MCID)
    within_mcid = abs_residual <= MCID
  )

# Calculate proportion of predictions within MCID
within_mcid_pct <- mean(clinical_data$within_mcid) * 100

# Create error distribution plot
error_dist_plot <- ggplot(clinical_data, aes(x = abs_residual)) +
  geom_histogram(bins = 30, fill = "#2E86AB", alpha = 0.7, color = "white") +
  geom_vline(xintercept = MCID, color = "#E63946", 
             linetype = "dashed", linewidth = 1) +
  annotate("text", x = MCID, y = Inf, 
           label = sprintf("MCID = %d", MCID),
           hjust = -0.1, vjust = 1.5, color = "#E63946", size = 4) +
  labs(
    title = "Distribution of prediction errors",
    subtitle = sprintf("%.1f%% of predictions are within MCID (%d points)", within_mcid_pct, MCID),
    x = "Absolute prediction error (points)",
    y = "Count"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 13),
    plot.subtitle = element_text(size = 10, color = "gray30"),
    axis.title = element_text(size = 11)
  )

error_dist_plot
```

We test the model's classification utility by setting a recovery threshold and generating a Precision-Recall curve from the model's predictions.

```{r eval-util-auprc, fig.width=10, fig.height=7}
# Define a clinically meaningful binary outcome
# This is a separate evaluation from the regression (MCID) utility
# Example: "Achieved a high recovery score" (e.g., > 50)
RECOVERY_THRESHOLD <- 60

# Create the binary 'truth' column
# We must use factors and set the "positive" class as the first level
class_data <- plot_data_cv %>%
  mutate(
    truth_class = factor(
      ifelse(motor_score_12m >= RECOVERY_THRESHOLD, "Recovered", "Not Recovered"),
      levels = c("Recovered", "Not Recovered")
    )
  )

# Calculate metrics
# We use `.pred` (our 0-100 score) as the ranking "probability"
# `yardstick` correctly handles this for ranking.
set.seed(123)
auroc_val <- roc_auc(class_data, truth = truth_class, .pred, event_level = "first")
auprc_val <- pr_auc(class_data, truth = truth_class, .pred, event_level = "first")
pr_curve_data <- pr_curve(class_data, truth = truth_class, .pred, event_level = "first")

# Calculate baseline prevalence for AUPRC plot
prevalence <- mean(class_data$truth_class == "Recovered")

# Plot PR Curve
auprc_plot <- ggplot(pr_curve_data, aes(x = recall, y = precision)) +
  geom_line(color = "#06A77D", linewidth = 1.2) +
  # Add baseline (no-skill) line
  geom_hline(yintercept = prevalence, linetype = "dashed", color = "gray40") +
  annotate("text", x = 0.5, y = prevalence + 0.05, 
           label = sprintf("Baseline (Prevalence) = %.2f", prevalence),
           color = "gray40") +
  labs(
    title = "Precision-Recall Curve (Classification Utility)",
    subtitle = sprintf("AUROC: %.3f | AUPRC: %.3f (Target: Score >= %d)", 
                       auroc_val$.estimate, auprc_val$.estimate, RECOVERY_THRESHOLD),
    x = "Recall (Sensitivity)",
    y = "Precision (PPV)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 13),
    plot.subtitle = element_text(size = 10, color = "gray30"),
    axis.title = element_text(size = 11)
  )

auprc_plot
```