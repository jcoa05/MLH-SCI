---
title: "Pragmatic implementation of stacked ensemble learning in resource-constrained clinical research: a methodological demonstration"
subtitle: "Supplementary Code for Machine Learning: Health Submission"
author: "Authors"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
    df_print: paged
  word_document:
    toc: true
    toc_depth: 3
    fig_width: 8
    fig_height: 6
---

<!-- 
================================================================================
DOCUMENT STRUCTURE (for navigation)
================================================================================
PHASE 1: ENVIRONMENT SETUP (Lines ~30-200)
  - Global options, packages, parallelization, hardware detection

PHASE 2: CONFIGURATION (Lines ~200-350)
  - Control knobs, monitoring functions, timing/memory tracking

PHASE 3: DATA & PREPROCESSING (Lines ~350-500)
  - Data loading, cross-validation strategy, preprocessing recipe

PHASE 4: MODEL SPECIFICATION (Lines ~500-700)
  - Base learner specs, hyperparameter grids, training count calculation

PHASE 5: MODEL TRAINING (Lines ~700-900)
  - Base learner tuning, stacked ensemble construction

PHASE 6: EVALUATION (Lines ~900-1200)
  - Two-stage CV evaluation, bootstrap CIs, performance metrics

PHASE 7: EXPLAINABILITY (Lines ~1200-1400)
  - Permutation importance, SHAP values (optional)

PHASE 8: FEASIBILITY REPORTING (Lines ~1400-1700)
  - Deployment metrics, resource consumption, final tables
================================================================================
-->

# PHASE 1: ENVIRONMENT SETUP {.tabset}

## Global Options

```{r setup, include=FALSE}
knitr::opts_chunk$set(
 echo = FALSE,
 warning = FALSE,
 message = FALSE,
 fig.align = 'center',
 dpi = 300,
 cache = FALSE
)
```

## Reproducibility

```{r reproducibility-setup, include=FALSE}
# This chunk ensures reproducible package versions via renv

if (!requireNamespace("renv", quietly = TRUE)) {
 message("Installing 'renv' to manage project dependencies...")
 install.packages("renv")
}

if (file.exists("renv.lock")) {
 status <- try(renv::status(), silent = TRUE)
 if (inherits(status, "try-error") ||
     length(status$library$missing) > 0 ||
     length(status$lockfile$missing) > 0) {
   message("Restoring exact package versions from renv.lock...")
   renv::restore(prompt = FALSE)
   message("Environment restored successfully.")
 } else {
   message("Environment synchronized. Using reproducible package versions.")
 }
} else {
 warning("CRITICAL: 'renv.lock' not found. Reproducibility not guaranteed.")
}
```

## Packages

```{r packages, message=FALSE}
if (!require(pacman, quietly = TRUE)) install.packages("pacman")

pacman::p_load(
 # Core ML Framework
 tidymodels, stacks,
 # Base Model Engines
 glmnet, ranger, xgboost, kernlab, earth,
 # Parallel Processing & Monitoring
 doFuture, tictoc, pryr, callr, ps,
 # Data Manipulation
 dplyr, tidyr, purrr, tibble, stringr, forcats,
 # Visualization & Tables
 ggplot2, gtsummary, gt, broom, viridis, patchwork,
 # Model Interpretation
 DALEX, DALEXtra, fastshap, yardstick, probably,
 # Memory tracking
 lobstr
)

# Set global seed for reproducibility
set.seed(123)
Sys.setenv(LANG = "en")

cat("Packages loaded successfully.\n")
```

## Parallelization

```{r parallelization}
total_cores <- parallel::detectCores(logical = TRUE)
num_workers <- max(1, total_cores - 1)

registerDoFuture()
plan(multisession, workers = num_workers)

cat(sprintf("Registered %d workers for parallel processing.\n", num_workers))
```

## Hardware Detection

```{r hardware-detection}
# Automatic hardware profiling (no manual edits required)

hardware_specs <- tibble(
 Specification = c(
   "CPU Cores (Physical)",
   "CPU Threads (Logical)",
   "Workers Used",
   "R Version",
   "Platform",
   "Operating System"
 ),
 Value = c(
   as.character(parallel::detectCores(logical = FALSE)),
   as.character(parallel::detectCores(logical = TRUE)),
   as.character(num_workers),
   R.version$version.string,
   R.version$platform,
   paste(Sys.info()["sysname"], Sys.info()["release"])
 )
)

cat("\n=== HARDWARE PROFILE ===\n")
print(hardware_specs)
```

# PHASE 2: CONFIGURATION {.tabset}

## Control Knobs

```{r control-knobs}
# ============================================================================
# CENTRALIZED PARAMETERS
# ============================================================================
# All tunable parameters in one location for easy modification

PARAMS <- list(
 # Cross-validation
 cv_folds = 5,
 cv_repeats = 2,
 
 # Model complexity
 trees_n = 1000,
 grid_size = 10,
 
 # Evaluation
 bootstrap_iters = 50,
 shap_nsim = 5,
 
 # Clinical thresholds (for secondary analyses)
 mcid_val = 4,
 recovery_thresh = 60
)

cat("Control parameters set:\n")
print(as_tibble(PARAMS))
```

## Memory Monitor

```{r monitor-setup, include=FALSE}
# Background memory monitoring function
# Tracks RSS (physical RAM) and VMS (virtual memory) for all R processes

monitor_system_ram <- function(log_file = "memory_log.csv", interval_seconds = 2) {
 write.csv(data.frame(timestamp = Sys.time(), rss_gb = 0, vms_gb = 0),
           log_file, row.names = FALSE)
 
 tryCatch({
   my_pid <- Sys.getpid()
   my_handle <- ps::ps_handle(my_pid)
   target_user <- tryCatch(ps::ps_username(my_handle), error = function(e) NULL)
  
   while(TRUE) {
     all_ps <- tryCatch(ps::ps(), error = function(e) NULL)
    
     if (!is.null(all_ps) && !is.null(target_user)) {
       target_indices <- which(
         grepl("rsession|rterm|rgui|rscript|rstudio", all_ps$name, ignore.case = TRUE) &
         all_ps$username == target_user
       )
       r_ps <- all_ps[target_indices, ]
      
       stats_list <- lapply(r_ps$pid, function(p) {
         tryCatch({
           h <- ps::ps_handle(pid = p)
           mem <- ps::ps_memory_info(h)
           return(c(rss = mem[["rss"]], vms = mem[["vms"]]))
         }, error = function(e) c(rss = 0, vms = 0))
       })
      
       stats_mat <- do.call(rbind, stats_list)
       total_rss_gb <- sum(stats_mat[, "rss"], na.rm = TRUE) / (1024^3)
       total_vms_gb <- sum(stats_mat[, "vms"], na.rm = TRUE) / (1024^3)
      
       write.table(
         data.frame(timestamp = Sys.time(), rss_gb = total_rss_gb, vms_gb = total_vms_gb),
         log_file, sep = ",", col.names = FALSE, row.names = FALSE, append = TRUE
       )
     }
     Sys.sleep(interval_seconds)
     if (file.exists("stop_monitoring.file")) break
   }
 }, error = function(e) {})
}

# Cleanup from previous runs
for (f in c("memory_log_tuning.csv", "memory_log_stacking.csv",
           "memory_log_shap.csv", "stop_monitoring.file")) {
 if (file.exists(f)) file.remove(f)
}
```

## Timing & Memory Tracking

```{r tracking-setup, include=FALSE}
# Initialize tracking tibbles

timing_results <- tibble(
 component = character(),
 wall_time_seconds = numeric(),
 wall_time_formatted = character()
)

memory_results <- tibble(
 component = character(),
 memory_mb = numeric(),
 memory_gb = numeric()
)

# Helper: Save timing
save_timing <- function(component_name, tic_output) {
 elapsed <- tic_output$toc - tic_output$tic
 timing_results <<- timing_results %>%
   add_row(
     component = component_name,
     wall_time_seconds = elapsed,
     wall_time_formatted = sprintf("%.2f min (%.0f sec)", elapsed/60, elapsed)
   )
}

# Helper: Record memory
record_memory <- function(component_name) {
 mem_bytes <- pryr::mem_used()
 mem_mb <- as.numeric(mem_bytes) / (1024^2)
 mem_gb <- mem_mb / 1024
 
 memory_results <<- memory_results %>%
   add_row(component = component_name, memory_mb = mem_mb, memory_gb = mem_gb)
 
 invisible(mem_mb)
}

# Record baseline
record_memory("Baseline (after package loading)")
```

# PHASE 3: DATA & PREPROCESSING {.tabset}

## Data Loading

```{r data-load}
df <- read.csv("data.csv")

cat(sprintf("Dataset loaded: %d observations, %d variables\n", nrow(df), ncol(df)))
cat(sprintf("Outcome variable: motor_score_12m (range: %d - %d)\n",
           min(df$motor_score_12m), max(df$motor_score_12m)))
cat(sprintf("Clustering variable: center_id (%d unique centers)\n",
           length(unique(df$center_id))))
```

## Cross-Validation Strategy

```{r cv-strategy}
# ============================================================================
# PRAGMATIC ADAPTATION #1: Grouped V-Fold CV Instead of Nested CV
# ============================================================================
# Trade-off: Accepts minor hyperparameter optimization bias in exchange for
# feasible computation time. Nested CV (5 outer x 5 inner x 2 repeats = 50
# ensemble constructions) would exceed overnight execution limits.
#
# Justification: Grouping by center_id simulates external validation by
# ensuring all observations from a given site appear exclusively in either
# the training or validation set, preventing center-level data leakage.
# ============================================================================

set.seed(123)
cv_splits <- group_vfold_cv(
 df,
 group = center_id,
 v = PARAMS$cv_folds,
 repeats = PARAMS$cv_repeats
)

cat(sprintf("Cross-validation: %d-fold grouped CV, %d repeats = %d total resamples\n",
           PARAMS$cv_folds, PARAMS$cv_repeats, PARAMS$cv_folds * PARAMS$cv_repeats))
```

## Preprocessing Recipe

```{r preprocessing-recipe}
data_recipe <- recipe(motor_score_12m ~ ., data = df) %>%
 # Exclude ID columns from modeling
 update_role(patient_id, new_role = "ID") %>%
 update_role(patient_id_global, new_role = "ID") %>%
 update_role(center_id, new_role = "ID") %>%
 # Preprocessing steps
 step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
 step_normalize(all_numeric_predictors(),
                -all_of(c("patient_id", "patient_id_global", "center_id"))) %>%
 step_integer(all_logical_predictors()) %>%
 step_zv(all_predictors())

cat("Preprocessing recipe defined.\n")
cat("Steps: dummy encoding -> normalization -> zero-variance removal\n")
```

# PHASE 4: MODEL SPECIFICATION {.tabset}

## Base Learner Specifications

```{r baselearner-specs}
# ============================================================================
# BASE LEARNER SELECTION RATIONALE
# ============================================================================
# Five diverse, CPU-compatible algorithms selected to maximize complementarity:
# - Linear (ElasticNet): Captures linear relationships, interpretable
# - Tree-based (RF, XGBoost): Captures non-linear interactions
# - Kernel-based (SVM): Captures complex decision boundaries
# - Spline-based (MARS): Captures piecewise-linear relationships
# ============================================================================

# 1. ElasticNet (penalized linear regression)
set.seed(123)
elnet_spec <- linear_reg(penalty = tune(), mixture = tune()) %>%
 set_engine("glmnet") %>%
 set_mode("regression")

# 2. Random Forest
set.seed(123)
rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 2000) %>%
 set_engine("ranger", num.threads = 1) %>%  # Single-threaded to prevent oversubscription
 set_mode("regression")

# 3. XGBoost
# ============================================================================
# NOTE: We intentionally OMIT early stopping during tuning.
# Early stopping with an internal validation split creates data leakage within
# CV folds. The cross-validation process itself acts as regularization.
# Reference: https://tune.tidymodels.org/articles/extras/optimizations.html
# ============================================================================
set.seed(123)
xgb_spec <- boost_tree(
 trees = 2000,
 tree_depth = tune(),
 learn_rate = tune(),
 min_n = tune(),
 loss_reduction = tune(),
 sample_size = tune()
) %>%
 set_engine("xgboost", nthread = 1) %>%
 set_mode("regression")

# 4. SVM (Radial Basis Function kernel)
set.seed(123)
svm_spec <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
 set_engine("kernlab") %>%
 set_mode("regression")

# 5. MARS (Multivariate Adaptive Regression Splines)
set.seed(123)
mars_spec <- mars(num_terms = tune(), prod_degree = tune()) %>%
 set_engine("earth") %>%
 set_mode("regression")

cat("Base learner specifications defined: ElasticNet, RF, XGBoost, SVM, MARS\n")
```

## Hyperparameter Grids

```{r hyperparameter-grids}
# ============================================================================
# PRAGMATIC ADAPTATION #2: Constrained Hyperparameter Grids
# ============================================================================
# Trade-off: Accepts potentially suboptimal hyperparameters for tractable
# computation time. Standard literature grids (50-100 configs per algorithm)
# would require ~5x more compute time.
#
# Strategy:
# - Random grids for high-dimensional spaces (ElasticNet, XGBoost, SVM)
# - Regular grids for lower-dimensional spaces (RF, MARS)
# ============================================================================

set.seed(123)

# ElasticNet: 2D random grid
elnet_grid <- grid_random(
 penalty(range = c(-5, 1)),
 mixture(range = c(0, 1)),
 size = PARAMS$grid_size
)

# Random Forest: Regular grid (2 parameters)
rf_grid <- grid_regular(
 mtry(range = c(5, 20)),
 min_n(range = c(5, 15)),
 levels = c(mtry = 4, min_n = 5)
)

# XGBoost: 5D random grid
xgb_grid <- grid_random(
 tree_depth(range = c(3, 10)),
 min_n(range = c(2, 20)),
 learn_rate(range = c(0.001, 0.3)),
 loss_reduction(range = c(0.001, 10)),
 sample_prop(range = c(0.5, 1.0)),
 size = PARAMS$grid_size
)

# SVM: 2D random grid
svm_grid <- grid_random(
 cost(range = c(-5, 5)),
 rbf_sigma(range = c(-5, 0)),
 size = PARAMS$grid_size
)

# MARS: Regular grid (2 parameters)
mars_grid <- grid_regular(
 num_terms(range = c(5, 25)),
 prod_degree(range = c(1L, 2L)),
 levels = c(num_terms = 4, prod_degree = 5)
)

cat("Hyperparameter grids defined.\n")
```

## Training Count Calculation

```{r training-counts}
# Calculate exact number of model training episodes for feasibility reporting

training_counts <- tibble(
 Model = c("ElasticNet", "Random Forest", "XGBoost", "SVM", "MARS"),
 Grid_Size = c(nrow(elnet_grid), nrow(rf_grid), nrow(xgb_grid),
               nrow(svm_grid), nrow(mars_grid)),
 CV_Folds = PARAMS$cv_folds,
 CV_Repeats = PARAMS$cv_repeats
) %>%
 mutate(
   Models_Per_Repeat = Grid_Size * CV_Folds,
   Total_Models = Models_Per_Repeat * CV_Repeats
 )

total_training_episodes <- sum(training_counts$Total_Models)

cat(sprintf("\n=== TRAINING COMPLEXITY ===\n"))
cat(sprintf("Total model training episodes: %d\n\n", total_training_episodes))
print(training_counts)
```

# PHASE 5: MODEL TRAINING {.tabset}

## Base Learner Tuning

```{r baselearner-tuning}
# Start background memory monitoring
if (file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")
if (file.exists("memory_log_tuning.csv")) file.remove("memory_log_tuning.csv")

bg_monitor <- callr::r_bg(monitor_system_ram,
                         args = list(log_file = "memory_log_tuning.csv"),
                         supervise = TRUE)
Sys.sleep(3)

# Create workflow set
tic.clearlog()
tic("Base learners tuning")

set.seed(123)
df_workflow_set <- workflow_set(
 preproc = list(recipe = data_recipe),
 models = list(
   elnet = elnet_spec,
   rf = rf_spec,
   xgb = xgb_spec,
   svm = svm_spec,
   mars = mars_spec
 )
) %>%
 option_add(grid = elnet_grid, id = "recipe_elnet") %>%
 option_add(grid = rf_grid, id = "recipe_rf") %>%
 option_add(grid = xgb_grid, id = "recipe_xgb") %>%
 option_add(grid = svm_grid, id = "recipe_svm") %>%
 option_add(grid = mars_grid, id = "recipe_mars")

# Control settings for stacking (saves predictions)
ctrl_stack <- control_stack_grid()

# Run parallel tuning
set.seed(123)
model_set_results <- workflow_map(
 df_workflow_set,
 fn = "tune_grid",
 resamples = cv_splits,
 metrics = metric_set(rmse, mae, rsq),
 control = ctrl_stack,
 verbose = TRUE
)

timing_baselearners <- toc(log = TRUE, quiet = TRUE)
save_timing("Base learners tuning", timing_baselearners)

cat(sprintf("\nTuning complete: %d models trained in %.1f minutes\n",
           total_training_episodes,
           (timing_baselearners$toc - timing_baselearners$tic) / 60))

# Stop monitoring
file.create("stop_monitoring.file")
Sys.sleep(3)
bg_monitor$kill()
if (file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")

record_memory("After base learner tuning")
```

## Stacked Ensemble Construction

```{r stacking}
# Start monitoring
if (file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")
if (file.exists("memory_log_stacking.csv")) file.remove("memory_log_stacking.csv")

bg_monitor <- callr::r_bg(monitor_system_ram,
                         args = list(log_file = "memory_log_stacking.csv"),
                         supervise = TRUE)
Sys.sleep(3)

tic.clearlog()
tic("Stacking and meta-learner fitting")

# Initialize stack with all candidates
set.seed(123)
df_stack <- stacks() %>%
 add_candidates(model_set_results)

cat(sprintf("Stack initialized with %d candidate models\n", ncol(df_stack) - 1))

# Blend with non-negative LASSO meta-learner
set.seed(123)
df_stack_blended <- df_stack %>%
 blend_predictions(
   penalty = 10^seq(-4, -0.5, length.out = 200),
   mixture = 1,
   non_negative = TRUE,
   metric = metric_set(rmse)
 )

cat("Stack blended successfully\n")

# Fit final ensemble on full training data
set.seed(123)
df_stack_fitted <- df_stack_blended %>%
 fit_members()

timing_stacking <- toc(log = TRUE, quiet = TRUE)
save_timing("Stacking and meta-learner fitting", timing_stacking)

cat("Final stacked ensemble fitted.\n")
record_memory("After stacking and fitting")

# Stop monitoring
file.create("stop_monitoring.file")
Sys.sleep(3)
bg_monitor$kill()
if (file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")
```

# PHASE 6: EVALUATION {.tabset}

## Two-Stage CV Evaluation

```{r two-stage-evaluation}
# ============================================================================
# PRAGMATIC ADAPTATION #3: Two-Stage Evaluation Framework
# ============================================================================
# Stage 1 (completed above): Hyperparameter optimization using grouped CV
# Stage 2 (this chunk): Independent CV evaluation of the fitted ensemble
#
# This avoids the 125-fold computational cost of fully nested CV while still
# producing approximately unbiased performance estimates for the final stack.
# ============================================================================

# Create L1 dataset (base learner predictions + IDs)
l1_data <- df_stack %>%
 mutate(.row = row_number()) %>%
 left_join(
   df %>%
     mutate(.row = row_number()) %>%
     select(.row, patient_id_global, center_id),
   by = ".row"
 )

cat(sprintf("L1 dataset created: %d observations\n", nrow(l1_data)))

# ============================================================================
# METHODOLOGICAL NOTE: Independent Seed for Stage 2 Evaluation
# ============================================================================
# We deliberately use a DIFFERENT seed (456) here than in Stage 1 tuning (123).
#
# Rationale: Stage 2 creates NEW, INDEPENDENT cross-validation folds that were
# never used during hyperparameter optimization. This ensures:
#   1. The meta-learner's penalty (lambda) is evaluated on truly held-out data
#   2. Performance estimates are not contaminated by optimization bias
#   3. The two-stage evaluation maintains methodological independence
#
# Using the same seed would recreate identical fold assignments, defeating the
# purpose of the two-stage evaluation framework described in Methods Section 2.5.2
# ============================================================================
set.seed(456)  # Different seed ensures fold independence from Stage 1
l1_folds <- group_vfold_cv(l1_data, group = center_id, v = 5)

# Get optimal penalty from Stage 1
best_penalty <- df_stack_blended$penalty$penalty
cat(sprintf("Using meta-learner penalty from Stage 1: %.6f\n", best_penalty))

# Define meta-learner spec (using !! to inject the fixed penalty value)
meta_spec <- linear_reg(penalty = !!best_penalty, mixture = 1) %>%
 set_engine("glmnet", lower.limits = 0)

# Meta-learner recipe
meta_recipe <- recipe(motor_score_12m ~ ., data = l1_data) %>%
 update_role(center_id, new_role = "ID") %>%
 update_role(patient_id_global, new_role = "ID") %>%
 update_role(.row, new_role = "ID")

# Create workflow and evaluate
meta_wf <- workflow() %>%
 add_recipe(meta_recipe) %>%
 add_model(meta_spec)

tic.clearlog()
tic("Meta-learner CV evaluation")

set.seed(456)
meta_cv_results <- fit_resamples(
 meta_wf,
 resamples = l1_folds,
 metrics = metric_set(rmse, rsq, mae),
 control = control_resamples(save_pred = TRUE)
)

timing_metacv <- toc(log = TRUE, quiet = TRUE)
save_timing("Meta-learner CV evaluation", timing_metacv)

# Extract predictions
cv_predictions <- tidyr::unnest(meta_cv_results, .predictions)

plot_data_cv <- cv_predictions %>%
 left_join(
   df %>%
     mutate(.row = row_number()) %>%
     select(.row, patient_id_global, baseline_ais),
   by = ".row"
 ) %>%
 select(patient_id_global, motor_score_12m, .pred, baseline_ais)

# Final CV metrics
final_cv_metrics <- collect_metrics(meta_cv_results)

cat(sprintf("Out-of-fold predictions: %d observations\n", nrow(plot_data_cv)))
cat("Stage 2 evaluation complete.\n")
```

## Bootstrap Confidence Intervals

```{r bootstrap-ci}
tic.clearlog()
tic("Bootstrap CIs")

set.seed(789)
bootstrap_metrics <- map_dfr(1:PARAMS$bootstrap_iters, function(i) {
 boot_sample <- plot_data_cv %>%
   slice_sample(prop = 1, replace = TRUE)
 
 tibble(
   iteration = i,
   rmse = rmse_vec(boot_sample$motor_score_12m, boot_sample$.pred),
   mae = mae_vec(boot_sample$motor_score_12m, boot_sample$.pred),
   rsq = rsq_vec(boot_sample$motor_score_12m, boot_sample$.pred)
 )
})

bootstrap_ci <- bootstrap_metrics %>%
 summarize(
   across(c(rmse, mae, rsq),
          list(lower = ~quantile(., 0.025), upper = ~quantile(., 0.975)))
 )

timing_bootstrap <- toc(log = TRUE, quiet = TRUE)
save_timing("Bootstrap CIs", timing_bootstrap)

cat("Bootstrap 95% Confidence Intervals:\n")
print(bootstrap_ci)
```

## Performance Metrics Table

```{r performance-table}
# Get unbiased stack metrics
stack_metrics_formatted <- final_cv_metrics %>%
 filter(.metric %in% c("rmse", "rsq", "mae")) %>%
 mutate(
   ci_lower = mean - 1.96 * std_err,
   ci_upper = mean + 1.96 * std_err,
   `Mean (95% CI)` = case_when(
     .metric == "rsq" ~ sprintf("%.3f [%.3f, %.3f]", mean, ci_lower, ci_upper),
     TRUE ~ sprintf("%.2f [%.2f, %.2f]", mean, ci_lower, ci_upper)
   ),
   Metric = case_when(
     .metric == "mae" ~ "MAE",
     .metric == "rmse" ~ "RMSE",
     .metric == "rsq" ~ "R-squared"
   )
 ) %>%
 select(Metric, `Mean (95% CI)`) %>%
 mutate(Model = "Stacked Ensemble")

# Get base learner metrics (optimistically biased from Stage 1)
base_learners_formatted <- model_set_results %>%
 rank_results(select_best = TRUE) %>%
 filter(.metric %in% c("rmse", "rsq", "mae")) %>%
 select(wflow_id, .metric, mean, std_err) %>%
 mutate(
   Model = case_when(
     stringr::str_detect(wflow_id, "elnet") ~ "Best ElasticNet",
     stringr::str_detect(wflow_id, "rf") ~ "Best Random Forest",
     stringr::str_detect(wflow_id, "xgb") ~ "Best XGBoost",
     stringr::str_detect(wflow_id, "svm") ~ "Best SVM",
     stringr::str_detect(wflow_id, "mars") ~ "Best MARS"
   ),
   ci_lower = mean - 1.96 * std_err,
   ci_upper = mean + 1.96 * std_err,
   `Mean (95% CI)` = case_when(
     .metric == "rsq" ~ sprintf("%.3f [%.3f, %.3f]", mean, ci_lower, ci_upper),
     TRUE ~ sprintf("%.2f [%.2f, %.2f]", mean, ci_lower, ci_upper)
   ),
   Metric = case_when(
     .metric == "mae" ~ "MAE",
     .metric == "rmse" ~ "RMSE",
     .metric == "rsq" ~ "R-squared"
   )
 ) %>%
 select(Model, Metric, `Mean (95% CI)`)

# Combine
n_retained_models <- df_stack_blended$coefs %>%
 broom::tidy() %>%
 filter(term != "(Intercept)", estimate > 0) %>%
 nrow()

full_performance_table <- bind_rows(base_learners_formatted, stack_metrics_formatted) %>%
 pivot_wider(names_from = Metric, values_from = `Mean (95% CI)`) %>%
 mutate(Model = factor(Model, levels = c(
   "Best ElasticNet", "Best Random Forest", "Best XGBoost",
   "Best SVM", "Best MARS", "Stacked Ensemble"
 ))) %>%
 arrange(Model)

# Display table
full_performance_table %>%
 gt() %>%
 tab_header(
   title = "Cross-Validated Performance Metrics",
   subtitle = sprintf("Stacked ensemble (retained %d candidates) vs. best base learners",
                      n_retained_models)
 ) %>%
 tab_footnote(
   footnote = "Base learner metrics are optimistically biased (Stage 1). Stack metrics are unbiased (Stage 2).",
   locations = cells_column_labels(columns = Model)
 ) %>%
 tab_style(
   style = list(cell_fill(color = "#E8F8F5"), cell_text(weight = "bold")),
   locations = cells_body(rows = Model == "Stacked Ensemble")
 )
```

# PHASE 7: EXPLAINABILITY {.tabset}

## Permutation Importance

```{r permutation-importance}
# Create custom predict function for stacked model
predict_function_stack <- function(model, newdata) {
 pred <- predict(model, newdata)
 return(pred$.pred)
}

# Prepare data (exclude outcome, keep all predictors including IDs for recipe)
explain_data <- df %>% select(-motor_score_12m)
explain_y <- df$motor_score_12m

# Create DALEX explainer
set.seed(123)
explainer_stack <- DALEX::explain(
 model = df_stack_fitted,
 data = explain_data,
 y = explain_y,
 predict_function = predict_function_stack,
 verbose = FALSE
)

cat("DALEX explainer created successfully\n")

# Calculate permutation-based variable importance
tic.clearlog()
tic("Permutation importance")

set.seed(123)
vi_permutation <- DALEX::model_parts(
 explainer_stack,
 loss_function = DALEX::loss_root_mean_square,
 B = 10,
 type = "difference"
)

timing_perm <- toc(log = TRUE, quiet = TRUE)
save_timing("Permutation importance", timing_perm)

cat("Permutation importance calculated.\n")
```

## SHAP Values (Optional)

```{r shap-values, eval=TRUE}
# ============================================================================
# PRAGMATIC ADAPTATION #4: Approximate SHAP via fastshap
# ============================================================================
# Trade-off: Approximate vs. exact Shapley values
# Exact SHAP requires 2^p coalition evaluations (for 26 predictors: >67 million
# evaluations per observation). fastshap uses Monte Carlo sampling for ~50x speedup.
# ============================================================================

# Start monitoring
if (file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")
if (file.exists("memory_log_shap.csv")) file.remove("memory_log_shap.csv")

bg_monitor <- callr::r_bg(monitor_system_ram,
                         args = list(log_file = "memory_log_shap.csv"),
                         supervise = TRUE)
Sys.sleep(3)

# Prediction wrapper
pfun <- function(object, newdata) {
 predict(object, newdata)$.pred
}

tic.clearlog()
tic("SHAP calculation")

set.seed(123)
shap_values <- fastshap::explain(
 object = df_stack_fitted,
 X = explain_data,
 pred_wrapper = pfun,
 nsim = PARAMS$shap_nsim,
 adjust = TRUE
)

timing_shap <- toc(log = TRUE, quiet = TRUE)
save_timing("SHAP calculation", timing_shap)

# Compute mean |SHAP|
shap_importance <- shap_values %>%
 as_tibble() %>%
 summarize(across(everything(), ~mean(abs(.)))) %>%
 pivot_longer(everything(), names_to = "feature", values_to = "mean_abs_shap") %>%
 arrange(desc(mean_abs_shap)) %>%
 slice_head(n = 20)

record_memory("After SHAP calculation")

# Stop monitoring
file.create("stop_monitoring.file")
Sys.sleep(3)
bg_monitor$kill()
if (file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")

cat("SHAP values calculated.\n")
```

# PHASE 8: FEASIBILITY REPORTING {.tabset}

## Meta-Learner Weights

```{r metalearner-weights, fig.width=10, fig.height=7}
# Extract and visualize meta-learner coefficients

meta_weights <- df_stack_fitted$coefs %>%
 broom::tidy() %>%
 filter(term != "(Intercept)") %>%
 mutate(
   model_type = case_when(
     stringr::str_detect(term, "elnet") ~ "ElasticNet",
     stringr::str_detect(term, "rf") ~ "Random Forest",
     stringr::str_detect(term, "xgb") ~ "XGBoost",
     stringr::str_detect(term, "svm") ~ "SVM",
     stringr::str_detect(term, "mars") ~ "MARS",
     TRUE ~ "Other"
   ),
   estimate = ifelse(estimate > 0, estimate, 0)
 ) %>%
 filter(estimate > 0) %>%
 arrange(desc(estimate))

# Aggregate by model type
weights_by_model <- meta_weights %>%
 group_by(model_type) %>%
 summarize(total_weight = sum(estimate), n_members = n(), .groups = "drop") %>%
 arrange(desc(total_weight))

# Store summary
meta_coef_summary <- tibble(
 total_candidates = ncol(df_stack) - 1,
 selected_members = nrow(meta_weights),
 selection_rate = nrow(meta_weights) / (ncol(df_stack) - 1),
 dominant_model = weights_by_model$model_type[1],
 dominant_weight_pct = 100 * weights_by_model$total_weight[1] / sum(weights_by_model$total_weight)
)

# Visualization
weights_plot_data <- weights_by_model %>%
 mutate(
   pct = 100 * total_weight / sum(total_weight),
   label_text = sprintf("%.1f%%\n%d members", pct, n_members)
 )

ggplot(weights_plot_data,
      aes(x = reorder(model_type, total_weight), y = total_weight, fill = model_type)) +
 geom_col(alpha = 0.85, width = 0.7) +
 geom_text(aes(label = label_text), hjust = -0.1, size = 3.5, fontface = "bold") +
 coord_flip(ylim = c(0, max(weights_plot_data$total_weight) * 1.2)) +
 scale_fill_brewer(palette = "Set2", guide = "none") +
 labs(
   title = "Meta-Learner Weight Distribution by Base Model Type",
   subtitle = sprintf("Non-negative LASSO selected %d of %d candidate models",
                      sum(weights_plot_data$n_members), ncol(df_stack) - 1),
   x = "Base Model Type",
   y = "Total Weight (Sum of LASSO Coefficients)"
 ) +
 theme_minimal() +
 theme(
   plot.title = element_text(face = "bold", size = 14),
   plot.subtitle = element_text(size = 10, color = "gray40"),
   axis.title = element_text(size = 11, face = "bold"),
   panel.grid.major.y = element_blank()
 )
```

## Deployment Metrics

```{r deployment-metrics}
# ============================================================================
# DEPLOYMENT FEASIBILITY METRICS
# ============================================================================

# 1. Model object size (RAM footprint for inference)
model_ram_size_mb <- as.numeric(lobstr::obj_size(df_stack_fitted)) / (1024^2)

# 2. Serialized file size (disk/transfer footprint)
saveRDS(df_stack_fitted, "temp_final_model.rds", compress = "xz")
model_disk_size_mb <- file.size("temp_final_model.rds") / (1024^2)
file.remove("temp_final_model.rds")

# 3. Single-patient inference latency
new_patient_profile <- df[1, ] %>% select(-motor_score_12m)

# Warm-up run
invisible(predict(df_stack_fitted, new_patient_profile))

# Benchmark (100 iterations)
tic.clearlog()
tic("inference_batch")
for(i in 1:100) {
 pred <- predict(df_stack_fitted, new_patient_profile)
}
t_inference <- toc(log = TRUE, quiet = TRUE)

latency_ms <- ((t_inference$toc - t_inference$tic) / 100) * 1000

# Store results
deployment_metrics <- tibble(
 Metric = c("Model Object Size (RAM)", "Serialized Model Size (Disk)", "Inference Latency"),
 Value = c(model_ram_size_mb, model_disk_size_mb, latency_ms),
 Unit = c("MB", "MB", "ms"),
 Implication = c(
   "Minimum RAM required to load model",
   "Bandwidth/storage for deployment",
   "Time per single prediction"
 )
)

timing_results <<- timing_results %>%
 add_row(
   component = "Single-Patient Inference (avg ms)",
   wall_time_seconds = latency_ms / 1000,
   wall_time_formatted = sprintf("%.2f ms", latency_ms)
 )

cat("\n=== DEPLOYMENT FEASIBILITY ===\n")
print(deployment_metrics)
cat(sprintf("\nThroughput: ~%.0f predictions per second\n", 1000/latency_ms))
```

## Environment Footprint

```{r environment-footprint}
# Calculate total package dependency footprint

core_pkgs <- c("tidymodels", "stacks", "glmnet", "ranger", "xgboost",
              "kernlab", "earth", "doFuture", "tictoc", "pryr",
              "dplyr", "tidyr", "purrr", "tibble", "stringr", "forcats",
              "ggplot2", "gtsummary", "gt", "broom", "viridis",
              "DALEX", "DALEXtra", "fastshap", "yardstick", "probably",
              "callr", "ps")

all_deps <- tools::package_dependencies(core_pkgs, recursive = TRUE)
all_pkgs_unique <- unique(c(core_pkgs, unlist(all_deps)))
all_pkgs_unique <- all_pkgs_unique[all_pkgs_unique %in% installed.packages()[, "Package"]]

# Estimate installation size
total_size_bytes <- 0
for (pkg in all_pkgs_unique) {
 loc <- system.file(package = pkg)
 if (loc != "") {
   files <- list.files(loc, recursive = TRUE, full.names = TRUE)
   total_size_bytes <- total_size_bytes + sum(file.info(files)$size, na.rm = TRUE)
 }
}
total_size_mb <- total_size_bytes / (1024^2)

cat(sprintf("\n=== ENVIRONMENT FOOTPRINT ===\n"))
cat(sprintf("Total dependencies: %d packages\n", length(all_pkgs_unique)))
cat(sprintf("Approximate installation size: %.2f MB\n", total_size_mb))
```

## System RAM Profile

```{r system-ram-profile}
# Process monitoring logs and extract peak values

get_peak_ram <- function(logfile, stage_name) {
 if (file.exists(logfile)) {
   df_log <- read.csv(logfile)
   if ("rss_gb" %in% names(df_log)) {
     peak_rss <- max(df_log$rss_gb, na.rm = TRUE)
     peak_vms <- max(df_log$vms_gb, na.rm = TRUE)
   } else {
     peak_rss <- NA_real_
     peak_vms <- NA_real_
   }
   return(tibble(Stage = stage_name, Peak_RSS_GB = peak_rss, Peak_VMS_GB = peak_vms))
 } else {
   return(tibble(Stage = stage_name, Peak_RSS_GB = NA_real_, Peak_VMS_GB = NA_real_))
 }
}

# Read all logs
system_peaks <- bind_rows(
 get_peak_ram("memory_log_tuning.csv", "Base Learner Tuning"),
 get_peak_ram("memory_log_stacking.csv", "Stacking & Refitting"),
 get_peak_ram("memory_log_shap.csv", "Explainability (SHAP)")
)

# Add to memory_results
for (i in 1:nrow(system_peaks)) {
 if (!is.na(system_peaks$Peak_RSS_GB[i])) {
   memory_results <<- memory_results %>%
     add_row(
       component = paste("Peak Physical RAM (RSS):", system_peaks$Stage[i]),
       memory_mb = system_peaks$Peak_RSS_GB[i] * 1024,
       memory_gb = system_peaks$Peak_RSS_GB[i]
     )
 }
}

cat("\n=== SYSTEM RAM PROFILE ===\n")
print(system_peaks)
```

## Computational ROI

```{r computational-roi}
# Calculate return on compute investment

tuning_time_sec <- timing_results %>%
 filter(component == "Base learners tuning") %>%
 pull(wall_time_seconds)

# Best single model performance
best_single_model_data <- model_set_results %>%
 rank_results(select_best = TRUE) %>%
 filter(.metric == "rmse") %>%
 slice_min(mean, n = 1)

single_rmse <- best_single_model_data$mean
single_model_name <- case_when(
 str_detect(best_single_model_data$wflow_id, "elnet") ~ "ElasticNet",
 str_detect(best_single_model_data$wflow_id, "rf") ~ "Random Forest",
 str_detect(best_single_model_data$wflow_id, "xgb") ~ "XGBoost",
 str_detect(best_single_model_data$wflow_id, "svm") ~ "SVM",
 str_detect(best_single_model_data$wflow_id, "mars") ~ "MARS"
)

# Stack performance
stack_rmse <- final_cv_metrics %>% filter(.metric == "rmse") %>% pull(mean)
stack_total_time_sec <- sum(timing_results$wall_time_seconds)
stack_total_time_min <- stack_total_time_sec / 60

# Single algorithm time estimate
single_algo_time_min <- (tuning_time_sec / 60) / 5
single_algo_time_sec <- tuning_time_sec / 5

# Compute efficiency
rmse_improvement_abs <- single_rmse - stack_rmse
rmse_improvement_pct <- 100 * rmse_improvement_abs / single_rmse
extra_time_min <- stack_total_time_min - single_algo_time_min
efficiency_ratio <- rmse_improvement_pct / extra_time_min

cat("\n=== COMPUTATIONAL RETURN ON INVESTMENT ===\n\n")
cat(sprintf("BASELINE (Best Single Model: %s):\n", single_model_name))
cat(sprintf("  RMSE: %.4f\n", single_rmse))
cat(sprintf("  Estimated time: %.1f minutes\n\n", single_algo_time_min))

cat("STACKED ENSEMBLE:\n")
cat(sprintf("  RMSE: %.4f\n", stack_rmse))
cat(sprintf("  Total time: %.1f minutes (%.2f hours)\n\n",
           stack_total_time_min, stack_total_time_min / 60))

cat("COMPARISON:\n")
cat(sprintf("  Performance gain: %.2f%% RMSE improvement\n", rmse_improvement_pct))
cat(sprintf("  Time multiplier: %.1fx increase\n", stack_total_time_sec / single_algo_time_sec))
cat(sprintf("  Extra time invested: %.1f minutes\n\n", extra_time_min))

cat("EFFICIENCY RATIO:\n")
cat(sprintf("  %.4f%% RMSE improvement per minute of extra computation\n", efficiency_ratio))
```

## Theoretical Projections

```{r theoretical-projections}
# Project time requirements for more rigorous approaches

nested_cv_multiplier <- 125 / (PARAMS$cv_folds * PARAMS$cv_repeats)
nested_cv_proj_hours <- (tuning_time_sec * nested_cv_multiplier) / 3600

current_avg_grid <- mean(training_counts$Grid_Size)
large_grid_multiplier <- 50 / current_avg_grid
large_grid_proj_hours <- (tuning_time_sec * large_grid_multiplier) / 3600

theoretical_projections <- tibble(
 Scenario = c("Nested CV (5x5x5)", "Larger Grids (50 configs/algorithm)"),
 Current_Hours = c(tuning_time_sec / 3600, tuning_time_sec / 3600),
 Multiplier = c(nested_cv_multiplier, large_grid_multiplier),
 Projected_Hours = c(nested_cv_proj_hours, large_grid_proj_hours)
)

cat("\n=== THEORETICAL TIME PROJECTIONS ===\n")
print(theoretical_projections)

cat(sprintf("\nInterpretation:\n"))
cat(sprintf("  Current workflow: %.2f hours\n", tuning_time_sec / 3600))
cat(sprintf("  Nested CV would require: %.2f hours (%.1fx increase)\n",
           nested_cv_proj_hours, nested_cv_multiplier))
cat(sprintf("  Standard grids would require: %.2f hours (%.1fx increase)\n",
           large_grid_proj_hours, large_grid_multiplier))
```

## Final Feasibility Summary

```{r feasibility-summary}
# ============================================================================
# DYNAMIC FEASIBILITY SUMMARY (replaces hardcoded claims)
# ============================================================================

peak_memory_gb <- max(memory_results$memory_gb, na.rm = TRUE)
total_time_hours <- sum(timing_results$wall_time_seconds) / 3600

cat("=== WORKFLOW FEASIBILITY RESULTS ===\n\n")

cat("1. WORKFLOW COMPLETION: SUCCESS\n")
cat("   The complete stacked ensemble workflow executed successfully on\n")
cat("   standard laptop hardware without cloud computing or GPU acceleration.\n\n")

cat("2. COMPUTATIONAL RESOURCE CONSUMPTION:\n")
cat(sprintf("   - Total execution time: %.2f hours\n", total_time_hours))
cat(sprintf("   - Peak memory usage: %.2f GB\n", peak_memory_gb))
cat(sprintf("   - Within 8GB limit: %s\n\n",
           ifelse(peak_memory_gb < 8, "YES", "NO")))

cat("3. TRAINING COMPLEXITY:\n")
cat(sprintf("   - Total model training episodes: %d\n", total_training_episodes))
cat(sprintf("   - Base learner candidates: %d\n", ncol(df_stack) - 1))
cat(sprintf("   - Members selected by meta-learner: %d (%.1f%% selection rate)\n",
           meta_coef_summary$selected_members,
           100 * meta_coef_summary$selection_rate))

cat("\n4. ENSEMBLE CONSTRUCTION:\n")
cat(sprintf("   - Dominant model contribution: %s (%.1f%% of weight)\n",
           meta_coef_summary$dominant_model,
           meta_coef_summary$dominant_weight_pct))

cat("\n=== CONCLUSION ===\n")
cat("All workflow components completed successfully within resource constraints\n")
cat("typical of standard laptop hardware. Peak memory remained below 8GB, and\n")
cat(sprintf("total execution time was %.2f hours, demonstrating feasibility for\n", total_time_hours))
cat("overnight computation on consumer hardware.\n")
```

## Timing Summary Table

```{r timing-summary-table}
timing_results %>%
 arrange(desc(wall_time_seconds)) %>%
 mutate(pct_total = 100 * wall_time_seconds / sum(wall_time_seconds)) %>%
 gt() %>%
 tab_header(
   title = "Computational Time Requirements",
   subtitle = sprintf("Total workflow execution time: %.2f hours",
                      sum(timing_results$wall_time_seconds) / 3600)
 ) %>%
 fmt_number(columns = wall_time_seconds, decimals = 1) %>%
 fmt_percent(columns = pct_total, decimals = 1, scale_values = FALSE) %>%
 cols_label(
   component = "Workflow Component",
   wall_time_seconds = "Time (seconds)",
   wall_time_formatted = "Time (formatted)",
   pct_total = "% of Total"
 )
```

## Session Information

```{r session-info}
cat("=== COMPUTATIONAL ENVIRONMENT ===\n\n")
cat("R version:", R.version$version.string, "\n")
cat("Platform:", R.version$platform, "\n")
cat("Running under:", Sys.info()["sysname"], Sys.info()["release"], "\n")
cat("Date executed:", format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z"), "\n\n")

cat("=== KEY ML PACKAGES ===\n\n")
pkgs <- c("tidymodels", "tune", "stacks", "recipes", "parsnip",
         "ranger", "xgboost", "earth", "glmnet", "kernlab",
         "DALEX", "fastshap", "yardstick")
for(pkg in pkgs) {
 if(requireNamespace(pkg, quietly = TRUE)) {
   cat(sprintf("  %s: %s\n", pkg, as.character(packageVersion(pkg))))
 }
}

print(sessionInfo())
```

## Cleanup

```{r cleanup}
# Remove monitoring logs
for (log_file in c("memory_log_tuning.csv", "memory_log_stacking.csv",
                  "memory_log_shap.csv", "stop_monitoring.file")) {
 if (file.exists(log_file)) file.remove(log_file)
}
cat("Monitoring logs cleaned up.\n")
```
