---
title: "Pragmatic implementation of stacked ensemble learning in resource-constrained clinical research: a methodological demonstration"
subtitle: "Supplementary Code for Machine Learning: Health Submission"
author: "Authors"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
    df_print: paged
  word_document:
    toc: true
    toc_depth: 3
    fig_width: 8
    fig_height: 6
---

<!-- 
================================================================================
DOCUMENT STRUCTURE (for navigation)
================================================================================
PHASE 1: ENVIRONMENT SETUP (Lines ~30-200)
  - Global options, packages, parallelization, hardware detection

PHASE 2: CONFIGURATION (Lines ~200-350)
  - Control knobs, monitoring functions, timing/memory tracking

PHASE 3: DATA & PREPROCESSING (Lines ~350-500)
  - Data loading, cross-validation strategy, preprocessing recipe

PHASE 4: MODEL SPECIFICATION (Lines ~500-700)
  - Base learner specs, hyperparameter grids, training count calculation

PHASE 5: MODEL TRAINING (Lines ~700-900)
  - Base learner tuning, stacked ensemble construction

PHASE 6: EVALUATION (Lines ~900-1200)
  - Two-stage CV evaluation, bootstrap CIs, performance metrics

PHASE 7: EXPLAINABILITY (Lines ~1200-1400)
  - Permutation importance, SHAP values (optional)

PHASE 8: FEASIBILITY REPORTING (Lines ~1400-1700)
  - Deployment metrics, resource consumption, final tables
================================================================================
-->

# PHASE 1: ENVIRONMENT SETUP {.tabset}

## Global Options

```{r setup, include=FALSE}
knitr::opts_chunk$set(
 echo = FALSE,
 warning = FALSE,
 message = FALSE,
 fig.align = 'center',
 dpi = 600,
 cache = FALSE
)
```

## Reproducibility

```{r reproducibility-setup, include=FALSE}
# This chunk ensures reproducible package versions via renv

if (!requireNamespace("renv", quietly = TRUE)) {
 message("Installing 'renv' to manage project dependencies...")
 install.packages("renv")
}

if (file.exists("renv.lock")) {
 status <- try(renv::status(), silent = TRUE)
 if (inherits(status, "try-error") ||
     length(status$library$missing) > 0 ||
     length(status$lockfile$missing) > 0) {
   message("Restoring exact package versions from renv.lock...")
   renv::restore(prompt = FALSE)
   message("Environment restored successfully.")
 } else {
   message("Environment synchronized. Using reproducible package versions.")
 }
} else {
 warning("CRITICAL: 'renv.lock' not found. Reproducibility not guaranteed.")
}
```

## Packages

```{r packages, message=FALSE}
if (!require(pacman, quietly = TRUE)) install.packages("pacman")

pacman::p_load(
 # Core ML Framework
 tidymodels, stacks,
 # Base Model Engines
 glmnet, ranger, xgboost, kernlab, earth,
 # Parallel Processing & Monitoring
 doFuture, foreach, tictoc, pryr, callr, ps,
 # Data Manipulation
 dplyr, tidyr, purrr, tibble, stringr, forcats,
 # Visualization & Tables
 ggplot2, gtsummary, gt, broom, viridis, patchwork,
 # Model Interpretation
 DALEX, DALEXtra, fastshap, yardstick, probably,
 # Memory tracking
 lobstr
)

# Set global seed for reproducibility
set.seed(123)
Sys.setenv(LANG = "en")

cat("Packages loaded successfully.\n")
```

## Parallelization

```{r parallelization}
total_cores <- parallel::detectCores(logical = TRUE)
num_workers <- max(1, total_cores - 1)

registerDoFuture()
plan(multisession, workers = num_workers)

cat(sprintf("Registered %d workers for parallel processing.\n", num_workers))
```

## Hardware Detection

```{r hardware-detection}
# Automatic hardware profiling (no manual edits required)

hardware_specs <- tibble(
 Specification = c(
   "CPU Cores (Physical)",
   "CPU Threads (Logical)",
   "Workers Used",
   "R Version",
   "Platform",
   "Operating System"
 ),
 Value = c(
   as.character(parallel::detectCores(logical = FALSE)),
   as.character(parallel::detectCores(logical = TRUE)),
   as.character(num_workers),
   R.version$version.string,
   R.version$platform,
   paste(Sys.info()["sysname"], Sys.info()["release"])
 )
)

cat("\n=== HARDWARE PROFILE ===\n")
print(hardware_specs)
```

# PHASE 2: CONFIGURATION {.tabset}

## Control Knobs

```{r control-knobs}
# ============================================================================
# CENTRALIZED PARAMETERS
# ============================================================================
# All tunable parameters in one location for easy modification

PARAMS <- list(
 # Cross-validation
 cv_folds = 5,
 cv_repeats = 5,
 
 # Model complexity
 grid_size = 25,
 
 # Evaluation
 bootstrap_iters = 1000,
 dalex_b = 100,
 shap_nsim = 100,
 
 # Clinical thresholds (for secondary analyses)
 mcid_val = 4,
 recovery_thresh = 60
)

cat("Control parameters set:\n")
print(as_tibble(PARAMS))
```

## Memory Monitor

```{r monitor-setup, include=FALSE}
# ============================================================================
# UNIFIED MEMORY MONITOR WITH PHASE LOGGING
# ============================================================================
# This monitor runs continuously throughout the workflow and logs:
# - RSS (Resident Set Size): Physical RAM actually used
# - VMS (Virtual Memory Size): Total virtual memory allocated
# - Phase: Current workflow phase for visualization
#
# The monitor writes to a single CSV file that is read at the end to generate
# a publication-quality figure showing RAM consumption across all phases.
# ============================================================================

MEMORY_LOG_FILE <- "memory_log_unified.csv"
PHASE_MARKER_FILE <- "current_phase.txt"

# Initialize the unified monitor
monitor_unified_ram <- function(log_file = "memory_log_unified.csv", 
                                phase_file = "current_phase.txt",
                                interval_seconds = 2) {
  
  # Initialize log file with headers
  write.csv(data.frame(
    timestamp = Sys.time(), 
    elapsed_sec = 0,
    rss_gb = 0, 
    vms_gb = 0,
    phase = "Initialization"
  ), log_file, row.names = FALSE)
  
  # Record start time

  start_time <- Sys.time()
  
  # Initialize phase file
  writeLines("Initialization", phase_file)
  
  tryCatch({
    my_pid <- Sys.getpid()
    my_handle <- ps::ps_handle(my_pid)
    target_user <- tryCatch(ps::ps_username(my_handle), error = function(e) NULL)
    
    while(TRUE) {
      # Read current phase from file (allows main process to update it)
      current_phase <- tryCatch(
        readLines(phase_file, n = 1, warn = FALSE),
        error = function(e) "Unknown"
      )
      
      all_ps <- tryCatch(ps::ps(), error = function(e) NULL)
      
      if (!is.null(all_ps) && !is.null(target_user)) {
        target_indices <- which(
          grepl("rsession|rterm|rgui|rscript|rstudio", all_ps$name, ignore.case = TRUE) &
            all_ps$username == target_user
        )
        r_ps <- all_ps[target_indices, ]
        
        stats_list <- lapply(r_ps$pid, function(p) {
          tryCatch({
            h <- ps::ps_handle(pid = p)
            mem <- ps::ps_memory_info(h)
            return(c(rss = mem[["rss"]], vms = mem[["vms"]]))
          }, error = function(e) c(rss = 0, vms = 0))
        })
        
        stats_mat <- do.call(rbind, stats_list)
        total_rss_gb <- sum(stats_mat[, "rss"], na.rm = TRUE) / (1024^3)
        total_vms_gb <- sum(stats_mat[, "vms"], na.rm = TRUE) / (1024^3)
        
        elapsed <- as.numeric(difftime(Sys.time(), start_time, units = "secs"))
        
        write.table(
          data.frame(
            timestamp = Sys.time(), 
            elapsed_sec = elapsed,
            rss_gb = total_rss_gb, 
            vms_gb = total_vms_gb,
            phase = current_phase
          ),
          log_file, sep = ",", col.names = FALSE, row.names = FALSE, append = TRUE
        )
      }
      
      Sys.sleep(interval_seconds)
      if (file.exists("stop_monitoring.file")) break
    }
  }, error = function(e) {
    message("Monitor error: ", e$message)
  })
}

# Helper function to update phase (call this from main workflow)
set_monitor_phase <- function(phase_name, phase_file = "current_phase.txt") {
  writeLines(phase_name, phase_file)
  Sys.sleep(0.5)  
}

# Cleanup from previous runs
for (f in c(MEMORY_LOG_FILE, PHASE_MARKER_FILE, "stop_monitoring.file")) {
  if (file.exists(f)) file.remove(f)
}

cat("Unified memory monitor configured.\n")

```

## Timing & Memory Tracking

```{r tracking-setup, include=FALSE}
# Initialize tracking tibbles

timing_results <- tibble(
 component = character(),
 wall_time_seconds = numeric(),
 wall_time_formatted = character()
)

memory_results <- tibble(
 component = character(),
 memory_mb = numeric(),
 memory_gb = numeric()
)

# Helper: Save timing
save_timing <- function(component_name, tic_output) {
 elapsed <- tic_output$toc - tic_output$tic
 timing_results <<- timing_results %>%
   add_row(
     component = component_name,
     wall_time_seconds = elapsed,
     wall_time_formatted = sprintf("%.2f min (%.0f sec)", elapsed/60, elapsed)
   )
}

# Helper: Record memory
record_memory <- function(component_name) {
 mem_bytes <- pryr::mem_used()
 mem_mb <- as.numeric(mem_bytes) / (1024^2)
 mem_gb <- mem_mb / 1024
 
 memory_results <<- memory_results %>%
   add_row(component = component_name, memory_mb = mem_mb, memory_gb = mem_gb)
 
 invisible(mem_mb)
}

# Record baseline
record_memory("Baseline (after package loading)")
```

# PHASE 3: DATA & PREPROCESSING {.tabset}

## Data Loading

```{r data-load}
df <- read.csv("data.csv")

cat(sprintf("Dataset loaded: %d observations, %d variables\n", nrow(df), ncol(df)))
cat(sprintf("Outcome variable: motor_score_12m (range: %d - %d)\n",
           min(df$motor_score_12m), max(df$motor_score_12m)))
cat(sprintf("Clustering variable: center_id (%d unique centers)\n",
           length(unique(df$center_id))))
```

## Cross-Validation Strategy

```{r cv-strategy}
# ============================================================================
# PRAGMATIC ADAPTATION #1: Grouped V-Fold CV Instead of Nested CV
# ============================================================================
# Trade-off: Accepts minor hyperparameter optimization bias in exchange for
# feasible computation time. Nested CV (5 outer x 5 inner x 2 repeats = 50
# ensemble constructions) would exceed overnight execution limits.
#
# Justification: Grouping by center_id simulates external validation by
# ensuring all observations from a given site appear exclusively in either
# the training or validation set, preventing center-level data leakage.
# ============================================================================

set.seed(123)
cv_splits <- group_vfold_cv(
 df,
 group = center_id,
 v = PARAMS$cv_folds,
 repeats = PARAMS$cv_repeats
)

cat(sprintf("Cross-validation: %d-fold grouped CV, %d repeats = %d total resamples\n",
           PARAMS$cv_folds, PARAMS$cv_repeats, PARAMS$cv_folds * PARAMS$cv_repeats))
```

## Preprocessing Recipe

```{r preprocessing-recipe}
data_recipe <- recipe(motor_score_12m ~ ., data = df) %>%
 # Exclude ID columns from modeling
 update_role(patient_id, new_role = "ID") %>%
 update_role(patient_id_global, new_role = "ID") %>%
 update_role(center_id, new_role = "ID") %>%
 # Preprocessing steps
 step_novel(all_nominal_predictors()) %>%
 step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
 step_normalize(all_numeric_predictors(),
                -all_of(c("patient_id", "patient_id_global", "center_id"))) %>%
 step_integer(all_logical_predictors()) %>%
 step_zv(all_predictors())

cat("Preprocessing recipe defined.\n")
cat("Steps: dummy encoding -> normalization -> zero-variance removal\n")
```

# PHASE 4: MODEL SPECIFICATION {.tabset}

## Base Learner Specifications

```{r baselearner-specs}
# ============================================================================
# BASE LEARNER SELECTION RATIONALE
# ============================================================================
# Five diverse, CPU-compatible algorithms selected to maximize complementarity:
# - Linear (ElasticNet): Captures linear relationships, interpretable
# - Tree-based (RF, XGBoost): Captures non-linear interactions
# - Kernel-based (SVM): Captures complex decision boundaries
# - Spline-based (MARS): Captures piecewise-linear relationships
# ============================================================================

# 1. ElasticNet (penalized linear regression)
set.seed(123)
elnet_spec <- linear_reg(penalty = tune(), mixture = tune()) %>%
 set_engine("glmnet") %>%
 set_mode("regression")

# 2. Random Forest
set.seed(123)
rf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 2000) %>%
 set_engine("ranger", num.threads = 1) %>%  # Single-threaded to prevent oversubscription
 set_mode("regression")

# 3. XGBoost
# ============================================================================
# NOTE: We intentionally OMIT early stopping during tuning.
# Early stopping with an internal validation split creates data leakage within
# CV folds. The cross-validation process itself acts as regularization.
# Reference: https://tune.tidymodels.org/articles/extras/optimizations.html
# ============================================================================
set.seed(123)
xgb_spec <- boost_tree(
 trees = 2000,
 tree_depth = tune(),
 learn_rate = tune(),
 min_n = tune(),
 loss_reduction = tune(),
 sample_size = tune()
) %>%
 set_engine("xgboost", nthread = 1) %>%
 set_mode("regression")

# 4. SVM (Radial Basis Function kernel)
set.seed(123)
svm_spec <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
 set_engine("kernlab") %>%
 set_mode("regression")

# 5. MARS (Multivariate Adaptive Regression Splines)
set.seed(123)
mars_spec <- mars(num_terms = tune(), prod_degree = tune()) %>%
 set_engine("earth") %>%
 set_mode("regression")

cat("Base learner specifications defined: ElasticNet, RF, XGBoost, SVM, MARS\n")
```

## Hyperparameter Grids

```{r hyperparameter-grids}
# ============================================================================
# PRAGMATIC ADAPTATION #2: Constrained Hyperparameter Grids
# ============================================================================
# Trade-off: Accepts potentially suboptimal hyperparameters for tractable
# computation time. Standard literature grids (50-100 configs per algorithm)
# would require ~5x more compute time.
#
# Strategy:
# - Random grids for high-dimensional spaces (ElasticNet, XGBoost, SVM)
# - Regular grids for lower-dimensional spaces (RF, MARS)
# ============================================================================

set.seed(123)

# ElasticNet: 2D random grid
elnet_grid <- grid_random(
 penalty(range = c(-5, 1)),
 mixture(range = c(0, 1)),
 size = PARAMS$grid_size
)

# Random Forest: Regular grid (2 parameters)
rf_grid <- grid_regular(
 mtry(range = c(5, 20)),
 min_n(range = c(5, 15)),
 levels = c(mtry = 5, min_n = 5)
)

# XGBoost: 5D random grid
xgb_grid <- grid_random(
 tree_depth(range = c(3, 10)),
 min_n(range = c(2, 20)),
 learn_rate(range = c(0.001, 0.3)),
 loss_reduction(range = c(0.001, 10)),
 sample_prop(range = c(0.5, 1.0)),
 size = PARAMS$grid_size
)

# SVM: 2D random grid
svm_grid <- grid_random(
 cost(range = c(-5, 2)),
 rbf_sigma(range = c(-5, 0)),
 size = PARAMS$grid_size
)

# MARS: Regular grid (2 parameters)
mars_grid <- grid_regular(
 num_terms(range = c(5, 25)),
 prod_degree(range = c(1L, 2L)),
 levels = c(num_terms = 5, prod_degree = 5)
)

cat("Hyperparameter grids defined.\n")
```

## Training Count Calculation

```{r training-counts}
# Calculate exact number of model training episodes for feasibility reporting

training_counts <- tibble(
 Model = c("ElasticNet", "Random Forest", "XGBoost", "SVM", "MARS"),
 Grid_Size = c(nrow(elnet_grid), nrow(rf_grid), nrow(xgb_grid),
               nrow(svm_grid), nrow(mars_grid)),
 CV_Folds = PARAMS$cv_folds,
 CV_Repeats = PARAMS$cv_repeats
) %>%
 mutate(
   Models_Per_Repeat = Grid_Size * CV_Folds,
   Total_Models = Models_Per_Repeat * CV_Repeats
 )

total_training_episodes <- sum(training_counts$Total_Models)

cat(sprintf("\n=== TRAINING COMPLEXITY ===\n"))
cat(sprintf("Total model training episodes: %d\n\n", total_training_episodes))
print(training_counts)

gc() #call garbage collection 
```

```{r grid-sensitivity-analysis, eval=FALSE}
# ============================================================================
# GRID SIZE SENSITIVITY ANALYSIS
# ============================================================================
# Demonstrates how computational requirements scale with grid size
# Only runs ElasticNet to minimize total runtime while showing scaling behavior
# Set eval=TRUE to run this analysis
# ============================================================================

sensitivity_results <- tibble(
  grid_size = integer(),
  time_seconds = numeric(),
  peak_rss_gb = numeric()
)

test_grid_sizes <- c(5, 15, 30)  # Test 3 configurations

for (gs in test_grid_sizes) {
  
  cat(sprintf("\n--- Testing grid_size = %d ---\n", gs))
  
  # Create grid for this size
  test_grid <- grid_random(
    penalty(range = c(-5, 1)),
    mixture(range = c(0, 1)),
    size = gs
  )
  
  # Create minimal CV splits (2-fold, 1 repeat for speed)
  test_splits <- group_vfold_cv(df, group = center_id, v = 2, repeats = 1)
  
  # Time the tuning
  start_time <- Sys.time()
  start_mem <- as.numeric(pryr::mem_used()) / (1024^3)
  
  set.seed(123)
  test_result <- tune_grid(
    workflow() %>%
      add_recipe(data_recipe) %>%
      add_model(elnet_spec),
    resamples = test_splits,
    grid = test_grid,
    metrics = metric_set(rmse),
    control = control_grid(verbose = FALSE)
  )
  
  end_time <- Sys.time()
  end_mem <- as.numeric(pryr::mem_used()) / (1024^3)
  
  elapsed <- as.numeric(difftime(end_time, start_time, units = "secs"))
  
  sensitivity_results <- sensitivity_results %>%
    add_row(
      grid_size = gs,
      time_seconds = elapsed,
      peak_rss_gb = max(start_mem, end_mem)
    )
  
  cat(sprintf("   Time: %.1f seconds, Memory: %.2f GB\n", elapsed, end_mem))
}

# Plot scaling behavior
sensitivity_plot <- ggplot(sensitivity_results, aes(x = grid_size, y = time_seconds)) +
  geom_point(size = 4, color = "#2E86AB") +
  geom_line(linewidth = 1, color = "#2E86AB") +
  geom_smooth(method = "lm", se = FALSE, linetype = "dashed", color = "gray50") +
  labs(
    title = "Computational Scaling: Grid Size vs. Tuning Time",
    subtitle = "ElasticNet only, 2-fold CV, 1 repeat (demonstration of scaling behavior)",
    x = "Hyperparameter Grid Size",
    y = "Tuning Time (seconds)"
  ) +
  theme_minimal()

print(sensitivity_plot)
print(sensitivity_results)

# Project to full workflow
cat("\n--- Projected Full Workflow Times ---\n")
# Assuming linear scaling, project what full 5-algorithm workflow would take
for (i in 1:nrow(sensitivity_results)) {
  gs <- sensitivity_results$grid_size[i]
  time_sec <- sensitivity_results$time_seconds[i]
  # Scale up: 5 algorithms, 5 folds, 2+ repeats
  projected_min <- (time_sec * 5 * 2.5 * PARAMS$cv_repeats) / 60
  cat(sprintf("   grid_size=%d: ~%.1f minutes projected\n", gs, projected_min))
}
```

# PHASE 5: MODEL TRAINING {.tabset}

## Base Learner Tuning

```{r baselearner-tuning}
# ============================================================================
# START UNIFIED MEMORY MONITOR (runs for entire training + stacking + SHAP)
# ============================================================================
if (file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")
if (file.exists(MEMORY_LOG_FILE)) file.remove(MEMORY_LOG_FILE)
if (file.exists(PHASE_MARKER_FILE)) file.remove(PHASE_MARKER_FILE)

bg_monitor <- callr::r_bg(
  monitor_unified_ram,
  args = list(log_file = MEMORY_LOG_FILE, phase_file = PHASE_MARKER_FILE),
  supervise = TRUE
)
on.exit({
  try({ bg_monitor$kill() }, silent = TRUE)
  if(file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")
}, add = TRUE)
Sys.sleep(3)  # Allow monitor to initialize

# Set phase
set_monitor_phase("Base Learner Tuning")

# Create workflow set
tic.clearlog()
tic("Base learners tuning")

set.seed(123)
df_workflow_set <- workflow_set(
 preproc = list(recipe = data_recipe),
 models = list(
   elnet = elnet_spec,
   rf = rf_spec,
   xgb = xgb_spec,
   svm = svm_spec,
   mars = mars_spec
 )
) %>%
 option_add(grid = elnet_grid, id = "recipe_elnet") %>%
 option_add(grid = rf_grid, id = "recipe_rf") %>%
 option_add(grid = xgb_grid, id = "recipe_xgb") %>%
 option_add(grid = svm_grid, id = "recipe_svm") %>%
 option_add(grid = mars_grid, id = "recipe_mars")

# Control settings for stacking (saves predictions)
ctrl_stack <- control_stack_grid()

# Run parallel tuning
set.seed(123)
model_set_results <- workflow_map(
 df_workflow_set,
 fn = "tune_grid",
 resamples = cv_splits,
 metrics = metric_set(rmse, mae, rsq),
 control = ctrl_stack,
 verbose = TRUE
)

timing_baselearners <- toc(log = TRUE, quiet = TRUE)
save_timing("Base learners tuning", timing_baselearners)

cat(sprintf("\nTuning complete: %d models trained in %.1f minutes\n",
           total_training_episodes,
           (timing_baselearners$toc - timing_baselearners$tic) / 60))

```

## Stacked Ensemble Construction

```{r stacking}

set_monitor_phase("Stacking")

tic.clearlog()
tic("Stacking and meta-learner fitting")

# Initialize stack with all candidates
set.seed(123)
df_stack <- stacks() %>%
 add_candidates(model_set_results)

cat(sprintf("Stack initialized with %d candidate models\n", ncol(df_stack) - 1))

# Blend with non-negative LASSO meta-learner
set.seed(123)
df_stack_blended <- df_stack %>%
 blend_predictions(
   penalty = 10^seq(-4, -0.5, length.out = 200),
   mixture = 1,
   non_negative = TRUE,
   metric = metric_set(rmse)
 )

cat("Stack blended successfully\n")

# Fit final ensemble on full training data
set.seed(123)
df_stack_fitted <- df_stack_blended %>%
 fit_members()

timing_stacking <- toc(log = TRUE, quiet = TRUE)
save_timing("Stacking and meta-learner fitting", timing_stacking)

cat("Final stacked ensemble fitted.\n")
record_memory("After stacking and fitting")

gc() #call garbage collection 
```

# PHASE 6: EVALUATION {.tabset}

## Two-Stage CV Evaluation

```{r two-stage-evaluation}
# ============================================================================
# PRAGMATIC ADAPTATION #3: Two-Stage Evaluation Framework
# ============================================================================
# Stage 1 (completed above): Hyperparameter optimization using grouped CV
# Stage 2 (this chunk): Independent CV evaluation of the fitted ensemble
#
# This avoids the 125-fold computational cost of fully nested CV while still
# producing approximately unbiased performance estimates for the final stack.
# ============================================================================

# Create L1 dataset (base learner predictions + IDs)
l1_data <- df_stack %>%
 mutate(.row = row_number()) %>%
 left_join(
   df %>%
     mutate(.row = row_number()) %>%
     select(.row, patient_id_global, center_id),
   by = ".row"
 )

cat(sprintf("L1 dataset created: %d observations\n", nrow(l1_data)))

# ============================================================================
# METHODOLOGICAL NOTE: Independent Seed for Stage 2 Evaluation
# ============================================================================
# We deliberately use a DIFFERENT seed (456) here than in Stage 1 tuning (123).
#
# Rationale: Stage 2 creates NEW, INDEPENDENT cross-validation folds that were
# never used during hyperparameter optimization. This ensures:
#   1. The meta-learner's penalty (lambda) is evaluated on truly held-out data
#   2. Performance estimates are not contaminated by optimization bias
#   3. The two-stage evaluation maintains methodological independence
#
# Using the same seed would recreate identical fold assignments, defeating the
# purpose of the two-stage evaluation framework described in Methods Section 2.5.2
# ============================================================================
set.seed(456)  # Different seed ensures fold independence from Stage 1
l1_folds <- group_vfold_cv(l1_data, group = center_id, v = 5)

# Get optimal penalty from Stage 1
best_penalty <- df_stack_blended$penalty$penalty
cat(sprintf("Using meta-learner penalty from Stage 1: %.6f\n", best_penalty))

# Define meta-learner spec (using !! to inject the fixed penalty value)
meta_spec <- linear_reg(penalty = !!best_penalty, mixture = 1) %>%
 set_engine("glmnet", lower.limits = 0)

# Meta-learner recipe
meta_recipe <- recipe(motor_score_12m ~ ., data = l1_data) %>%
 update_role(center_id, new_role = "ID") %>%
 update_role(patient_id_global, new_role = "ID") %>%
 update_role(.row, new_role = "ID")

# Create workflow and evaluate
meta_wf <- workflow() %>%
 add_recipe(meta_recipe) %>%
 add_model(meta_spec)

tic.clearlog()
tic("Meta-learner CV evaluation")

set.seed(456)
meta_cv_results <- fit_resamples(
 meta_wf,
 resamples = l1_folds,
 metrics = metric_set(rmse, rsq, mae),
 control = control_resamples(save_pred = TRUE)
)

timing_metacv <- toc(log = TRUE, quiet = TRUE)
save_timing("Meta-learner CV evaluation", timing_metacv)

# Extract predictions
cv_predictions <- tidyr::unnest(meta_cv_results, .predictions)

plot_data_cv <- cv_predictions %>%
 left_join(
   df %>%
     mutate(.row = row_number()) %>%
     select(.row, patient_id_global, baseline_ais),
   by = ".row"
 ) %>%
 select(patient_id_global, motor_score_12m, .pred, baseline_ais)

# Final CV metrics
final_cv_metrics <- collect_metrics(meta_cv_results)

cat(sprintf("Out-of-fold predictions: %d observations\n", nrow(plot_data_cv)))
cat("Stage 2 evaluation complete.\n")
```

## Bootstrap Confidence Intervals

```{r bootstrap-ci}
tic.clearlog()
tic("Bootstrap CIs")

set.seed(789)
bootstrap_metrics <- map_dfr(1:PARAMS$bootstrap_iters, function(i) {
 boot_sample <- plot_data_cv %>%
   slice_sample(prop = 1, replace = TRUE)
 
 tibble(
   iteration = i,
   rmse = rmse_vec(boot_sample$motor_score_12m, boot_sample$.pred),
   mae = mae_vec(boot_sample$motor_score_12m, boot_sample$.pred),
   rsq = rsq_vec(boot_sample$motor_score_12m, boot_sample$.pred)
 )
})

bootstrap_ci <- bootstrap_metrics %>%
 summarize(
   across(c(rmse, mae, rsq),
          list(lower = ~quantile(., 0.025), upper = ~quantile(., 0.975)))
 )

timing_bootstrap <- toc(log = TRUE, quiet = TRUE)
save_timing("Bootstrap CIs", timing_bootstrap)

cat("Bootstrap 95% Confidence Intervals:\n")
print(bootstrap_ci)

gc() #call garbage collection 
```

## Performance Metrics Table

```{r performance-table}
# Get unbiased stack metrics
stack_metrics_formatted <- final_cv_metrics %>%
 filter(.metric %in% c("rmse", "rsq", "mae")) %>%
 mutate(
   ci_lower = mean - 1.96 * std_err,
   ci_upper = mean + 1.96 * std_err,
   `Mean (95% CI)` = case_when(
     .metric == "rsq" ~ sprintf("%.3f [%.3f, %.3f]", mean, ci_lower, ci_upper),
     TRUE ~ sprintf("%.2f [%.2f, %.2f]", mean, ci_lower, ci_upper)
   ),
   Metric = case_when(
     .metric == "mae" ~ "MAE",
     .metric == "rmse" ~ "RMSE",
     .metric == "rsq" ~ "R-squared"
   )
 ) %>%
 select(Metric, `Mean (95% CI)`) %>%
 mutate(Model = "Stacked Ensemble")

# Get base learner metrics (optimistically biased from Stage 1)
base_learners_formatted <- model_set_results %>%
 rank_results(select_best = TRUE) %>%
 filter(.metric %in% c("rmse", "rsq", "mae")) %>%
 select(wflow_id, .metric, mean, std_err) %>%
 mutate(
   Model = case_when(
     stringr::str_detect(wflow_id, "elnet") ~ "Best ElasticNet",
     stringr::str_detect(wflow_id, "rf") ~ "Best Random Forest",
     stringr::str_detect(wflow_id, "xgb") ~ "Best XGBoost",
     stringr::str_detect(wflow_id, "svm") ~ "Best SVM",
     stringr::str_detect(wflow_id, "mars") ~ "Best MARS"
   ),
   ci_lower = mean - 1.96 * std_err,
   ci_upper = mean + 1.96 * std_err,
   `Mean (95% CI)` = case_when(
     .metric == "rsq" ~ sprintf("%.3f [%.3f, %.3f]", mean, ci_lower, ci_upper),
     TRUE ~ sprintf("%.2f [%.2f, %.2f]", mean, ci_lower, ci_upper)
   ),
   Metric = case_when(
     .metric == "mae" ~ "MAE",
     .metric == "rmse" ~ "RMSE",
     .metric == "rsq" ~ "R-squared"
   )
 ) %>%
 select(Model, Metric, `Mean (95% CI)`)

# Combine
n_retained_models <- df_stack_blended$coefs %>%
 broom::tidy() %>%
 filter(term != "(Intercept)", estimate > 0) %>%
 nrow()

full_performance_table <- bind_rows(base_learners_formatted, stack_metrics_formatted) %>%
 pivot_wider(names_from = Metric, values_from = `Mean (95% CI)`) %>%
 mutate(Model = factor(Model, levels = c(
   "Best ElasticNet", "Best Random Forest", "Best XGBoost",
   "Best SVM", "Best MARS", "Stacked Ensemble"
 ))) %>%
 arrange(Model)

# Display table
full_performance_table %>%
 gt() %>%
 tab_header(
   title = "Cross-Validated Performance Metrics",
   subtitle = sprintf("Stacked ensemble (retained %d candidates) vs. best base learners",
                      n_retained_models)
 ) %>%
 tab_footnote(
   footnote = "Base learner metrics are optimistically biased (Stage 1). Stack metrics are unbiased (Stage 2).",
   locations = cells_column_labels(columns = Model)
 ) %>%
 tab_style(
   style = list(cell_fill(color = "#E8F8F5"), cell_text(weight = "bold")),
   locations = cells_body(rows = Model == "Stacked Ensemble")
 )
```

# PHASE 7: EXPLAINABILITY {.tabset}

## Permutation Importance

```{r permutation-importance}
set_monitor_phase("Permutation Importance")

# Create custom predict function for stacked model
predict_function_stack <- function(model, newdata) {
 pred <- predict(model, newdata)
 return(pred$.pred)
}

# Prepare data (exclude outcome, keep all predictors including IDs for recipe)
explain_data <- df %>% select(-motor_score_12m)
explain_y <- df$motor_score_12m

# Create DALEX explainer
set.seed(123)
explainer_stack <- DALEX::explain(
 model = df_stack_fitted,
 data = explain_data,
 y = explain_y,
 predict_function = predict_function_stack,
 verbose = FALSE
)

cat("DALEX explainer created successfully\n")

# Calculate permutation-based variable importance
tic.clearlog()
tic("Permutation importance")

set.seed(123)
vi_permutation <- DALEX::model_parts(
 explainer_stack,
 loss_function = DALEX::loss_root_mean_square,
 B = PARAMS$dalex_b,
 type = "difference"
)

timing_perm <- toc(log = TRUE, quiet = TRUE)
save_timing("Permutation importance", timing_perm)

cat("Permutation importance calculated.\n")
```

## SHAP Values

```{r adaptive-shap-workers}
# 1. Robust RAM Detection Helper
get_system_ram_gb <- function() {
  tryCatch({
    if (Sys.info()[["sysname"]] == "Windows") {
      # Windows: Use wmic
      ram_bytes <- system("wmic computersystem get totalphysicalmemory", intern = TRUE)
      as.numeric(grep("\\d+", ram_bytes, value = TRUE)) / (1024^3)
    } else {
      # Linux/macOS: Use generic detection via system command
      if (file.exists("/proc/meminfo")) {
        # Linux
        mem_info <- readLines("/proc/meminfo")
        total_kb <- as.numeric(gsub("\\D", "", grep("MemTotal", mem_info, value = TRUE)))
        total_kb / (1024^2)
      } else {
        # macOS fallback
        as.numeric(substring(system("sysctl hw.memsize", intern = TRUE), 13)) / (1024^3)
      }
    }
  }, error = function(e) {
    # Fail-safe default: Assume constrained environment (8GB)
    warning("Could not detect system RAM. Defaulting to conservative 8GB limit.")
    return(8) 
  })
}

# 2. Get Metrics
system_ram_gb <- get_system_ram_gb()
model_size_gb <- as.numeric(lobstr::obj_size(df_stack_fitted)) / (1024^3)
safety_threshold_gb <- 8.0 

# 3. Determine Base Worker Count
if (system_ram_gb <= (safety_threshold_gb + 0.5)) {
  # Constrained Mode (<= ~8GB)
  proposed_workers <- min(2, num_workers)
  mode_msg <- "CONSTRAINED MODE (<= 8GB RAM)"
} else {
  # Performance Mode (> 8GB)
  proposed_workers <- min(4, num_workers)
  mode_msg <- "PERFORMANCE MODE (> 8GB RAM)"
}

# 4. Safety Override (The "Anti-Crash" Check)
# In multisession (socket) parallelization, each worker copies the model object.
# We need: (Model_Size * Workers) + Overhead < System RAM
est_memory_load <- model_size_gb * proposed_workers
safe_limit <- system_ram_gb * 0.60 # Leave 40% for OS and overhead

if (est_memory_load > safe_limit) {
  shap_workers <- 1
  warning_msg <- sprintf("Memory safety override triggered. Model (%.2f GB) too large for parallel workers.", model_size_gb)
} else {
  shap_workers <- proposed_workers
  warning_msg <- NULL
}

# 5. Report Configuration
cat(sprintf("\n--- SHAP PARALLEL CONFIGURATION ---\n"))
cat(sprintf("System RAM detected: %.2f GB\n", system_ram_gb))
cat(sprintf("Model object size:   %.2f GB\n", model_size_gb))
cat(sprintf("Mode:                %s\n", mode_msg))
if (!is.null(warning_msg)) cat(sprintf("WARNING: %s\n", warning_msg))
cat(sprintf("Final Worker Count:  %d\n", shap_workers))
cat("-----------------------------------\n\n")

# 6. Apply Configuration
# Clean memory before starting heavy task
gc() 

original_plan <- plan() 
if (shap_workers > 1) {
  plan(multisession, workers = shap_workers)
} else {
  plan(sequential)
}
```


```{r shap-values, eval=TRUE}
# ============================================================================
# PRAGMATIC ADAPTATION #4: Approximate SHAP via fastshap
# ============================================================================
# Trade-off: Approximate vs. exact Shapley values
# Exact SHAP requires 2^p coalition evaluations (for 26 predictors: >67 million
# evaluations per observation). fastshap uses Monte Carlo sampling for ~50x speedup.
# ============================================================================
set_monitor_phase("SHAP Calculation")

# Prediction wrapper
pfun <- function(object, newdata) {
 predict(object, newdata)$.pred
}

tic.clearlog()
tic("SHAP calculation")

set.seed(123)
shap_values <- fastshap::explain(
 object = df_stack_fitted,
 X = explain_data,
 pred_wrapper = pfun,
 nsim = PARAMS$shap_nsim,
 adjust = TRUE,
 parallel = TRUE, 
  # Explicitly export packages required by the Stacked Ensemble prediction method
  .packages = c("stacks", "tidymodels", "dplyr", "ranger", "xgboost", "earth", "kernlab")
)

# Restore the original global parallel plan
plan(original_plan)

timing_shap <- toc(log = TRUE, quiet = TRUE)
save_timing("SHAP calculation", timing_shap)

# Compute mean |SHAP|
shap_importance <- shap_values %>%
 as_tibble() %>%
 summarize(across(everything(), ~mean(abs(.)))) %>%
 pivot_longer(everything(), names_to = "feature", values_to = "mean_abs_shap") %>%
 arrange(desc(mean_abs_shap)) %>%
 slice_head(n = 20)

record_memory("After SHAP calculation")

cat("SHAP values calculated.\n")
```

```{r stop-monitor-and-plot, fig.width=12, fig.height=6}
# ============================================================================
# STOP MONITOR AND GENERATE MEMORY TIMELINE FIGURE
# ============================================================================

# Stop the unified monitor
set_monitor_phase("Complete")
Sys.sleep(2)
file.create("stop_monitoring.file")
Sys.sleep(3)
tryCatch(bg_monitor$kill(), error = function(e) NULL)
if (file.exists("stop_monitoring.file")) file.remove("stop_monitoring.file")

# Read the unified log
if (file.exists(MEMORY_LOG_FILE)) {
  memory_timeline <- read.csv(MEMORY_LOG_FILE, stringsAsFactors = FALSE)
  
  # Calculate key metrics
  global_peak_rss_gb <- max(memory_timeline$rss_gb, na.rm = TRUE)
  global_peak_vms_gb <- max(memory_timeline$vms_gb, na.rm = TRUE)
  total_runtime_min <- max(memory_timeline$elapsed_sec, na.rm = TRUE) / 60
  
  # Find phase boundaries for annotation
  phase_changes <- memory_timeline %>%
    mutate(phase_change = phase != lag(phase, default = "")) %>%
    filter(phase_change) %>%
    select(elapsed_sec, phase) %>%
    mutate(elapsed_min = elapsed_sec / 60)
  
  # Print summary
  cat("\n")
  cat("==================================================================\n")
  cat("           GLOBAL MEMORY CONSUMPTION SUMMARY                      \n")
  cat("==================================================================\n")
  cat(sprintf("Peak Physical RAM (RSS): %.2f GB\n", global_peak_rss_gb))
  cat(sprintf("Peak Virtual Memory (VMS): %.2f GB\n", global_peak_vms_gb))
  cat(sprintf("Total Runtime: %.1f minutes (%.2f hours)\n", 
              total_runtime_min, total_runtime_min / 60))
  cat("------------------------------------------------------------------\n")
  cat(sprintf("8GB Feasibility Threshold: %s\n", 
              ifelse(global_peak_rss_gb < 8, "PASSED", "FAILED")))
  cat(sprintf("Headroom: %.2f GB (%.1f%% of 8GB limit)\n",
              8 - global_peak_rss_gb, 
              100 * (8 - global_peak_rss_gb) / 8))
  cat("==================================================================\n\n")
  
  # Store for later use in feasibility summary
  feasibility_result <- list(
    peak_rss_gb = global_peak_rss_gb,
    peak_vms_gb = global_peak_vms_gb,
    runtime_min = total_runtime_min,
    passed_8gb = global_peak_rss_gb < 8
  )
  
  # Create publication-quality figure
  memory_plot <- memory_timeline %>%
    mutate(elapsed_min = elapsed_sec / 60) %>%
    pivot_longer(cols = c(rss_gb, vms_gb), 
                 names_to = "memory_type", 
                 values_to = "gb") %>%
    mutate(memory_type = case_when(
      memory_type == "rss_gb" ~ "Physical RAM (RSS)",
      memory_type == "vms_gb" ~ "Virtual Memory (VMS)"
    )) %>%
    ggplot(aes(x = elapsed_min, y = gb, color = memory_type)) +
    geom_line(linewidth = 0.8, alpha = 0.8) +
    # Add 8GB threshold line
    geom_hline(yintercept = 8, linetype = "dashed", color = "red", linewidth = 1) +
    annotate("text", x = max(memory_timeline$elapsed_sec/60) * 0.02, y = 8.3, 
             label = "8GB Feasibility Threshold", hjust = 0, color = "red", 
             fontface = "bold", size = 3.5) +
    # Add phase boundary lines
    geom_vline(data = phase_changes, aes(xintercept = elapsed_min),
               linetype = "dotted", color = "gray40", alpha = 0.7) +
    # Add phase labels at top
    geom_text(data = phase_changes, 
              aes(x = elapsed_min, y = max(memory_timeline$vms_gb, na.rm = TRUE) * 0.95, 
                  label = phase),
              angle = 90, hjust = 1, vjust = -0.5, size = 3, color = "gray30") +
    scale_color_manual(values = c("Physical RAM (RSS)" = "#2E86AB", 
                                   "Virtual Memory (VMS)" = "#A23B72")) +
    labs(
      title = "Memory Consumption Throughout Stacked Ensemble Workflow",
      subtitle = sprintf("Peak RSS: %.2f GB | Runtime: %.1f min | 8GB Threshold: %s",
                        global_peak_rss_gb, total_runtime_min,
                        ifelse(global_peak_rss_gb < 8, "PASSED", "FAILED")),
      x = "Elapsed Time (minutes)",
      y = "Memory (GB)",
      color = "Memory Type"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(size = 11, color = "gray30"),
      legend.position = "bottom",
      panel.grid.minor = element_blank()
    ) +
    coord_cartesian(ylim = c(0, max(10, global_peak_vms_gb * 1.1)))
  
  print(memory_plot)
  
  # Save high-res version
  ggsave("memory_timeline.png", memory_plot, width = 12, height = 6, dpi = 300)
  cat("Memory timeline figure saved as memory_timeline.png\n")
  
} else {
  warning("Memory log file not found. Monitor may not have run correctly.")
  feasibility_result <- list(
    peak_rss_gb = NA,
    peak_vms_gb = NA,
    runtime_min = NA,
    passed_8gb = NA
  )
}
```


# PHASE 8: FEASIBILITY REPORTING {.tabset}

## Meta-Learner Weights

```{r metalearner-weights, fig.width=10, fig.height=7}
# Extract and visualize meta-learner coefficients

meta_weights <- df_stack_fitted$coefs %>%
 broom::tidy() %>%
 filter(term != "(Intercept)") %>%
 mutate(
   model_type = case_when(
     stringr::str_detect(term, "elnet") ~ "ElasticNet",
     stringr::str_detect(term, "rf") ~ "Random Forest",
     stringr::str_detect(term, "xgb") ~ "XGBoost",
     stringr::str_detect(term, "svm") ~ "SVM",
     stringr::str_detect(term, "mars") ~ "MARS",
     TRUE ~ "Other"
   ),
   estimate = ifelse(estimate > 0, estimate, 0)
 ) %>%
 filter(estimate > 0) %>%
 arrange(desc(estimate))

# Aggregate by model type
weights_by_model <- meta_weights %>%
 group_by(model_type) %>%
 summarize(total_weight = sum(estimate), n_members = n(), .groups = "drop") %>%
 arrange(desc(total_weight))

# Store summary
meta_coef_summary <- tibble(
 total_candidates = ncol(df_stack) - 1,
 selected_members = nrow(meta_weights),
 selection_rate = nrow(meta_weights) / (ncol(df_stack) - 1),
 dominant_model = weights_by_model$model_type[1],
 dominant_weight_pct = 100 * weights_by_model$total_weight[1] / sum(weights_by_model$total_weight)
)

# Visualization
weights_plot_data <- weights_by_model %>%
 mutate(
   pct = 100 * total_weight / sum(total_weight),
   label_text = sprintf("%.1f%%\n%d members", pct, n_members)
 )

ggplot(weights_plot_data,
      aes(x = reorder(model_type, total_weight), y = total_weight, fill = model_type)) +
 geom_col(alpha = 0.85, width = 0.7) +
 geom_text(aes(label = label_text), hjust = -0.1, size = 3.5, fontface = "bold") +
 coord_flip(ylim = c(0, max(weights_plot_data$total_weight) * 1.2)) +
 scale_fill_brewer(palette = "Set2", guide = "none") +
 labs(
   title = "Meta-Learner Weight Distribution by Base Model Type",
   subtitle = sprintf("Non-negative LASSO selected %d of %d candidate models",
                      sum(weights_plot_data$n_members), ncol(df_stack) - 1),
   x = "Base Model Type",
   y = "Total Weight (Sum of LASSO Coefficients)"
 ) +
 theme_minimal() +
 theme(
   plot.title = element_text(face = "bold", size = 14),
   plot.subtitle = element_text(size = 10, color = "gray40"),
   axis.title = element_text(size = 11, face = "bold"),
   panel.grid.major.y = element_blank()
 )
```

```{r feasibility-tradeoff-figure, fig.width=10, fig.height=7}
# ============================================================================
# FEASIBILITY TRADE-OFF FIGURE
# ============================================================================
# Shows the performance-computation trade-off: what you get for your compute
# ============================================================================

# Extract per-model best performance
model_performance <- model_set_results %>%
  rank_results(select_best = TRUE) %>%
  filter(.metric == "rmse") %>%
  select(wflow_id, mean, std_err) %>%
  mutate(
    model = case_when(
      str_detect(wflow_id, "elnet") ~ "ElasticNet",
      str_detect(wflow_id, "rf") ~ "Random Forest",
      str_detect(wflow_id, "xgb") ~ "XGBoost",
      str_detect(wflow_id, "svm") ~ "SVM",
      str_detect(wflow_id, "mars") ~ "MARS"
    ),
    ci_lower = mean - 1.96 * std_err,
    ci_upper = mean + 1.96 * std_err,
    type = "Base Learner"
  )

# Add stack performance
stack_perf <- final_cv_metrics %>%
  filter(.metric == "rmse") %>%
  mutate(
    model = "Stacked Ensemble",
    ci_lower = mean - 1.96 * std_err,
    ci_upper = mean + 1.96 * std_err,
    type = "Ensemble",
    wflow_id = "stack"
  ) %>%
  select(wflow_id, mean, std_err, model, ci_lower, ci_upper, type)

# Get training episodes per model
training_effort <- training_counts %>%
  mutate(model = Model) %>%
  select(model, Total_Models)

# Combine
plot_data <- bind_rows(model_performance, stack_perf) %>%
  left_join(training_effort, by = "model") %>%
  mutate(
    # Stack uses all training episodes
    Total_Models = ifelse(model == "Stacked Ensemble", 
                          total_training_episodes, 
                          Total_Models)
  )

# Create figure
tradeoff_plot <- ggplot(plot_data, aes(x = Total_Models, y = mean, color = type)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0, linewidth = 1) +
  geom_text(aes(label = model), vjust = -1.5, hjust = 0.5, size = 3.5) +
  scale_color_manual(values = c("Base Learner" = "#2E86AB", "Ensemble" = "#E94F37")) +
  labs(
    title = "Performance vs. Computational Investment",
    subtitle = "Trade-off between model training effort and prediction accuracy",
    x = "Training Episodes (model fits)",
    y = "RMSE (lower is better)",
    color = "Model Type"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 11, color = "gray30"),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  ) +
  # Expand x-axis slightly for labels
  scale_x_continuous(expand = expansion(mult = c(0.05, 0.15)))

print(tradeoff_plot)

# Save
ggsave("feasibility_tradeoff.png", tradeoff_plot, width = 10, height = 7, dpi = 300)
cat("Feasibility trade-off figure saved.\n")
```

## Deployment Metrics

```{r deployment-metrics}
# ============================================================================
# DEPLOYMENT FEASIBILITY METRICS
# ============================================================================

# 1. Model object size (RAM footprint for inference)
model_ram_size_mb <- as.numeric(lobstr::obj_size(df_stack_fitted)) / (1024^2)

# 2. Serialized file size (disk/transfer footprint)
saveRDS(df_stack_fitted, "temp_final_model.rds", compress = "xz")
model_disk_size_mb <- file.size("temp_final_model.rds") / (1024^2)
file.remove("temp_final_model.rds")
gc()

# 3. Single-patient inference latency
new_patient_profile <- df[1, ] %>% select(-motor_score_12m)

# Warm-up run
invisible(predict(df_stack_fitted, new_patient_profile))

# Benchmark (100 iterations)
tic.clearlog()
tic("inference_batch")
for(i in 1:100) {
 pred <- predict(df_stack_fitted, new_patient_profile)
}
t_inference <- toc(log = TRUE, quiet = TRUE)

latency_ms <- ((t_inference$toc - t_inference$tic) / 100) * 1000

# Store results
deployment_metrics <- tibble(
 Metric = c("Model Object Size (RAM)", "Serialized Model Size (Disk)", "Inference Latency"),
 Value = c(model_ram_size_mb, model_disk_size_mb, latency_ms),
 Unit = c("MB", "MB", "ms"),
 Implication = c(
   "Minimum RAM required to load model",
   "Bandwidth/storage for deployment",
   "Time per single prediction"
 )
)

timing_results <<- timing_results %>%
 add_row(
   component = "Single-Patient Inference (avg ms)",
   wall_time_seconds = latency_ms / 1000,
   wall_time_formatted = sprintf("%.2f ms", latency_ms)
 )

cat("\n=== DEPLOYMENT FEASIBILITY ===\n")
print(deployment_metrics)
cat(sprintf("\nThroughput: ~%.0f predictions per second\n", 1000/latency_ms))
```

## Environment Footprint

```{r environment-footprint}
# Calculate total package dependency footprint

core_pkgs <- c("tidymodels", "stacks", "glmnet", "ranger", "xgboost",
              "kernlab", "earth", "doFuture", "tictoc", "pryr",
              "dplyr", "tidyr", "purrr", "tibble", "stringr", "forcats",
              "ggplot2", "gtsummary", "gt", "broom", "viridis",
              "DALEX", "DALEXtra", "fastshap", "yardstick", "probably",
              "callr", "ps")

all_deps <- tools::package_dependencies(core_pkgs, recursive = TRUE)
all_pkgs_unique <- unique(c(core_pkgs, unlist(all_deps)))
all_pkgs_unique <- all_pkgs_unique[all_pkgs_unique %in% installed.packages()[, "Package"]]

# Estimate installation size
total_size_bytes <- 0
for (pkg in all_pkgs_unique) {
 loc <- system.file(package = pkg)
 if (loc != "") {
   files <- list.files(loc, recursive = TRUE, full.names = TRUE)
   total_size_bytes <- total_size_bytes + sum(file.info(files)$size, na.rm = TRUE)
 }
}
total_size_mb <- total_size_bytes / (1024^2)

cat(sprintf("\n=== ENVIRONMENT FOOTPRINT ===\n"))
cat(sprintf("Total dependencies: %d packages\n", length(all_pkgs_unique)))
cat(sprintf("Approximate installation size: %.2f MB\n", total_size_mb))
```

## System RAM Profile

```{r system-ram-profile}
# Process monitoring logs and extract peak values

get_peak_ram <- function(logfile, stage_name) {
 if (file.exists(logfile)) {
   df_log <- read.csv(logfile)
   if ("rss_gb" %in% names(df_log)) {
     peak_rss <- max(df_log$rss_gb, na.rm = TRUE)
     peak_vms <- max(df_log$vms_gb, na.rm = TRUE)
   } else {
     peak_rss <- NA_real_
     peak_vms <- NA_real_
   }
   return(tibble(Stage = stage_name, Peak_RSS_GB = peak_rss, Peak_VMS_GB = peak_vms))
 } else {
   return(tibble(Stage = stage_name, Peak_RSS_GB = NA_real_, Peak_VMS_GB = NA_real_))
 }
}

# Read all logs
system_peaks <- bind_rows(
 get_peak_ram("memory_log_tuning.csv", "Base Learner Tuning"),
 get_peak_ram("memory_log_stacking.csv", "Stacking & Refitting"),
 get_peak_ram("memory_log_shap.csv", "Explainability (SHAP)")
)

# Add to memory_results
for (i in 1:nrow(system_peaks)) {
 if (!is.na(system_peaks$Peak_RSS_GB[i])) {
   memory_results <<- memory_results %>%
     add_row(
       component = paste("Peak Physical RAM (RSS):", system_peaks$Stage[i]),
       memory_mb = system_peaks$Peak_RSS_GB[i] * 1024,
       memory_gb = system_peaks$Peak_RSS_GB[i]
     )
 }
}

cat("\n=== SYSTEM RAM PROFILE ===\n")
print(system_peaks)
```

## Computational ROI

```{r computational-roi}
# Calculate return on compute investment

tuning_time_sec <- timing_results %>%
 filter(component == "Base learners tuning") %>%
 pull(wall_time_seconds)

# Best single model performance
best_single_model_data <- model_set_results %>%
 rank_results(select_best = TRUE) %>%
 filter(.metric == "rmse") %>%
 slice_min(mean, n = 1)

single_rmse <- best_single_model_data$mean
single_model_name <- case_when(
 str_detect(best_single_model_data$wflow_id, "elnet") ~ "ElasticNet",
 str_detect(best_single_model_data$wflow_id, "rf") ~ "Random Forest",
 str_detect(best_single_model_data$wflow_id, "xgb") ~ "XGBoost",
 str_detect(best_single_model_data$wflow_id, "svm") ~ "SVM",
 str_detect(best_single_model_data$wflow_id, "mars") ~ "MARS"
)

# Stack performance
stack_rmse <- final_cv_metrics %>% filter(.metric == "rmse") %>% pull(mean)
stack_total_time_sec <- sum(timing_results$wall_time_seconds)
stack_total_time_min <- stack_total_time_sec / 60

# Single algorithm time estimate
single_algo_time_min <- (tuning_time_sec / 60) / 5
single_algo_time_sec <- tuning_time_sec / 5

# Compute efficiency
rmse_improvement_abs <- single_rmse - stack_rmse
rmse_improvement_pct <- 100 * rmse_improvement_abs / single_rmse
extra_time_min <- stack_total_time_min - single_algo_time_min
efficiency_ratio <- rmse_improvement_pct / extra_time_min

cat("\n=== COMPUTATIONAL RETURN ON INVESTMENT ===\n\n")
cat(sprintf("BASELINE (Best Single Model: %s):\n", single_model_name))
cat(sprintf("  RMSE: %.4f\n", single_rmse))
cat(sprintf("  Estimated time: %.1f minutes\n\n", single_algo_time_min))

cat("STACKED ENSEMBLE:\n")
cat(sprintf("  RMSE: %.4f\n", stack_rmse))
cat(sprintf("  Total time: %.1f minutes (%.2f hours)\n\n",
           stack_total_time_min, stack_total_time_min / 60))

cat("COMPARISON:\n")
cat(sprintf("  Performance gain: %.2f%% RMSE improvement\n", rmse_improvement_pct))
cat(sprintf("  Time multiplier: %.1fx increase\n", stack_total_time_sec / single_algo_time_sec))
cat(sprintf("  Extra time invested: %.1f minutes\n\n", extra_time_min))

cat("EFFICIENCY RATIO:\n")
cat(sprintf("  %.4f%% RMSE improvement per minute of extra computation\n", efficiency_ratio))
```

## Theoretical Projections

```{r theoretical-projections}
# Project time requirements for more rigorous approaches

nested_cv_multiplier <- 125 / (PARAMS$cv_folds * PARAMS$cv_repeats)
nested_cv_proj_hours <- (tuning_time_sec * nested_cv_multiplier) / 3600

current_avg_grid <- mean(training_counts$Grid_Size)
large_grid_multiplier <- 50 / current_avg_grid
large_grid_proj_hours <- (tuning_time_sec * large_grid_multiplier) / 3600

theoretical_projections <- tibble(
 Scenario = c("Nested CV (5x5x5)", "Larger Grids (50 configs/algorithm)"),
 Current_Hours = c(tuning_time_sec / 3600, tuning_time_sec / 3600),
 Multiplier = c(nested_cv_multiplier, large_grid_multiplier),
 Projected_Hours = c(nested_cv_proj_hours, large_grid_proj_hours)
)

cat("\n=== THEORETICAL TIME PROJECTIONS ===\n")
print(theoretical_projections)

cat(sprintf("\nInterpretation:\n"))
cat(sprintf("  Current workflow: %.2f hours\n", tuning_time_sec / 3600))
cat(sprintf("  Nested CV would require: %.2f hours (%.1fx increase)\n",
           nested_cv_proj_hours, nested_cv_multiplier))
cat(sprintf("  Standard grids would require: %.2f hours (%.1fx increase)\n",
           large_grid_proj_hours, large_grid_multiplier))
```

## Final Feasibility Summary

```{r feasibility-summary}
# ============================================================================
# DYNAMIC FEASIBILITY SUMMARY
# ============================================================================

peak_memory_gb <- ifelse(exists("feasibility_result") && !is.na(feasibility_result$peak_rss_gb),
                         feasibility_result$peak_rss_gb,
                         max(memory_results$memory_gb, na.rm = TRUE))

total_time_hours <- sum(timing_results$wall_time_seconds, na.rm = TRUE) / 3600
total_time_min <- sum(timing_results$wall_time_seconds, na.rm = TRUE) / 60

cat("==================================================================\n")
cat("           WORKFLOW FEASIBILITY RESULTS                          \n")
cat("==================================================================\n\n")

cat("1. WORKFLOW COMPLETION: SUCCESS\n")
cat("   The complete stacked ensemble workflow executed successfully on\n")
cat("   standard laptop hardware without cloud computing or GPU acceleration.\n\n")

cat("2. RESOURCE CONSUMPTION:\n")
cat(sprintf("   - Total execution time: %.1f minutes (%.2f hours)\n", 
            total_time_min, total_time_hours))
cat(sprintf("   - Peak physical RAM (RSS): %.2f GB\n", peak_memory_gb))
cat(sprintf("   - 8GB feasibility threshold: %s\n",
            ifelse(peak_memory_gb < 8, "PASSED", "FAILED")))
cat(sprintf("   - Headroom remaining: %.2f GB\n\n", 8 - peak_memory_gb))

cat("3. TRAINING COMPLEXITY:\n")
cat(sprintf("   - Total model training episodes: %d\n", total_training_episodes))
cat(sprintf("   - Cross-validation configuration: %d-fold x %d repeats\n",
            PARAMS$cv_folds, PARAMS$cv_repeats))
cat(sprintf("   - Base learner candidates generated: %d\n", ncol(df_stack) - 1))
cat(sprintf("   - Members selected by meta-learner: %d (%.1f%% selection rate)\n\n",
            meta_coef_summary$selected_members,
            100 * meta_coef_summary$selection_rate))

cat("4. ENSEMBLE COMPOSITION:\n")
cat(sprintf("   - Dominant model type: %s (%.1f%% of weight)\n",
            meta_coef_summary$dominant_model,
            meta_coef_summary$dominant_weight_pct))

cat("\n5. DEPLOYMENT FEASIBILITY:\n")
cat(sprintf("   - Model RAM footprint: %.2f MB\n", 
            deployment_metrics$Value[deployment_metrics$Metric == "Model Object Size (RAM)"]))
cat(sprintf("   - Serialized model size: %.2f MB\n",
            deployment_metrics$Value[deployment_metrics$Metric == "Serialized Model Size (Disk)"]))
cat(sprintf("   - Inference latency: %.2f ms per prediction\n",
            deployment_metrics$Value[deployment_metrics$Metric == "Inference Latency"]))

cat("\n==================================================================\n")
cat("                         CONCLUSION                              \n")
cat("==================================================================\n")
if (peak_memory_gb < 8 && total_time_hours < 8) {
  cat("All workflow components completed successfully within the defined\n")
  cat("resource constraints (8GB RAM, overnight execution). This demonstrates\n")
  cat("that stacked ensemble methods are feasible for deployment in\n")
  cat("resource-constrained research settings.\n")
} else {
  cat("WARNING: Workflow exceeded one or more feasibility thresholds.\n")
  cat(sprintf("   - RAM: %.2f GB (threshold: 8GB) - %s\n", 
              peak_memory_gb, ifelse(peak_memory_gb < 8, "OK", "EXCEEDED")))
  cat(sprintf("   - Time: %.2f hours (threshold: 8 hours) - %s\n",
              total_time_hours, ifelse(total_time_hours < 8, "OK", "EXCEEDED")))
}
cat("==================================================================\n")
```

## Timing Summary Table

```{r timing-summary-table}
timing_results %>%
 arrange(desc(wall_time_seconds)) %>%
 mutate(pct_total = 100 * wall_time_seconds / sum(wall_time_seconds)) %>%
 gt() %>%
 tab_header(
   title = "Computational Time Requirements",
   subtitle = sprintf("Total workflow execution time: %.2f hours",
                      sum(timing_results$wall_time_seconds) / 3600)
 ) %>%
 fmt_number(columns = wall_time_seconds, decimals = 1) %>%
 fmt_percent(columns = pct_total, decimals = 1, scale_values = FALSE) %>%
 cols_label(
   component = "Workflow Component",
   wall_time_seconds = "Time (seconds)",
   wall_time_formatted = "Time (formatted)",
   pct_total = "% of Total"
 )
```

## Session Information

```{r session-info}
cat("=== COMPUTATIONAL ENVIRONMENT ===\n\n")
cat("R version:", R.version$version.string, "\n")
cat("Platform:", R.version$platform, "\n")
cat("Running under:", Sys.info()["sysname"], Sys.info()["release"], "\n")
cat("Date executed:", format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z"), "\n\n")

cat("=== KEY ML PACKAGES ===\n\n")
pkgs <- c("tidymodels", "tune", "stacks", "recipes", "parsnip",
         "ranger", "xgboost", "earth", "glmnet", "kernlab",
         "DALEX", "fastshap", "yardstick")
for(pkg in pkgs) {
 if(requireNamespace(pkg, quietly = TRUE)) {
   cat(sprintf("  %s: %s\n", pkg, as.character(packageVersion(pkg))))
 }
}

print(sessionInfo())
```

## Cleanup

```{r cleanup}
# Remove monitoring logs
for (log_file in c("memory_log_tuning.csv", "memory_log_stacking.csv",
                  "memory_log_shap.csv", "stop_monitoring.file")) {
 if (file.exists(log_file)) file.remove(log_file)
}
cat("Monitoring logs cleaned up.\n")
```
